[
  {
    "objectID": "resources/resources.html",
    "href": "resources/resources.html",
    "title": "Additional Resources",
    "section": "",
    "text": "Generally, the R Epidemiology handbook is a great place to look for how to clean, analyze, and visualize data in R.\n\n\nHelp documentation\nSearch the RStudio “Help” tab for documentation on R packages and specific functions. This is within the pane that also contains Files, Plots, and Packages (typically in the lower-right pane). As a shortcut, you can also type the name of a package or function into the R console after a question-mark to open the relevant Help page. Do not include parentheses.\nFor example: ?filter or ?diagrammeR.\nInteractive tutorials\nThere are several ways to learn R interactively within RStudio.\nRStudio itself offers a Tutorial pane that is powered by the learnr R package. Simply install this package and open a tutorial via the new “Tutorial” tab in the upper-right RStudio pane (which also contains Environment and History tabs).\nThe R package swirl offers interactive courses in the R Console. Install and load this package, then run the command swirl() (empty parentheses) in the R console. You will see prompts appear in the Console. Respond by typing in the Console. It will guide you through a course of your choice.\n\n\n\nThere are many PDF “cheatsheets” available on the RStudio website, for example:\n\nFactors with forcats package\n\nDates and times with lubridate package\n\nStrings with stringr package\n\niterative opertaions with purrr package\n\nData import\n\nData transformation cheatsheet with dplyr package\n\nR Markdown (to create documents like PDF, Word, Powerpoint…)\n\nShiny (to build interactive web apps)\n\nData visualization with ggplot2 package\n\nCartography (GIS)\n\nleaflet package (interactive maps)\n\nPython with R (reticulate package)\n\nThis is an online R resource specifically for Excel users\n\n\n\n\n\nR Markdown is a widely-used tool for creating automated, reproducible, and share-worthy outputs, such as reports. It can generate static or interactive outputs, in Word, pdf, html, powerpoint, and other formats.\nAn R Markdown script intersperces R code and text such that the script actually becomes your output document. You can create an entire formatted document, including narrative text (can be dynamic to change based on your data), tables, figures, bullets/numbers, bibliographies, etc.\n\nThe creators of RStudio have an official beginner’s guide on RMarkdown.\nR for Data Science also has a great chapter on Quarto.\nThe definitive R Markdown guide\nQuarto is the new R Markdown put out by Posit, the makers of RStudio (and formerly named RStudio Inc.). You can find a quick tutorial on Quarto here\n\n\n\n\n\nSome great lecture slides on data visualization\nanother plotting cheat sheet\ntidyverse ggplot basics page\n\nplotting continuous variables\n\nR for Data Science pages on data visualization\ngraphics for communicaton\n\n\n\n\n\n\nThe full flextable book is here: https://ardata-fr.github.io/flextable-book/\nThe Github site is here\n\nA manual of all the flextable functions can be found here\nA gallery of beautiful example flextable tables with code can be accessed here\n\n\n\n\nThe R Epidemiology handbook chapter on pivoting.\nThe Data Carpentry page on dplyr\n\nThe tidyverse reference pages on group_by() and grouping\n\nThis page on Data manipulation\n\nSummarize with conditions in dplyr\n\nA helpful tutorial on pivoting tutorial\nThe tidyverse page on joins\n\nThe R for Data Science page on relational data\n\n\n\n\n\nThese lecture notes are a good resource on regression.\n\n\n\n\n\nLecture slides on surveys\nUCLA stats page\n\nArticle by Pew research for a general audeince on why we use survey weights\n\n\n\n\n\n\n\nR has a vibrant twitter community where you can learn tips, shortcuts, and news - follow these accounts:\n\nEpiHandbook (makers of the textbook these materials are based on) @epiRhandbook\n\nR Function A Day @rfuntionaday is an incredible resource\n\nR for Data Science @rstats4ds\n\nRStudio @RStudio\n\nRStudio Tips @rstudiotips\n\nR-Bloggers @Rbloggers\n\nR-ladies @RLadiesGlobal\n\nHadley Wickham @hadleywickham\n\nAlso:\n#epitwitter and #rstats\n\n\n\nA definitive text is the R for Data Science book by Garrett Grolemund and Hadley Wickham\nThe R4Epis project website aims to “develop standardised data cleaning, analysis and reporting tools to cover common types of outbreaks and population-based surveys that would be conducted in an MSF emergency response setting.” You can find R basics training materials, templates for RMarkdown reports on outbreaks and surveys, and tutorials to help you set them up.\n\n\n\n\nMateriales de RStudio en Español\nIntroduction à R et au tidyverse (Francais)"
  },
  {
    "objectID": "resources/resources.html#other-r-resources",
    "href": "resources/resources.html#other-r-resources",
    "title": "Additional Resources",
    "section": "",
    "text": "Generally, the R Epidemiology handbook is a great place to look for how to clean, analyze, and visualize data in R.\n\n\nHelp documentation\nSearch the RStudio “Help” tab for documentation on R packages and specific functions. This is within the pane that also contains Files, Plots, and Packages (typically in the lower-right pane). As a shortcut, you can also type the name of a package or function into the R console after a question-mark to open the relevant Help page. Do not include parentheses.\nFor example: ?filter or ?diagrammeR.\nInteractive tutorials\nThere are several ways to learn R interactively within RStudio.\nRStudio itself offers a Tutorial pane that is powered by the learnr R package. Simply install this package and open a tutorial via the new “Tutorial” tab in the upper-right RStudio pane (which also contains Environment and History tabs).\nThe R package swirl offers interactive courses in the R Console. Install and load this package, then run the command swirl() (empty parentheses) in the R console. You will see prompts appear in the Console. Respond by typing in the Console. It will guide you through a course of your choice.\n\n\n\nThere are many PDF “cheatsheets” available on the RStudio website, for example:\n\nFactors with forcats package\n\nDates and times with lubridate package\n\nStrings with stringr package\n\niterative opertaions with purrr package\n\nData import\n\nData transformation cheatsheet with dplyr package\n\nR Markdown (to create documents like PDF, Word, Powerpoint…)\n\nShiny (to build interactive web apps)\n\nData visualization with ggplot2 package\n\nCartography (GIS)\n\nleaflet package (interactive maps)\n\nPython with R (reticulate package)\n\nThis is an online R resource specifically for Excel users\n\n\n\n\n\nR Markdown is a widely-used tool for creating automated, reproducible, and share-worthy outputs, such as reports. It can generate static or interactive outputs, in Word, pdf, html, powerpoint, and other formats.\nAn R Markdown script intersperces R code and text such that the script actually becomes your output document. You can create an entire formatted document, including narrative text (can be dynamic to change based on your data), tables, figures, bullets/numbers, bibliographies, etc.\n\nThe creators of RStudio have an official beginner’s guide on RMarkdown.\nR for Data Science also has a great chapter on Quarto.\nThe definitive R Markdown guide\nQuarto is the new R Markdown put out by Posit, the makers of RStudio (and formerly named RStudio Inc.). You can find a quick tutorial on Quarto here\n\n\n\n\n\nSome great lecture slides on data visualization\nanother plotting cheat sheet\ntidyverse ggplot basics page\n\nplotting continuous variables\n\nR for Data Science pages on data visualization\ngraphics for communicaton\n\n\n\n\n\n\nThe full flextable book is here: https://ardata-fr.github.io/flextable-book/\nThe Github site is here\n\nA manual of all the flextable functions can be found here\nA gallery of beautiful example flextable tables with code can be accessed here\n\n\n\n\nThe R Epidemiology handbook chapter on pivoting.\nThe Data Carpentry page on dplyr\n\nThe tidyverse reference pages on group_by() and grouping\n\nThis page on Data manipulation\n\nSummarize with conditions in dplyr\n\nA helpful tutorial on pivoting tutorial\nThe tidyverse page on joins\n\nThe R for Data Science page on relational data\n\n\n\n\n\nThese lecture notes are a good resource on regression.\n\n\n\n\n\nLecture slides on surveys\nUCLA stats page\n\nArticle by Pew research for a general audeince on why we use survey weights\n\n\n\n\n\n\n\nR has a vibrant twitter community where you can learn tips, shortcuts, and news - follow these accounts:\n\nEpiHandbook (makers of the textbook these materials are based on) @epiRhandbook\n\nR Function A Day @rfuntionaday is an incredible resource\n\nR for Data Science @rstats4ds\n\nRStudio @RStudio\n\nRStudio Tips @rstudiotips\n\nR-Bloggers @Rbloggers\n\nR-ladies @RLadiesGlobal\n\nHadley Wickham @hadleywickham\n\nAlso:\n#epitwitter and #rstats\n\n\n\nA definitive text is the R for Data Science book by Garrett Grolemund and Hadley Wickham\nThe R4Epis project website aims to “develop standardised data cleaning, analysis and reporting tools to cover common types of outbreaks and population-based surveys that would be conducted in an MSF emergency response setting.” You can find R basics training materials, templates for RMarkdown reports on outbreaks and surveys, and tutorials to help you set them up.\n\n\n\n\nMateriales de RStudio en Español\nIntroduction à R et au tidyverse (Francais)"
  },
  {
    "objectID": "resources/getting-help.html",
    "href": "resources/getting-help.html",
    "title": "Where to get help",
    "section": "",
    "text": "Where to get help\n\nUse the built-in RStudio help interface to search for more information on R functions\n\n\n\n\n\nRStudio help interface.\n\n\n\n\nOne of the fastest ways to get help, is to use the RStudio help interface. This panel by default can be found at the lower right hand panel of RStudio. As seen in the screenshot, by typing the word “Mean”, RStudio tries to also give a number of suggestions that you might be interested in. The description is then shown in the display window.\n\n\nI know the name of the function I want to use, but I’m not sure how to use it\nIf you need help with a specific function, let’s say barplot(), you can type:\n\n?barplot\n\nIf you just need to remind yourself of the names of the arguments, you can use:\n\nargs(lm)\n\n\n\nI want to use a function that does X, there must be a function for it but I don’t know which one…\nIf you are looking for a function to do a particular task, you can use the help.search() function, which is called by the double question mark ??. However, this only looks through the installed packages for help pages with a match to your search request\n\n??kruskal\n\nIf you can’t find what you are looking for, you can use the rdocumentation.org website that searches through the help files across all packages available.\nFinally, a generic Google or internet search “R &lt;task&gt;” will often either send you to the appropriate package documentation or a helpful forum where someone else has already asked your question.\n\n\nI am stuck… I get an error message that I don’t understand\nStart by googling the error message. However, this doesn’t always work very well because often, package developers rely on the error catching provided by R. You end up with general error messages that might not be very helpful to diagnose a problem (e.g. “subscript out of bounds”). If the message is very generic, you might also include the name of the function or package you’re using in your query.\nHowever, you should check Stack Overflow. Search using the [r] tag. Most questions have already been answered, but the challenge is to use the right words in the search to find the answers:\nhttp://stackoverflow.com/questions/tagged/r\nThe Introduction to R can also be dense for people with little programming experience but it is a good place to understand the underpinnings of the R language.\nThe R FAQ is dense and technical but it is full of useful information.\n\n\nAsking for help\nThe key to receiving help from someone is for them to rapidly grasp your problem. You should make it as easy as possible to pinpoint where the issue might be.\nTry to use the correct words to describe your problem. For instance, a package is not the same thing as a library. Most people will understand what you meant, but others have really strong feelings about the difference in meaning. The key point is that it can make things confusing for people trying to help you. Be as precise as possible when describing your problem.\nIf possible, try to reduce what doesn’t work to a simple reproducible example. If you can reproduce the problem using a very small data frame instead of your 50000 rows and 10000 columns one, provide the small one with the description of your problem. When appropriate, try to generalise what you are doing so even people who are not in your field can understand the question. For instance instead of using a subset of your real dataset, create a small (3 columns, 5 rows) generic one. For more information on how to write a reproducible example see this article by Hadley Wickham.\nTo share an object with someone else, if it’s relatively small, you can use the function dput(). It will output R code that can be used to recreate the exact same object as the one in memory:\n\n## iris is an example data frame that comes with R and head() is a\n## function that returns the first part of the data frame\ndput(head(iris))\n\nstructure(list(Sepal.Length = c(5.1, 4.9, 4.7, 4.6, 5, 5.4), \n    Sepal.Width = c(3.5, 3, 3.2, 3.1, 3.6, 3.9), Petal.Length = c(1.4, \n    1.4, 1.3, 1.5, 1.4, 1.7), Petal.Width = c(0.2, 0.2, 0.2, \n    0.2, 0.2, 0.4), Species = structure(c(1L, 1L, 1L, 1L, 1L, \n    1L), levels = c(\"setosa\", \"versicolor\", \"virginica\"), class = \"factor\")), row.names = c(NA, \n6L), class = \"data.frame\")\n\n\nIf the object is larger, provide either the raw file (i.e., your CSV file) with your script up to the point of the error (and after removing everything that is not relevant to your issue). Alternatively, in particular if your question is not related to a data frame, you can save any R object to a file[^export]:\n\nsaveRDS(iris, file=\"/tmp/iris.rds\")\n\nThe content of this file is however not human readable and cannot be posted directly on Stack Overflow. Instead, it can be sent to someone by email who can read it with the readRDS() command (here it is assumed that the downloaded file is in a Downloads folder in the user’s home directory):\n\nsome_data &lt;- readRDS(file=\"~/Downloads/iris.rds\")\n\nLast, but certainly not least, always include the output of sessionInfo() as it provides critical information about your platform, the versions of R and the packages that you are using, and other information that can be very helpful to understand your problem.\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.1    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.1       htmltools_0.5.6   rstudioapi_0.15.0 rmarkdown_2.25   \n [9] knitr_1.44        jsonlite_1.8.7    xfun_0.40         digest_0.6.33    \n[13] rlang_1.1.1       evaluate_0.22    \n\n\n\n\nWhere to ask for help?\n\nThe person sitting next to you. Don’t hesitate to talk to your neighbour during the workshop, compare your answers, and ask for help.\nThe instructors. We’re here to help you.\nYour friendly colleagues: if you know someone with more experience than you, they might be able and willing to help you.\nStack Overflow: if your question hasn’t been answered before and is well crafted, chances are you will get an answer in less than 5 min. Remember to follow their guidelines on how to ask a good question.\nThe R-help mailing list: it is read by a lot of people (including most of the R core team), a lot of people post to it, but the tone can be pretty dry, and it is not always very welcoming to new users. If your question is valid, you are likely to get an answer very fast but don’t expect that it will come with smiley faces. Also, here more than anywhere else, be sure to use correct vocabulary (otherwise you might get an answer pointing to the misuse of your words rather than answering your question). You will also have more success if your question is about a base function rather than a specific package.\nIf your question is about a specific package, see if there is a mailing list for it. Usually it’s included in the DESCRIPTION file of the package that can be accessed using packageDescription(\"name-of-package\"). You may also want to try to email the author of the package directly, or open an issue on the code repository (e.g., GitHub).\nThere are also some topic-specific mailing lists (GIS, phylogenetics, etc…), the complete list is here.\n\n\n\nMore resources\n\nThe Posting Guide for the R mailing lists.\nHow to ask for R help useful guidelines.\nThis blog post by Jon Skeet has quite comprehensive advice on how to ask programming questions.\nThe reprex package is very helpful to create reproducible examples when asking for help. The rOpenSci community call “How to ask questions so they get answered” (Github link and video recording) includes a presentation of the reprex package and of its philosophy.\n\n\nThe materials in this lesson have been adapted from the Introduction to data analysis with R and Bioconductor workshop, which is a part of the Carpentries Incubator. These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "notebooks/session5.html",
    "href": "notebooks/session5.html",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DT) # Creating nice datatables\nlibrary(janitor) # Automatic cleaning\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(readxl) # Read excel files"
  },
  {
    "objectID": "notebooks/session5.html#goal",
    "href": "notebooks/session5.html#goal",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "Goal",
    "text": "Goal\nOur goal is to determine if, in 2008, patients with Diabetes are more likely to be re-admitted to hospitals than those without.\nThis is is not meant to demonstrate a methodologically robust way to perform this analysis. This is an example of an exploratory analysis, where we’re checking our intuition on a small sample of a full dataset. Deeper consideration of our definitions, more thorough detection of corner cases, and more steps would be required to consider this a robust analysis. Instead, the goal of this example analysis is to demonstrate how to perform common data wrangling steps when handling claims data."
  },
  {
    "objectID": "notebooks/session5.html#load-patient-data",
    "href": "notebooks/session5.html#load-patient-data",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "Load patient data",
    "text": "Load patient data\nWhile some outpatient procedures can also count torwards readmission rates for hospitals, let’s only focus on inpatient data for this analysis.\n\nLoad Beneficiaries summary file\nLet’s load the data and the column labels.\n\nben_summary &lt;- read_csv(\"../data/de_synpuf/DE1_0_2008_Beneficiary_Summary_File_Sample_1.csv\")\n\nRows: 116352 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): DESYNPUF_ID, BENE_ESRD_IND, SP_STATE_CODE, BENE_COUNTY_CD, PLAN_CV...\ndbl (27): BENE_BIRTH_DT, BENE_DEATH_DT, BENE_SEX_IDENT_CD, BENE_RACE_CD, BEN...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nben_summary_labels &lt;- read_csv(\"../data/de_synpuf/ben_metadata.csv\")\n\nRows: 32 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, Label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary(ben_summary)\n\n DESYNPUF_ID        BENE_BIRTH_DT      BENE_DEATH_DT      BENE_SEX_IDENT_CD\n Length:116352      Min.   :19090101   Min.   :20080101   Min.   :1.000    \n Class :character   1st Qu.:19281101   1st Qu.:20080301   1st Qu.:1.000    \n Mode  :character   Median :19360501   Median :20080701   Median :2.000    \n                    Mean   :19364181   Mean   :20080654   Mean   :1.553    \n                    3rd Qu.:19420301   3rd Qu.:20081001   3rd Qu.:2.000    \n                    Max.   :19831201   Max.   :20081201   Max.   :2.000    \n                                       NA's   :114538                      \n  BENE_RACE_CD   BENE_ESRD_IND      SP_STATE_CODE      BENE_COUNTY_CD    \n Min.   :1.000   Length:116352      Length:116352      Length:116352     \n 1st Qu.:1.000   Class :character   Class :character   Class :character  \n Median :1.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :1.285                                                           \n 3rd Qu.:1.000                                                           \n Max.   :5.000                                                           \n                                                                         \n BENE_HI_CVRAGE_TOT_MONS BENE_SMI_CVRAGE_TOT_MONS BENE_HMO_CVRAGE_TOT_MONS\n Min.   : 0.00           Min.   : 0.0             Min.   : 0.000          \n 1st Qu.:12.00           1st Qu.:12.0             1st Qu.: 0.000          \n Median :12.00           Median :12.0             Median : 0.000          \n Mean   :11.14           Mean   :10.5             Mean   : 2.576          \n 3rd Qu.:12.00           3rd Qu.:12.0             3rd Qu.: 0.000          \n Max.   :12.00           Max.   :12.0             Max.   :12.000          \n                                                                          \n PLAN_CVRG_MOS_NUM   SP_ALZHDMTA        SP_CHF       SP_CHRNKIDN   \n Length:116352      Min.   :1.000   Min.   :1.000   Min.   :1.000  \n Class :character   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000  \n Mode  :character   Median :2.000   Median :2.000   Median :2.000  \n                    Mean   :1.807   Mean   :1.715   Mean   :1.839  \n                    3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n                    Max.   :2.000   Max.   :2.000   Max.   :2.000  \n                                                                   \n    SP_CNCR         SP_COPD       SP_DEPRESSN     SP_DIABETES   \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.936   Mean   :1.865   Mean   :1.787   Mean   :1.621  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n                                                                \n  SP_ISCHMCHT     SP_OSTEOPRS       SP_RA_OA      SP_STRKETIA   \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.579   Mean   :1.827   Mean   :1.846   Mean   :1.955  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n                                                                \n  MEDREIMB_IP       BENRES_IP         PPPYMT_IP         MEDREIMB_OP     \n Min.   : -3000   Min.   :    0.0   Min.   :    0.00   Min.   :  -90.0  \n 1st Qu.:     0   1st Qu.:    0.0   1st Qu.:    0.00   1st Qu.:    0.0  \n Median :     0   Median :    0.0   Median :    0.00   Median :   20.0  \n Mean   :  2214   Mean   :  249.1   Mean   :   99.14   Mean   :  622.2  \n 3rd Qu.:     0   3rd Qu.:    0.0   3rd Qu.:    0.00   3rd Qu.:  550.0  \n Max.   :164220   Max.   :53096.0   Max.   :68000.00   Max.   :50020.0  \n                                                                        \n   BENRES_OP         PPPYMT_OP         MEDREIMB_CAR     BENRES_CAR    \n Min.   :    0.0   Min.   :    0.00   Min.   :    0   Min.   :   0.0  \n 1st Qu.:    0.0   1st Qu.:    0.00   1st Qu.:    0   1st Qu.:   0.0  \n Median :    0.0   Median :    0.00   Median :  610   Median : 170.0  \n Mean   :  197.5   Mean   :   25.72   Mean   : 1162   Mean   : 328.7  \n 3rd Qu.:  180.0   3rd Qu.:    0.00   3rd Qu.: 1650   3rd Qu.: 480.0  \n Max.   :12450.0   Max.   :14400.00   Max.   :21160   Max.   :5260.0  \n                                                                      \n   PPPYMT_CAR     \n Min.   :   0.00  \n 1st Qu.:   0.00  \n Median :   0.00  \n Mean   :  18.36  \n 3rd Qu.:   0.00  \n Max.   :2110.00  \n                  \n\nDT::datatable(ben_summary_labels)\n\n\n\n\n\n\n\n\nLoad inpatient table\nNow let’s load and link in the inpatient data. To make things simpler, we will drop everything except for the 2008 data.\n\ninp_data &lt;- read_csv(\"../data/de_synpuf/DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv\")\n\nRows: 66773 Columns: 81\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (23): DESYNPUF_ID, PRVDR_NUM, AT_PHYSN_NPI, OP_PHYSN_NPI, OT_PHYSN_NPI, ...\ndbl (13): CLM_ID, SEGMENT, CLM_FROM_DT, CLM_THRU_DT, CLM_PMT_AMT, NCH_PRMRY_...\nlgl (45): HCPCS_CD_1, HCPCS_CD_2, HCPCS_CD_3, HCPCS_CD_4, HCPCS_CD_5, HCPCS_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ninp_labels &lt;- read_csv(\"../data/de_synpuf/inp_metadata.csv\")\n\nRows: 23 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, Label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary(inp_data)\n\n DESYNPUF_ID            CLM_ID             SEGMENT       CLM_FROM_DT      \n Length:66773       Min.   :1.960e+14   Min.   :1.000   Min.   :20071127  \n Class :character   1st Qu.:1.963e+14   1st Qu.:1.000   1st Qu.:20080812  \n Mode  :character   Median :1.965e+14   Median :1.000   Median :20090316  \n                    Mean   :1.965e+14   Mean   :1.001   Mean   :20088463  \n                    3rd Qu.:1.968e+14   3rd Qu.:1.000   3rd Qu.:20091112  \n                    Max.   :1.970e+14   Max.   :2.000   Max.   :20101230  \n                                                        NA's   :68        \n  CLM_THRU_DT        PRVDR_NUM          CLM_PMT_AMT    NCH_PRMRY_PYR_CLM_PD_AMT\n Min.   :20080101   Length:66773       Min.   :-8000   Min.   :    0.0         \n 1st Qu.:20080818   Class :character   1st Qu.: 4000   1st Qu.:    0.0         \n Median :20090321   Mode  :character   Median : 7000   Median :    0.0         \n Mean   :20088611                      Mean   : 9574   Mean   :  398.9         \n 3rd Qu.:20091117                      3rd Qu.:11000   3rd Qu.:    0.0         \n Max.   :20101231                      Max.   :57000   Max.   :68000.0         \n NA's   :68                                                                    \n AT_PHYSN_NPI       OP_PHYSN_NPI       OT_PHYSN_NPI        CLM_ADMSN_DT     \n Length:66773       Length:66773       Length:66773       Min.   :20071127  \n Class :character   Class :character   Class :character   1st Qu.:20080812  \n Mode  :character   Mode  :character   Mode  :character   Median :20090316  \n                                                          Mean   :20088463  \n                                                          3rd Qu.:20091112  \n                                                          Max.   :20101230  \n                                                                            \n ADMTNG_ICD9_DGNS_CD CLM_PASS_THRU_PER_DIEM_AMT NCH_BENE_IP_DDCTBL_AMT\n Length:66773        Min.   :  0.00             Min.   :1024          \n Class :character    1st Qu.:  0.00             1st Qu.:1024          \n Mode  :character    Median :  0.00             Median :1068          \n                     Mean   : 28.98             Mean   :1057          \n                     3rd Qu.: 10.00             3rd Qu.:1068          \n                     Max.   :500.00             Max.   :1100          \n                                                NA's   :2178          \n NCH_BENE_PTA_COINSRNC_LBLTY_AM NCH_BENE_BLOOD_DDCTBL_LBLTY_AM\n Min.   :    0.00               Min.   :   0.00               \n 1st Qu.:    0.00               1st Qu.:   0.00               \n Median :    0.00               Median :   0.00               \n Mean   :   90.03               Mean   :   1.59               \n 3rd Qu.:    0.00               3rd Qu.:   0.00               \n Max.   :34000.00               Max.   :2000.00               \n                                                              \n CLM_UTLZTN_DAY_CNT NCH_BENE_DSCHRG_DT  CLM_DRG_CD        ICD9_DGNS_CD_1    \n Min.   :  0.000    Min.   :20080101   Length:66773       Length:66773      \n 1st Qu.:  2.000    1st Qu.:20080818   Class :character   Class :character  \n Median :  4.000    Median :20090321   Mode  :character   Mode  :character  \n Mean   :  5.583    Mean   :20088612                                        \n 3rd Qu.:  7.000    3rd Qu.:20091117                                        \n Max.   :136.000    Max.   :20101231                                        \n NA's   :68                                                                 \n ICD9_DGNS_CD_2     ICD9_DGNS_CD_3     ICD9_DGNS_CD_4     ICD9_DGNS_CD_5    \n Length:66773       Length:66773       Length:66773       Length:66773      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n ICD9_DGNS_CD_6     ICD9_DGNS_CD_7     ICD9_DGNS_CD_8     ICD9_DGNS_CD_9    \n Length:66773       Length:66773       Length:66773       Length:66773      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n ICD9_DGNS_CD_10    ICD9_PRCDR_CD_1    ICD9_PRCDR_CD_2    ICD9_PRCDR_CD_3   \n Length:66773       Length:66773       Length:66773       Length:66773      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n ICD9_PRCDR_CD_4    ICD9_PRCDR_CD_5    ICD9_PRCDR_CD_6    HCPCS_CD_1    \n Length:66773       Length:66773       Length:66773       Mode:logical  \n Class :character   Class :character   Class :character   NA's:66773    \n Mode  :character   Mode  :character   Mode  :character                 \n                                                                        \n                                                                        \n                                                                        \n                                                                        \n HCPCS_CD_2     HCPCS_CD_3     HCPCS_CD_4     HCPCS_CD_5     HCPCS_CD_6    \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_7     HCPCS_CD_8     HCPCS_CD_9     HCPCS_CD_10    HCPCS_CD_11   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_12    HCPCS_CD_13    HCPCS_CD_14    HCPCS_CD_15    HCPCS_CD_16   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_17    HCPCS_CD_18    HCPCS_CD_19    HCPCS_CD_20    HCPCS_CD_21   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_22    HCPCS_CD_23    HCPCS_CD_24    HCPCS_CD_25    HCPCS_CD_26   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_27    HCPCS_CD_28    HCPCS_CD_29    HCPCS_CD_30    HCPCS_CD_31   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_32    HCPCS_CD_33    HCPCS_CD_34    HCPCS_CD_35    HCPCS_CD_36   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_37    HCPCS_CD_38    HCPCS_CD_39    HCPCS_CD_40    HCPCS_CD_41   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_42    HCPCS_CD_43    HCPCS_CD_44    HCPCS_CD_45   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                            \n                                                            \n                                                            \n                                                            \n                                                            \n\nDT::datatable(inp_labels)"
  },
  {
    "objectID": "notebooks/session5.html#cleaning",
    "href": "notebooks/session5.html#cleaning",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "Cleaning",
    "text": "Cleaning\n\nDuplicates\nLet’s start by checking the data for duplicates. We can use janitor’s get_dupes function. We’ll include the variables we’re interested in. For the beneficiaries table we have no duplicates, which is as expected.\n\nben_summary |&gt; janitor::get_dupes(DESYNPUF_ID)\n\nNo duplicate combinations found of: DESYNPUF_ID\n\n\n# A tibble: 0 × 33\n# ℹ 33 variables: DESYNPUF_ID &lt;chr&gt;, dupe_count &lt;int&gt;, BENE_BIRTH_DT &lt;dbl&gt;,\n#   BENE_DEATH_DT &lt;dbl&gt;, BENE_SEX_IDENT_CD &lt;dbl&gt;, BENE_RACE_CD &lt;dbl&gt;,\n#   BENE_ESRD_IND &lt;chr&gt;, SP_STATE_CODE &lt;chr&gt;, BENE_COUNTY_CD &lt;chr&gt;,\n#   BENE_HI_CVRAGE_TOT_MONS &lt;dbl&gt;, BENE_SMI_CVRAGE_TOT_MONS &lt;dbl&gt;,\n#   BENE_HMO_CVRAGE_TOT_MONS &lt;dbl&gt;, PLAN_CVRG_MOS_NUM &lt;chr&gt;, SP_ALZHDMTA &lt;dbl&gt;,\n#   SP_CHF &lt;dbl&gt;, SP_CHRNKIDN &lt;dbl&gt;, SP_CNCR &lt;dbl&gt;, SP_COPD &lt;dbl&gt;,\n#   SP_DEPRESSN &lt;dbl&gt;, SP_DIABETES &lt;dbl&gt;, SP_ISCHMCHT &lt;dbl&gt;, …\n\n\nWe do have duplicates for the inpatient dataset.\n\ninp_data |&gt; janitor::get_dupes(DESYNPUF_ID, CLM_ADMSN_DT, NCH_BENE_DSCHRG_DT)\n\n# A tibble: 38 × 82\n   DESYNPUF_ID      CLM_ADMSN_DT NCH_BENE_DSCHRG_DT dupe_count  CLM_ID SEGMENT\n   &lt;chr&gt;                   &lt;dbl&gt;              &lt;dbl&gt;      &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 094DCF21CB0EBFE7     20090124           20090228          2 1.97e14       2\n 2 094DCF21CB0EBFE7     20090124           20090228          2 1.97e14       1\n 3 0A4C77361EA1E338     20081120           20081130          2 1.96e14       1\n 4 0A4C77361EA1E338     20081120           20081130          2 1.96e14       1\n 5 11A88A493AF8EBFF     20081007           20081010          2 1.96e14       1\n 6 11A88A493AF8EBFF     20081007           20081010          2 1.97e14       1\n 7 33836C211EA4F936     20080422           20080527          2 1.97e14       2\n 8 33836C211EA4F936     20080422           20080527          2 1.97e14       1\n 9 404C2E3849FBEC8A     20090526           20090527          2 1.96e14       1\n10 404C2E3849FBEC8A     20090526           20090527          2 1.97e14       1\n# ℹ 28 more rows\n# ℹ 76 more variables: CLM_FROM_DT &lt;dbl&gt;, CLM_THRU_DT &lt;dbl&gt;, PRVDR_NUM &lt;chr&gt;,\n#   CLM_PMT_AMT &lt;dbl&gt;, NCH_PRMRY_PYR_CLM_PD_AMT &lt;dbl&gt;, AT_PHYSN_NPI &lt;chr&gt;,\n#   OP_PHYSN_NPI &lt;chr&gt;, OT_PHYSN_NPI &lt;chr&gt;, ADMTNG_ICD9_DGNS_CD &lt;chr&gt;,\n#   CLM_PASS_THRU_PER_DIEM_AMT &lt;dbl&gt;, NCH_BENE_IP_DDCTBL_AMT &lt;dbl&gt;,\n#   NCH_BENE_PTA_COINSRNC_LBLTY_AM &lt;dbl&gt;, NCH_BENE_BLOOD_DDCTBL_LBLTY_AM &lt;dbl&gt;,\n#   CLM_UTLZTN_DAY_CNT &lt;dbl&gt;, CLM_DRG_CD &lt;chr&gt;, ICD9_DGNS_CD_1 &lt;chr&gt;, …\n\n\nThese do, however, have distinct claim IDs.\n\ninp_data |&gt; janitor::get_dupes(DESYNPUF_ID, CLM_ID, CLM_ADMSN_DT, NCH_BENE_DSCHRG_DT)\n\n# A tibble: 14 × 82\n   DESYNPUF_ID       CLM_ID CLM_ADMSN_DT NCH_BENE_DSCHRG_DT dupe_count SEGMENT\n   &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;              &lt;dbl&gt;      &lt;int&gt;   &lt;dbl&gt;\n 1 094DCF21CB0EBFE7 1.97e14     20090124           20090228          2       2\n 2 094DCF21CB0EBFE7 1.97e14     20090124           20090228          2       1\n 3 33836C211EA4F936 1.97e14     20080422           20080527          2       2\n 4 33836C211EA4F936 1.97e14     20080422           20080527          2       1\n 5 43587B9174906162 1.96e14     20090208           20090315          2       2\n 6 43587B9174906162 1.96e14     20090208           20090315          2       1\n 7 4DBAC26594BCC7D8 1.97e14     20090522           20090626          2       2\n 8 4DBAC26594BCC7D8 1.97e14     20090522           20090626          2       1\n 9 6C1C1181D34593A4 1.97e14     20091215           20100119          2       2\n10 6C1C1181D34593A4 1.97e14     20091215           20100119          2       1\n11 7E00D9D78DAA008D 1.97e14     20100405           20100510          2       2\n12 7E00D9D78DAA008D 1.97e14     20100405           20100510          2       1\n13 829E51643F143729 1.97e14     20090731           20090904          2       2\n14 829E51643F143729 1.97e14     20090731           20090904          2       1\n# ℹ 76 more variables: CLM_FROM_DT &lt;dbl&gt;, CLM_THRU_DT &lt;dbl&gt;, PRVDR_NUM &lt;chr&gt;,\n#   CLM_PMT_AMT &lt;dbl&gt;, NCH_PRMRY_PYR_CLM_PD_AMT &lt;dbl&gt;, AT_PHYSN_NPI &lt;chr&gt;,\n#   OP_PHYSN_NPI &lt;chr&gt;, OT_PHYSN_NPI &lt;chr&gt;, ADMTNG_ICD9_DGNS_CD &lt;chr&gt;,\n#   CLM_PASS_THRU_PER_DIEM_AMT &lt;dbl&gt;, NCH_BENE_IP_DDCTBL_AMT &lt;dbl&gt;,\n#   NCH_BENE_PTA_COINSRNC_LBLTY_AM &lt;dbl&gt;, NCH_BENE_BLOOD_DDCTBL_LBLTY_AM &lt;dbl&gt;,\n#   CLM_UTLZTN_DAY_CNT &lt;dbl&gt;, CLM_DRG_CD &lt;chr&gt;, ICD9_DGNS_CD_1 &lt;chr&gt;,\n#   ICD9_DGNS_CD_2 &lt;chr&gt;, ICD9_DGNS_CD_3 &lt;chr&gt;, ICD9_DGNS_CD_4 &lt;chr&gt;, …\n\n\nLet’s remove duplicate columns based on patient, admission date, and discharge date for this analysis.\n\ninp_data_clean &lt;- inp_data |&gt; distinct(across(c(DESYNPUF_ID, CLM_ADMSN_DT, NCH_BENE_DSCHRG_DT)), .keep_all = TRUE)\n\nMissing data looks fine for the beneficiary summary, the only column with a large amount of missing data is death date, which makes sense.\n\ncolMeans(is.na(ben_summary))\n\n             DESYNPUF_ID            BENE_BIRTH_DT            BENE_DEATH_DT \n               0.0000000                0.0000000                0.9844094 \n       BENE_SEX_IDENT_CD             BENE_RACE_CD            BENE_ESRD_IND \n               0.0000000                0.0000000                0.0000000 \n           SP_STATE_CODE           BENE_COUNTY_CD  BENE_HI_CVRAGE_TOT_MONS \n               0.0000000                0.0000000                0.0000000 \nBENE_SMI_CVRAGE_TOT_MONS BENE_HMO_CVRAGE_TOT_MONS        PLAN_CVRG_MOS_NUM \n               0.0000000                0.0000000                0.0000000 \n             SP_ALZHDMTA                   SP_CHF              SP_CHRNKIDN \n               0.0000000                0.0000000                0.0000000 \n                 SP_CNCR                  SP_COPD              SP_DEPRESSN \n               0.0000000                0.0000000                0.0000000 \n             SP_DIABETES              SP_ISCHMCHT              SP_OSTEOPRS \n               0.0000000                0.0000000                0.0000000 \n                SP_RA_OA              SP_STRKETIA              MEDREIMB_IP \n               0.0000000                0.0000000                0.0000000 \n               BENRES_IP                PPPYMT_IP              MEDREIMB_OP \n               0.0000000                0.0000000                0.0000000 \n               BENRES_OP                PPPYMT_OP             MEDREIMB_CAR \n               0.0000000                0.0000000                0.0000000 \n              BENRES_CAR               PPPYMT_CAR \n               0.0000000                0.0000000 \n\n\nFor the inpatient data, we have a lot of columns for Healthcare Common Procedure Coding System (HCPCS) code which appear to be entirely NA. Let’s see which columns have 100% missing data.\nWe’ll remove these columns, but store the data in a new variable to maintain the original dataset.\n\n# Calculates the percent NA in each column and only shows those with 100\nall_missing &lt;- names(inp_data_clean)[colMeans(is.na(inp_data_clean)) == 1.0]\nall_missing\n\n [1] \"HCPCS_CD_1\"  \"HCPCS_CD_2\"  \"HCPCS_CD_3\"  \"HCPCS_CD_4\"  \"HCPCS_CD_5\" \n [6] \"HCPCS_CD_6\"  \"HCPCS_CD_7\"  \"HCPCS_CD_8\"  \"HCPCS_CD_9\"  \"HCPCS_CD_10\"\n[11] \"HCPCS_CD_11\" \"HCPCS_CD_12\" \"HCPCS_CD_13\" \"HCPCS_CD_14\" \"HCPCS_CD_15\"\n[16] \"HCPCS_CD_16\" \"HCPCS_CD_17\" \"HCPCS_CD_18\" \"HCPCS_CD_19\" \"HCPCS_CD_20\"\n[21] \"HCPCS_CD_21\" \"HCPCS_CD_22\" \"HCPCS_CD_23\" \"HCPCS_CD_24\" \"HCPCS_CD_25\"\n[26] \"HCPCS_CD_26\" \"HCPCS_CD_27\" \"HCPCS_CD_28\" \"HCPCS_CD_29\" \"HCPCS_CD_30\"\n[31] \"HCPCS_CD_31\" \"HCPCS_CD_32\" \"HCPCS_CD_33\" \"HCPCS_CD_34\" \"HCPCS_CD_35\"\n[36] \"HCPCS_CD_36\" \"HCPCS_CD_37\" \"HCPCS_CD_38\" \"HCPCS_CD_39\" \"HCPCS_CD_40\"\n[41] \"HCPCS_CD_41\" \"HCPCS_CD_42\" \"HCPCS_CD_43\" \"HCPCS_CD_44\" \"HCPCS_CD_45\"\n\n# For this analysis let's just remove these columns.\ninp_data_clean &lt;- inp_data_clean |&gt; select(-matches(all_missing)) \n\n\n\nConverting Date Variables\nWe have a number of variables which are dates, but which R has read in as numeric values. Let’s convert that data now. We can check the date format and see that it’s in YYYYMMDD, which lubridate will recognize with ymd().\nLet’s save some time and get every column name whose label includes the word “date”.\n\n# Uses regex to check for date or Date\nfilter(inp_labels, str_detect(Label, \"date|Date\"))\n\n# A tibble: 4 × 2\n  Name               Label                    \n  &lt;chr&gt;              &lt;chr&gt;                    \n1 CLM_FROM_DT        Claims start date        \n2 CLM_THRU_DT        Claims end date          \n3 CLM_ADMSN_DT       Inpatient admission date \n4 NCH_BENE_DSCHRG_DT Inpatient discharged date\n\nfilter(ben_summary_labels, str_detect(Label, \"date|Date\")) \n\n# A tibble: 2 × 2\n  Name          Label        \n  &lt;chr&gt;         &lt;chr&gt;        \n1 BENE_BIRTH_DT Date of birth\n2 BENE_DEATH_DT Date of death\n\n\nNote that this is fine for a first pass, but for a final analysis we would want to double check this shortcut for correctness.\nMoreover, if we take a careful look at the column labels, we can see that every date column ends in \"_DT\", and no other columns appear to, which helps confirm our guess.\n\ndate_cols &lt;- filter(inp_labels, str_detect(Label, \"date|Date\")) |&gt; select(Name)\n\nThere are multiple ways to accomplish the conversion, let’s use the across function with ends_with.\n\ninp_data_clean &lt;- inp_data_clean |&gt;\n  mutate(across(ends_with(\"_DT\"), ymd))\n\nThere are other columns which involve lengths of time, but let’s not worry about those for now, since we wouldn’t want to convert those directly into dates.\nLet’s do the same for the beneficiary summary.\n\nben_summary_clean &lt;- ben_summary |&gt;\n  mutate(across(ends_with(\"_DT\"), ymd))\n\nLet’s calculate age in case we want to examine the study population further. Since we’re only using data from 2008 we can base our age calculation around that.\nWe can use simply arithmetic with lubridate which will give us an age in days.\n\nben_summary_clean &lt;- ben_summary_clean |&gt;\n  mutate(age = as.numeric(ymd(20080101) - BENE_BIRTH_DT)/365)\n\nhist(ben_summary_clean$age)\n\n\n\n\nWe’ll see later how to deal with more exact time differences, handling things like leap years, daylight savings, etc. later on. We’re not going to recode all of the variables, but for our analysis let’s at least recode SP_DIABETES. We can check from the coding table that 1 means yes and 2 means no. We can recode this to be a logical TRUE/FALSE value.\n\nben_summary_clean &lt;- ben_summary_clean |&gt;\n  mutate(SP_DIABETES = SP_DIABETES==1)\n\n\n# Calculate the percent of population with diabetes\nsum(ben_summary_clean$SP_DIABETES) / nrow(ben_summary_clean)\n\n[1] 0.3786785"
  },
  {
    "objectID": "notebooks/session5.html#getting-planned-diagnosis-and-procedure-codes",
    "href": "notebooks/session5.html#getting-planned-diagnosis-and-procedure-codes",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "Getting planned diagnosis and procedure codes",
    "text": "Getting planned diagnosis and procedure codes\nWe also need to account for most readmission metrics only caring about unplanned readmissions. The data is provided in the data folder in this repository, but to note we can find tables listing the diagnosis CCS categories which are considered always planned here. We also need to download the single-level CCS category mappings here.\n\nccs_diag &lt;- read_csv(\"../data/de_synpuf/dxref_2015.csv\")\n\nRows: 15073 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): 'ICD-9-CM CODE', 'CCS CATEGORY', 'CCS CATEGORY DESCRIPTION', 'ICD-9...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(ccs_diag)\n\n# A tibble: 6 × 6\n  `'ICD-9-CM CODE'` `'CCS CATEGORY'` `'CCS CATEGORY DESCRIPTION'`\n  &lt;chr&gt;             &lt;chr&gt;            &lt;chr&gt;                       \n1 '     '           '0    '          'No DX'                     \n2 '01000'           '1    '          'Tuberculosis'              \n3 '01001'           '1    '          'Tuberculosis'              \n4 '01002'           '1    '          'Tuberculosis'              \n5 '01003'           '1    '          'Tuberculosis'              \n6 '01004'           '1    '          'Tuberculosis'              \n# ℹ 3 more variables: `'ICD-9-CM CODE DESCRIPTION'` &lt;chr&gt;,\n#   `'OPTIONAL CCS CATEGORY'` &lt;chr&gt;,\n#   `'OPTIONAL CCS CATEGORY DESCRIPTION'` &lt;chr&gt;\n\n\nLet’s clean these codes up, removing trailing quotes and whitespace.\n\nccs_diag &lt;- clean_names(ccs_diag)\n\nccs_diag &lt;- ccs_diag |&gt;\n  mutate(across(everything(),\n    gsub, pattern = \"\\'\", replacement = \"\")) |&gt;\n  mutate(across(everything(), str_trim)) |&gt;\n  mutate(ccs_category = as.numeric(ccs_category))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(everything(), gsub, pattern = \"'\", replacement = \"\")`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\nhead(ccs_diag)\n\n# A tibble: 6 × 6\n  icd_9_cm_code ccs_category ccs_category_description icd_9_cm_code_description \n  &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                    &lt;chr&gt;                     \n1 \"\"                       0 No DX                    INVALID CODES IN USER DATA\n2 \"01000\"                  1 Tuberculosis             PRIM TB COMPLEX-UNSPEC    \n3 \"01001\"                  1 Tuberculosis             PRIM TB COMPLEX-NO EXAM   \n4 \"01002\"                  1 Tuberculosis             PRIM TB COMPLEX-EXM UNKN  \n5 \"01003\"                  1 Tuberculosis             PRIM TB COMPLEX-MICRO DX  \n6 \"01004\"                  1 Tuberculosis             PRIM TB COMPLEX-CULT DX   \n# ℹ 2 more variables: optional_ccs_category &lt;chr&gt;,\n#   optional_ccs_category_description &lt;chr&gt;\n\n\nNow we can select just the codes where the diagnosis matches the codes considered always planned. There are also potentially planned procedures, but we’re going to ignore those for today.\n\nplanned_diag_ccs &lt;- c(45, 254)\n\nplanned_diag &lt;- ccs_diag |&gt;\n  filter(ccs_category %in% planned_diag_ccs)"
  },
  {
    "objectID": "notebooks/session5.html#create-hospital-readmission-variable",
    "href": "notebooks/session5.html#create-hospital-readmission-variable",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "Create hospital readmission variable",
    "text": "Create hospital readmission variable\nNow let’s start getting into the meat of the analysis, identifying hospital readmission.\nFor this initial approach, let’s choose to define readmission as those who were readmitted to a hospital within 30 days of discharge.\n\nDefine readmission\nIn our inpatient table, we have CLM_ADMSN_DT as the inpatient admissions date and NCH_BENE_DSCHRD_DT as the inpatient discharged date.\nOur goal is to create a table where we have each patient hospital admission, and an additional column specifying whether or not there was an unplanned readmission within 30 days.\nWe’ll also not count patients who were admitted to a hospital within 30 days of the end of 2008, since we don’t know whether or not they were re-admitted within the allowed timeframe.\n\n\nCreating a patient-grouped table\nFirst, we need to organize admission and discharge dates by patient.\n\npatient_tbl &lt;- inp_data_clean |&gt;\n  group_by(DESYNPUF_ID) |&gt;\n  arrange(CLM_ADMSN_DT)\n\npatient_tbl\n\n# A tibble: 66,754 × 36\n# Groups:   DESYNPUF_ID [37,780]\n   DESYNPUF_ID      CLM_ID SEGMENT CLM_FROM_DT CLM_THRU_DT PRVDR_NUM CLM_PMT_AMT\n   &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;date&gt;      &lt;date&gt;      &lt;chr&gt;           &lt;dbl&gt;\n 1 1DBED23FBABFF6… 1.96e14       1 2007-11-27  2008-01-01  2601JS           8000\n 2 35F495900193DF… 1.97e14       1 2007-11-28  2008-01-02  3301BB          57000\n 3 81E7E08EDC17B3… 1.96e14       1 2007-12-04  2008-01-06  2600QH           5000\n 4 804C869EDCCA70… 1.96e14       1 2007-12-05  2008-01-09  1002AV          15000\n 5 95F29932C3D13B… 1.97e14       1 2007-12-06  2008-01-10  4200MU          30000\n 6 99BCF42FD5B0E4… 1.96e14       1 2007-12-08  2008-01-12  3300JH           6000\n 7 3AF93E2D9C4605… 1.96e14       1 2007-12-10  2008-01-14  10S0MP          11000\n 8 073A3ED384BF74… 1.97e14       1 2007-12-12  2008-01-16  3302UN          57000\n 9 06040CC1DB09E7… 1.97e14       1 2007-12-13  2008-01-06  3401KT          39000\n10 1A0BA62856EAF7… 1.97e14       1 2007-12-13  2008-01-01  4200QS           6000\n# ℹ 66,744 more rows\n# ℹ 29 more variables: NCH_PRMRY_PYR_CLM_PD_AMT &lt;dbl&gt;, AT_PHYSN_NPI &lt;chr&gt;,\n#   OP_PHYSN_NPI &lt;chr&gt;, OT_PHYSN_NPI &lt;chr&gt;, CLM_ADMSN_DT &lt;date&gt;,\n#   ADMTNG_ICD9_DGNS_CD &lt;chr&gt;, CLM_PASS_THRU_PER_DIEM_AMT &lt;dbl&gt;,\n#   NCH_BENE_IP_DDCTBL_AMT &lt;dbl&gt;, NCH_BENE_PTA_COINSRNC_LBLTY_AM &lt;dbl&gt;,\n#   NCH_BENE_BLOOD_DDCTBL_LBLTY_AM &lt;dbl&gt;, CLM_UTLZTN_DAY_CNT &lt;dbl&gt;,\n#   NCH_BENE_DSCHRG_DT &lt;date&gt;, CLM_DRG_CD &lt;chr&gt;, ICD9_DGNS_CD_1 &lt;chr&gt;, …\n\n\nNow we need to, for each row, check if the discharge rate of the previous row is within 30 days of the next row. Since the data is grouped, R will only make this calculate within each patient.\nWe can get the value from the previous row using lag.\n\npatient_tbl &lt;- patient_tbl |&gt; \n  mutate(prev_dschrg_date = lag(NCH_BENE_DSCHRG_DT))\n\nhead(patient_tbl$prev_dschrg_date)\n\n[1] NA NA NA NA NA NA\n\nhead(na.omit(patient_tbl$prev_dschrg_date))\n\n[1] \"2008-01-10\" \"2008-01-09\" \"2008-01-17\" \"2008-01-03\" \"2008-01-01\"\n[6] \"2008-01-02\"\n\n\nNote that most of the values are NA, since most patients only had a single admission in 2008. At this point we no longer need the data to be grouped by patient.\nNow we can calculate the interval to see if it is within 30 days. lubridate has a useful interval object which lets us define an interval and ask if a time is within that interval using %within%. This will handle the complexities of dates for us, though in this case it’s unlikely any of the intervals would actually be affected.\nTo save some calculation time we’ll also remove the rows where the previous date is NA and add the data back in afterwards.\n\nreadmissions &lt;- patient_tbl |&gt; \n  ungroup() |&gt;\n  drop_na(prev_dschrg_date) |&gt;\n  mutate(is_readmit = prev_dschrg_date %within% interval(CLM_ADMSN_DT, CLM_ADMSN_DT - days(30)))\n\n# The number of readmissions\nsum(readmissions$is_readmit)\n\n[1] 6713\n\n\nNow we need to set the is_readmit variable to false for any planned readmissions.\n\nreadmissions &lt;- readmissions |&gt;\n  mutate(is_readmit = ifelse(\n    ADMTNG_ICD9_DGNS_CD %in% planned_diag$icd_9_cm_code,\n    FALSE, is_readmit))\n\nsum(readmissions$is_readmit)\n\n[1] 6544\n\n\n\n\nConnecting the data back together\nNow we can add back in the single-visit patients to the dataset. We’ll have to replace their NA values from the join with FALSE.\n\ninp_data_clean &lt;- inp_data_clean |&gt;\n  left_join(select(readmissions, DESYNPUF_ID, CLM_ADMSN_DT, NCH_BENE_DSCHRG_DT, is_readmit), by = c(\"DESYNPUF_ID\", \"CLM_ADMSN_DT\", \"NCH_BENE_DSCHRG_DT\")) |&gt;\n  mutate(is_readmit = replace_na(is_readmit, FALSE))\n\nsum(inp_data_clean$is_readmit)\n\n[1] 6544\n\n\nAnd now we can add the diabetes variable to the datset.\n\nreadmission_dataset &lt;- inp_data_clean |&gt;\n  left_join(select(ben_summary_clean, DESYNPUF_ID, SP_DIABETES), by = c(\"DESYNPUF_ID\"))"
  },
  {
    "objectID": "notebooks/session5.html#calculating-the-readmission-rate",
    "href": "notebooks/session5.html#calculating-the-readmission-rate",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "Calculating the readmission rate",
    "text": "Calculating the readmission rate\nWe have all of the data we need. Let’s calculate the readmission rate for diabetic and non-diabetic patients.\n\nlibrary(flextable)\n\n\nAttaching package: 'flextable'\n\n\nThe following object is masked from 'package:purrr':\n\n    compose\n\nproc_freq(readmission_dataset,\"SP_DIABETES\",\"is_readmit\")\n\n\nSP_DIABETESis_readmitFALSETRUETotalFALSECount17,257 (25.9%)1,050 (1.6%)18,307 (27.4%)Mar. pct (1)28.7% ; 94.3%16.0% ; 5.7%TRUECount42,953 (64.3%)5,494 (8.2%)48,447 (72.6%)Mar. pct71.3% ; 88.7%84.0% ; 11.3%TotalCount60,210 (90.2%)6,544 (9.8%)66,754 (100.0%) (1) Columns and rows percentages\n\n\nIf we wanted to answer this question more confidently we’d have to consider how to account for possible sources of bias in our data and decide what we’re investigating more closely. Do we care that diabetes patients might be undergoing procedures which more often result in readmission? Are diabetic patients more often served by hospitals with higher readmission rates? But for learning how to manipulate data in R, this is as far as we’ll for now."
  },
  {
    "objectID": "notebooks/session5.html#types-of-missing-data-in-r",
    "href": "notebooks/session5.html#types-of-missing-data-in-r",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "Types of missing data in R",
    "text": "Types of missing data in R\n\nNA\nIn R, missing values are represented by a reserved (special) value - NA. Note that this is typed without quotes. “NA” is different and is just a normal character value. Your data may have other ways of representing missingness, such as “99”, or “Missing”, or “Unknown” - you may even have empty character value “” which looks “blank”, or a single space ” “.\n\n\nNULL\nNULL is another reserved value in R. It is the logical representation of a statement that is neither true nor false. It is returned by expressions or functions whose values are undefined. It can be thought of as “intentionally left blank”. Null-ness can be assessed using is.null() and conversion can made with as.null().\n\n\nNaN\nImpossible values are represented by the special value NaN. An example of this is when you force R to divide 0 by 0. You can assess this with is.nan(). You may also encounter complementary functions including is.infinite() and is.finite().\n\n\nInf\nInf represents an infinite value, such as when you divide a number by 0.\nAs an example of how this might impact your work: let’s say you have a vector/column z that contains these values: z &lt;- c(1, 22, NA, Inf, NaN, 5)\nIf you want to use max() on the column to find the highest value, you can use the na.rm = TRUE to remove the NA from the calculation, but the Inf and NaN remain and Inf will be returned. To resolve this, you can use brackets [ ] and is.finite() to subset such that only finite values are used for the calculation: max(z[is.finite(z)]).\n\nz &lt;- c(1, 22, NA, Inf, NaN, 5)\nmax(z)                           # returns NA\nmax(z, na.rm=T)                  # returns Inf\nmax(z[is.finite(z)])             # returns 22"
  },
  {
    "objectID": "notebooks/session5.html#handling-missing-data",
    "href": "notebooks/session5.html#handling-missing-data",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data\n\nRemoving missing data\nWe have a variety of options for handling missing data. First, we can use drop_na to remove rows with missing missing values\n\nlinelist &lt;- readRDS(\"../data/linelist_cleaned.rds\")\n\nlinelist %&gt;% \n  drop_na() %&gt;%     # remove rows with ANY missing values\n  nrow()\n\n[1] 1818\n\n\nor remove rows where particular columns have missing values.\n\nlinelist %&gt;% \n  drop_na(contains(\"date\")) %&gt;% # remove rows missing values in any \"date\" column \n  nrow()\n\n[1] 3029\n\n\nThe base R function omit.na performs a similar function.\n\n\nImputing Missing Data\nMethods for imputing missing data range in complexity. There are a variety of specialized packages to handle missing data in R.\nWe can use what we’ve already learned to replace missing values with the mean using the replace_na function.\n\nlinelist &lt;- linelist %&gt;%\n  mutate(temp_replace_na_with_mean = replace_na(temp, mean(temp, na.rm = T)))\n\nWe could also replace missing data values with a particular fixed value.\n\nlinelist &lt;- linelist %&gt;%\n  mutate(outcome_replace_na_with_death = replace_na(outcome, \"Death\"))\n\nLet’s take things a step further and use a statistical model to predict what missing values might be. We can create a simple prediction model for temperature based on fever and age, and use it to impute missing temperature values.\n\nsimple_temperature_model_fit &lt;- lm(temp ~ fever + age_years, data = linelist)\n\n#using our simple temperature model to predict values just for the observations where temp is missing\npredictions_for_missing_temps &lt;- predict(simple_temperature_model_fit,\n                                        newdata = linelist %&gt;% filter(is.na(temp))) \n\n\n\nCarrying observations forward\nAs opposed to imputing values directly, for data with a time element we might want to instead replace missing values with the last observation or a baseline observation. The fill() function from the tidyr can perform this behavior for us. .\nTo show the fill() syntax we’ll make up a simple time series dataset containing the number of cases of a disease for each quarter of the years 2000 and 2001. However, the year value for subsequent quarters after Q1 are missing so we’ll need to impute them.\n\n#creating our simple dataset\ndisease &lt;- tibble::tribble(\n  ~quarter, ~year, ~cases,\n  \"Q1\",    2000,    66013,\n  \"Q2\",      NA,    69182,\n  \"Q3\",      NA,    53175,\n  \"Q4\",      NA,    21001,\n  \"Q1\",    2001,    46036,\n  \"Q2\",      NA,    58842,\n  \"Q3\",      NA,    44568,\n  \"Q4\",      NA,    50197)\n\n#imputing the missing year values:\ndisease %&gt;% fill(year)\n\n# A tibble: 8 × 3\n  quarter  year cases\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Q1       2000 66013\n2 Q2       2000 69182\n3 Q3       2000 53175\n4 Q4       2000 21001\n5 Q1       2001 46036\n6 Q2       2001 58842\n7 Q3       2001 44568\n8 Q4       2001 50197\n\n\nThe zoo package has more powerful functions for imputing missing data in time series data."
  },
  {
    "objectID": "notebooks/session5.html#current-date",
    "href": "notebooks/session5.html#current-date",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "Current date",
    "text": "Current date\nYou can get the current “system” date or system datetime of your computer by doing the following with base R.\n\n# get the system date - this is a DATE class\nSys.Date()\n\n[1] \"2023-10-26\"\n\n# get the system time - this is a DATETIME class\nSys.time()\n\n[1] \"2023-10-26 16:42:34 EDT\"\n\n\nWith the lubridate package these can also be returned with today() and now(), respectively. date() returns the current date and time with weekday and month names."
  },
  {
    "objectID": "notebooks/session5.html#convert-to-date",
    "href": "notebooks/session5.html#convert-to-date",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "Convert to Date",
    "text": "Convert to Date\n\nbase R\nas.Date() is the standard, base R function to convert an object or column to class Date (note capitalization of “D”).\nUse of as.Date() requires that:\n\nYou specify the existing format of the raw character date or the origin date if supplying dates as numbers (see section on Excel dates)\n\nIf used on a character column, all date values must have the same exact format (if this is not the case, try parse_date() from the parsedate package)\n\nFirst, check the class of your column with class() from base R. If you are unsure or confused about the class of your data (e.g. you see “POSIXct”, etc.) it can be easiest to first convert the column to class Character with as.character(), and then convert it to class Date.\nSecond, within the as.Date() function, use the format = argument to tell R the current format of the character date components - which characters refer to the month, the day, and the year, and how they are separated. If your values are already in one of R’s standard date formats (“YYYY-MM-DD” or “YYYY/MM/DD”) the format = argument is not necessary.\nTo format =, provide a character string (in quotes) that represents the current date format using the special “strptime” abbreviations below. For example, if your character dates are currently in the format “DD/MM/YYYY”, like “24/04/1968”, then you would use format = \"%d/%m/%Y\" to convert the values into dates. Putting the format in quotation marks is necessary. And don’t forget any slashes or dashes!\n\n# Convert to class date\nlinelist &lt;- linelist %&gt;% \n  mutate(date_onset = as.Date(date_of_onset, format = \"%d/%m/%Y\"))\n\n\n\nlubridate\nConverting character objects to dates can be made easier by using the lubridate package. This is a tidyverse package designed to make working with dates and times more simple and consistent than in base R. For these reasons, lubridate is often considered the gold-standard package for dates and time, and is recommended whenever working with them.\nThe lubridate package provides several different helper functions designed to convert character objects to dates in an intuitive, and more lenient way than specifying the format in as.Date(). These functions are specific to the rough date format, but allow for a variety of separators, and synonyms for dates (e.g. 01 vs Jan vs January) - they are named after abbreviations of date formats.\nThe ymd() function flexibly converts date values supplied as year, then month, then day.\n\n# read date in year-month-day format\nymd(\"2020-10-11\")\n\n[1] \"2020-10-11\"\n\nymd(\"20201011\")\n\n[1] \"2020-10-11\"\n\n\nThe mdy() function flexibly converts date values supplied as month, then day, then year.\n\n# read date in month-day-year format\nmdy(\"10/11/2020\")\n\n[1] \"2020-10-11\"\n\nmdy(\"Oct 11 20\")\n\n[1] \"2020-10-11\"\n\n\nThe dmy() function flexibly converts date values supplied as day, then month, then year.\n\n# read date in day-month-year format\ndmy(\"11 10 2020\")\n\n[1] \"2020-10-11\"\n\ndmy(\"11 October 2020\")\n\n[1] \"2020-10-11\"\n\n\nYou can use the lubridate functions make_date() and make_datetime() to combine multiple numeric columns into one date column. For example if you have numeric columns onset_day, onset_month, and onset_year in the data frame linelist:\n\nlinelist &lt;- linelist %&gt;% \n  mutate(onset_date = make_date(year = onset_year, month = onset_month, day = onset_day))"
  },
  {
    "objectID": "notebooks/session5.html#working-with-dates-1",
    "href": "notebooks/session5.html#working-with-dates-1",
    "title": "Data Wrangling II and Other Analysis Skills",
    "section": "Working with dates",
    "text": "Working with dates\nlubridate can also be used for a variety of other functions, such as extracting aspects of a date/datetime, performing date arithmetic, or calculating date intervals\nHere we define a date to use for the examples:\n\n# create object of class Date\nexample_date &lt;- ymd(\"2020-03-01\")\n\n\nExtract date components\nYou can extract common aspects such as month, day, weekday:\n\nmonth(example_date)  # month number\n\n[1] 3\n\nday(example_date)    # day (number) of the month\n\n[1] 1\n\nwday(example_date)   # day number of the week (1-7)\n\n[1] 1\n\n\nYou can also extract time components from a datetime object or column. This can be useful if you want to view the distribution of admission times.\n\nexample_datetime &lt;- ymd_hm(\"2020-03-01 14:45\")\n\nhour(example_datetime)     # extract hour\nminute(example_datetime)   # extract minute\nsecond(example_datetime)   # extract second\n\n\n\nDate math\nYou can add certain numbers of days or weeks using their respective function from lubridate.\n\n# add 3 days to this date\nexample_date + days(3)\n\n[1] \"2020-03-04\"\n\n# add 7 weeks and subtract two days from this date\nexample_date + weeks(7) - days(2)\n\n[1] \"2020-04-17\"\n\n\n\n\nDate intervals\nThe difference between dates can be calculated by:\n\nEnsure both dates are of class date\n\nUse subtraction to return the “difftime” difference between the two dates\n\nIf necessary, convert the result to numeric class to perform subsequent mathematical calculations\n\nBelow the interval between two dates is calculated and displayed. You can find intervals by using the subtraction “minus” symbol on values that are class Date. Note, however that the class of the returned value is “difftime” as displayed below, and must be converted to numeric.\n\n# find the interval between this date and Feb 20 2020 \noutput &lt;- example_date - ymd(\"2020-02-20\")\noutput    # print\n\nTime difference of 10 days\n\nclass(output)\n\n[1] \"difftime\"\n\n\nTo do subsequent operations on a “difftime”, convert it to numeric with as.numeric().\nThis can all be brought together to work with data - for example:\n\nlinelist &lt;- linelist %&gt;%\n  \n  # convert date of onset from character to date objects by specifying dmy format\n  mutate(date_onset = dmy(date_onset),\n         date_hospitalisation = dmy(date_hospitalisation)) %&gt;%\n  \n  # filter out all cases without onset in march\n  filter(month(date_onset) == 3) %&gt;%\n    \n  # find the difference in days between onset and hospitalisation\n  mutate(days_onset_to_hosp = date_hospitalisation - date_of_onset)\n\nlead() and lag() are functions from the dplyr package which help find previous (lagged) or subsequent (leading) values in a vector - typically a numeric or date vector. This is useful when doing calculations of change/difference between time units."
  },
  {
    "objectID": "notebooks/session3.html",
    "href": "notebooks/session3.html",
    "title": "Modeling and Testing",
    "section": "",
    "text": "We import the dataset of cases from a simulated Ebola epidemic.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DT)\n\nset.seed(1) # Set the random seed\n# set.seed(Sys.time()) # Useful if you want to \"unset\" the seed\n\nlinelist &lt;- readRDS(\"../data/linelist_cleaned.rds\")\n\nIf we always only ever look at the first rows of a dataset, we might miss other patterns later on. Thus, this time let’s look at a random 100 samples of this dataset.\n\n# Base R way\ndatatable(linelist[sample(nrow(linelist), 100),], filter = \"top\")\n\n\n\n\n\n# Tidyverse way\nlinelist |&gt;\n  slice_sample(n = 100) |&gt;\n  datatable(filter = \"top\")"
  },
  {
    "objectID": "notebooks/session3.html#example-analysis---infant-mortality",
    "href": "notebooks/session3.html#example-analysis---infant-mortality",
    "title": "Modeling and Testing",
    "section": "Example Analysis - Infant Mortality",
    "text": "Example Analysis - Infant Mortality\n\ninfdeath = read.table( \"../data/infdeath.txt\", head=TRUE, sep=\"\\t\")\ndim(infdeath)\n\n[1] 20  4\n\ncolnames(infdeath)\n\n[1] \"country\" \"safe\"    \"breast\"  \"death\"  \n\n\nWe have 20 measurements on - safe=access to safe water - breast= percentage of mothers breast feeding at 6 months - death = infant death rate\nWhen performing any time of analysis where we look at the relationship between two variables, we need to be careful to not mix up correlation and causation.\n\nplot(infdeath$breast, infdeath$death, xlab=\"% Breast Feeding\", \n     ylab=\"Infant Death Rate\", pty=\"s\", pch=19, cex.axis=1.5,\n     cex.lab=1.5)\n\n\n\n\nLooking at this data naively, it suggests that longer time breast feeding leads to greater infant mortality.\n\nplot(infdeath$safe, infdeath$breast, ylab=\"% Breast Feeding\", \n     xlab=\"% Access to safe water\", pty=\"s\", pch=19, cex.axis=1.5,\n     cex.lab=1.5)\n\n\n\n\nHowever, there is a latent variable which is actually influencing our result. Countries with access to safe water breast feed for less time.\nOften, as in the example above, there is a third variable (access to clean water) that affects both the response and the predictor. To truly get at causation we typically need a randomized controlled experiment However, we cannot always do experiments, e.g. most epidemiology.\nWe can use the lm function in R to create a linear model. We then also use R’s formula notation to tell it what we want the model to describe. Here, we want to examine the relationship between mortality and breastfeeding, which we can specify by giving R the linear model y~x, or here infdeath$death ~ infdeath$breast. Note that we don’t need to tell R that we want the model to have an intercept, it incorporates one automatically.\n\nplot(infdeath$breast, infdeath$death, xlab=\"% Breast Feeding\", \n     ylab=\"Infant Death Rate\", pty=\"s\", pch=19, cex.axis=1.5,\n     cex.lab=1.5, xlim=c(0, 100), ylim=c(-30,150))\nlm1 = lm(infdeath$death~infdeath$breast)\nabline(lm1, col=\"blue\")\n\n\n\n\nWe can directly output the model coefficients and other information, here the learned slope and intercept of the line, by using summary.\n\nsummary(lm1)\n\n\nCall:\nlm(formula = infdeath$death ~ infdeath$breast)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.568 -21.047   1.368  19.479  33.705 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -26.978     20.148  -1.339 0.205392    \ninfdeath$breast    1.467      0.288   5.093 0.000265 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.86 on 12 degrees of freedom\n  (6 observations deleted due to missingness)\nMultiple R-squared:  0.6837,    Adjusted R-squared:  0.6573 \nF-statistic: 25.94 on 1 and 12 DF,  p-value: 0.000265\n\n\nWe can see that the intercept is about -27 and the slope is about 1.5.\n\nResidual Plots\nWe can plot the residuals (\\(y\\)-axis) against any covariates (\\(x\\)’s) and the predicted values (\\(\\hat{y}\\)’s). We want to look for trends, such as increasing or decreasing variability (fans), and outliers. When data are collected over time, we sometimes look for relationships between successive residuals.\nLet’s return to our child birth example and make a residual plot."
  },
  {
    "objectID": "notebooks/session3.html#confidence-and-prediction-intervals",
    "href": "notebooks/session3.html#confidence-and-prediction-intervals",
    "title": "Modeling and Testing",
    "section": "Confidence and Prediction intervals",
    "text": "Confidence and Prediction intervals\nThere are two kinds of predictions that are interesting:\n-   *Confidence interval*: predict the mean response for a given value of $x$\n-   *Prediction interval*: predict the value of the response $y$ for an individual whose covariate is $x$\nAlmost all regression methods in R have prediction methods associated with them. Confidence intervals have less variability than prediction intervals.\nReturning to our example on infant death\n\nlm1 = lm(death~breast, data=infdeath)\nsummary(lm1)\n\n\nCall:\nlm(formula = death ~ breast, data = infdeath)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.568 -21.047   1.368  19.479  33.705 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -26.978     20.148  -1.339 0.205392    \nbreast         1.467      0.288   5.093 0.000265 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.86 on 12 degrees of freedom\n  (6 observations deleted due to missingness)\nMultiple R-squared:  0.6837,    Adjusted R-squared:  0.6573 \nF-statistic: 25.94 on 1 and 12 DF,  p-value: 0.000265\n\n\nWe can predict the mean response for country where 62% of the women breast feed at 6 months using the `predict1 function.\n\npredict(lm1, newdata=list(breast=62), interval=\"confidence\")\n\n       fit     lwr      upr\n1 63.96593 49.8048 78.12705\n\n\nOr we can predict the response for a specific country where 62% of the women breast feed at 6 months\n\npredict(lm1, newdata=list(breast=62), interval=\"prediction\")\n\n       fit      lwr      upr\n1 63.96593 10.08279 117.8491\n\n\nNote how the lower and upper bounds are much tighter for the confidence interval than for the prediction interval."
  },
  {
    "objectID": "notebooks/session3.html#regressions-using-factors",
    "href": "notebooks/session3.html#regressions-using-factors",
    "title": "Modeling and Testing",
    "section": "Regressions using factors",
    "text": "Regressions using factors\nAs we’ve seen, a factor is a variable that takes on a discrete set of different values. These values can be unordered (e.g. Male/Female or European, Asian, African, etc.) or they can be ordered (age: less than 18, 18-40, more than 40). In regression models, we typically implement factors using dummy variables.\n\nFitting factors in regression models\nSuppose we have a factor with \\(K\\) levels We can fit factors in two ways\n-   **M1:** includes an intercept in the model and then use $K-1$ indicator variables\n-   **M2L=:** no intercept and use $K$ indicator variables.\nIn M1 the intercept is the mean value of \\(y\\), and each \\(\\beta_j\\) is the difference in mean for the \\(j^{th}\\) retained factor level from the overall mean. In M2 each of the \\(\\beta_j\\) is the mean value for \\(y\\) only within factor level \\(j\\).\nTo show an example, suppose our factor is sex, which has two levels, M and F. Our models could be\n\\[\n  M1:  Y = \\beta_0 + \\beta_M \\cdot 1_{M} + \\epsilon \\\\\n  E[Y | F]= \\beta_0 \\quad \\mbox{and} \\quad E[Y|M] = \\beta_0 + \\beta_M\n\\]\n\nheights = runif(40, min=60, max=75)\nsex = sample(c(\"M\",\"F\"), 40, replace=TRUE)\nlm1 = lm(heights ~ sex)\nsummary(lm1)\n\n\nCall:\nlm(formula = heights ~ sex)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8438 -2.9168 -0.0762  1.8674  9.0004 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   65.419      0.923  70.877   &lt;2e-16 ***\nsexM           0.903      1.245   0.726    0.473    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.916 on 38 degrees of freedom\nMultiple R-squared:  0.01366,   Adjusted R-squared:  -0.01229 \nF-statistic: 0.5265 on 1 and 38 DF,  p-value: 0.4725\n\n\nOr\n\\[\n  M2: y = \\beta_M \\cdot 1_{M} + \\beta_{F} \\cdot 1_{F} + \\epsilon \\\\\n  E[Y|F] = \\beta_F \\quad \\mbox{and} E[Y|M] = \\beta_M\n\\]\n\n# Adding -1 tells R that we don't want an intercept\nlm2 = lm(heights ~ sex - 1)\nsummary(lm2)\n\n\nCall:\nlm(formula = heights ~ sex - 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8438 -2.9168 -0.0762  1.8674  9.0004 \n\nCoefficients:\n     Estimate Std. Error t value Pr(&gt;|t|)    \nsexF  65.4191     0.9230   70.88   &lt;2e-16 ***\nsexM  66.3222     0.8349   79.44   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.916 on 38 degrees of freedom\nMultiple R-squared:  0.9967,    Adjusted R-squared:  0.9965 \nF-statistic:  5667 on 2 and 38 DF,  p-value: &lt; 2.2e-16\n\n\nNote that there are some issues you have to worry about when fitting a model without an intercept. With model M1 that has an intercept then for each group the test for \\(H_0: \\beta_j = 0\\) tests whether that group mean is different from the mean for the group that was used to determine the intercept. However, in M2 the test for each group, \\(H_0: \\beta_j = 0\\) is then comparing the mean for that group to zero (0). In M2, multiple-\\(R^2\\) does not have a reasonable interpretation, and we get a very high value."
  },
  {
    "objectID": "notebooks/session3.html#example-analysis---spline-regression-with-nhanes",
    "href": "notebooks/session3.html#example-analysis---spline-regression-with-nhanes",
    "title": "Modeling and Testing",
    "section": "Example Analysis - Spline Regression with NHANES",
    "text": "Example Analysis - Spline Regression with NHANES\n\nLoad the data\nThere are 6063 observations, some are incomplete and have missing values for some covariates. There are 22 covariates, which have cryptic names and you need to use the meta-data to resolve them. The survey is very complex and typically any analysis requires a substantial amount of reading of the documentation. Here we will guide you past some of the hurdles.\nWe load up the data and the metadata. In the metadata we have a textual description of the phenotype, the short name, and the target. The target tells us which of the sampled individuals was eligible to answer the question.\n\nnhanesDataPath = \"\"\nload(\"../data/nhanes_spline/d4.rda\")\nload(\"../data/nhanes_spline/metaD.rda\")\nDT::datatable(metaD)\n\n\n\n\n\n\nWe will look at the relationship between the variable LBXTC (which is Total Cholesterol in mg/dL measured by a blood draw) and the age of the participant in years.\n\n\n\n\n\nAnd we can see that in this plot, over-plotting is a substantial issue here. You might also notice what seems like a lot of data at age 80, this is because any age over 80 was truncated to 80 to prevent reidentification of survey participants. In a complete analysis, this should probably be adjusted for in some way, but we will ignore it for now.\nWe can try some other methods, such as hexbin plotting and smoothScatter to get a better idea of the distribution of the data.\n\n\n\n\n\nNow we can see a few outliers - with extremely high serum cholesterol. We get a sense that the trend is not exactly a straight line, but rather a parabola, lower for the young and the old and a bit higher in the middle.\nWe fit a linear model first.\n\nlm1 = lm(d4$LBXTC ~ d4$RIDAGEYR)\nsummary(lm1)\n\n\nCall:\nlm(formula = d4$LBXTC ~ d4$RIDAGEYR)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-114.68  -27.95   -2.91   23.34  357.19 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 170.0140     1.4340  118.56   &lt;2e-16 ***\nd4$RIDAGEYR   0.3708     0.0285   13.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.24 on 5689 degrees of freedom\n  (372 observations deleted due to missingness)\nMultiple R-squared:  0.02891,   Adjusted R-squared:  0.02874 \nF-statistic: 169.4 on 1 and 5689 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(lm1$fitted.values, lm1$residuals)\n\n##fit a loess curve\nl2 = loess(lm1$residuals ~ lm1$fitted.values)\npl = predict(l2, newdata=sort(lm1$fitted.values))\nlines(x=sort(lm1$fitted.values), y=pl, col=\"blue\", lwd=2)\nabline(h=0, col=\"red\")\n\n\n\n\nNotice that both terms in the model are very significant, but that the multiple \\(R^2\\) is only around 2%.\nSo age, in years, is not explaining very much of the variation. But because we have such a large data set, the parameter estimates are found to be significantly different from zero.\n\n\nSpline Models\nWhen a linear model does not appear sufficient we can try other models. One choice is to use natural splines, which are very flexible. They create a “knotted” line, where at each knot the slope of the line can change. They are based on B-splines with the prevision that the model is linear outside the range of the data.\nHowever, we have to decide the number of knots we want to use in the model. Based on the initial analysis, we chose to use df=7, which gives five internal knots when fitting the splines. You have almost 6,000 degrees of freedom here, so using up a few to get a more appropriate fit seems good.\n\nlibrary(\"splines\")\nlm2 = lm(d4$LBXTC ~ ns(d4$RIDAGEYR, df=7))\nsummary(lm2)\n\n\nCall:\nlm(formula = d4$LBXTC ~ ns(d4$RIDAGEYR, df = 7))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-113.43  -26.32   -2.88   22.47  343.31 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               154.799      2.178  71.071  &lt; 2e-16 ***\nns(d4$RIDAGEYR, df = 7)1   39.956      3.379  11.826  &lt; 2e-16 ***\nns(d4$RIDAGEYR, df = 7)2   32.705      4.074   8.028 1.20e-15 ***\nns(d4$RIDAGEYR, df = 7)3   55.583      3.637  15.283  &lt; 2e-16 ***\nns(d4$RIDAGEYR, df = 7)4   42.275      3.725  11.347  &lt; 2e-16 ***\nns(d4$RIDAGEYR, df = 7)5   30.111      3.352   8.984  &lt; 2e-16 ***\nns(d4$RIDAGEYR, df = 7)6   41.098      5.758   7.137 1.07e-12 ***\nns(d4$RIDAGEYR, df = 7)7   15.992      2.478   6.453 1.19e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.62 on 5683 degrees of freedom\n  (372 observations deleted due to missingness)\nMultiple R-squared:  0.1044,    Adjusted R-squared:  0.1033 \nF-statistic: 94.65 on 7 and 5683 DF,  p-value: &lt; 2.2e-16\n\n\nWe can use an anova to compare the models.\n\nanova(lm1, lm2)\n\nAnalysis of Variance Table\n\nModel 1: d4$LBXTC ~ d4$RIDAGEYR\nModel 2: d4$LBXTC ~ ns(d4$RIDAGEYR, df = 7)\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   5689 9674185                                  \n2   5683 8922039  6    752146 79.848 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "notebooks/session3.html#principle-components",
    "href": "notebooks/session3.html#principle-components",
    "title": "Modeling and Testing",
    "section": "Principle Components",
    "text": "Principle Components\nLet’s now take the continuous variables and look at principle components. To calculate the PCA we’ll also need to remove any missing data in continuous variables.\n\nlibrary(ggfortify)\n\ncvars = c(\"RIDAGEYR\", \"INDFMPIR\", \"LBDHDD\", \"LBXGH\", \"BMXBMI\", \"LBXTC\")\ncont_d = d4[, cvars]\n\ncomplete_ind = complete.cases(cont_d)\n\ncont_d = cont_d[complete_ind,]\nd4_missing = d4[!complete_ind,]\nd4_comp = d4[complete_ind,]\n\npcs = prcomp(cont_d)\nautoplot(pcs)\n\n\n\nwhich(abs(pcs$x[,1]) &gt; max(abs(pcs$x[,1]))*0.95)\n\n3781 3862 \n3151 3219 \n\nwhich(abs(pcs$x[,2]) &gt; max(abs(pcs$x[,2]))*0.95)\n\n 215  859 3129 3348 3363 3837 4735 4787 5932 \n 173  705 2593 2778 2791 3199 3937 3985 4951"
  },
  {
    "objectID": "notebooks/session3.html#random-forests",
    "href": "notebooks/session3.html#random-forests",
    "title": "Modeling and Testing",
    "section": "Random Forests",
    "text": "Random Forests\nRandom Forests are a simple way to get a sense of how important different variables are in predicting a variable of interest.\n\nlibrary(\"randomForest\")\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nrf1 = randomForest(LBXTC ~ ., proximity=TRUE, data=cont_d)\nvarImpPlot(rf1)"
  },
  {
    "objectID": "notebooks/session3.html#preparing-survey-data",
    "href": "notebooks/session3.html#preparing-survey-data",
    "title": "Modeling and Testing",
    "section": "Preparing Survey Data",
    "text": "Preparing Survey Data\n\nRemoving NA weights\nWe can’t have any missing values in the survey design variables.\nRemove all rows with NA for the main survey design variables; WTMEC2YR, SDMVPSU, and SDMVSTRA.\n\nwt_nhanes &lt;- all_nhanes %&gt;%\n  drop_na(WTMEC2YR, SDMVPSU, SDMVSTRA)"
  },
  {
    "objectID": "notebooks/session3.html#creating-combined-survey-weights",
    "href": "notebooks/session3.html#creating-combined-survey-weights",
    "title": "Modeling and Testing",
    "section": "Creating combined survey weights",
    "text": "Creating combined survey weights\nCurrently, our survey weights WTSAF2YR are for each 2 year cycle. We need to combine them to represent the full 6-year period we are investigating. Luckily NHANES has an official guide for combining these weights. It turns out, all we need to do is divide all weights by 3.\n\n##try de-tidying it to see if we can get the models to behave\nwt_nhanes &lt;- wt_nhanes %&gt;%\n  mutate(WTMEC6YR = WTMEC2YR * 1/3)\nwt_nhanes=data.frame(wt_nhanes)\n wt_nhanes$diabetes = factor(wt_nhanes$diabetes, levels=c(\"nondiabetic\",\"prediabetic\",\"diabetic\"))"
  },
  {
    "objectID": "notebooks/session3.html#creating-the-survey-design-object",
    "href": "notebooks/session3.html#creating-the-survey-design-object",
    "title": "Modeling and Testing",
    "section": "Creating the survey design object",
    "text": "Creating the survey design object\nWe need to use some specialized survey analysis methods which are contained in the survey package written by Thomas Lumley.\n\nnhanes_design &lt;- svydesign(id     = ~SDMVPSU,\n                          strata  = ~SDMVSTRA,\n                          weights = ~WTMEC6YR,\n                          nest    = TRUE,\n                          survey.lonely.psu = \"adjust\",\n                          data    = wt_nhanes)\n\nsummary(nhanes_design)\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (93) clusters.\nsvydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC6YR, \n    nest = TRUE, survey.lonely.psu = \"adjust\", data = wt_nhanes)\nProbabilities:\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n1.556e-05 8.077e-05 1.532e-04       Inf 3.014e-04       Inf \nStratum Sizes: \n            44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\nobs        760 653 768 631 628 936 651 629 696 617 689 668 772 694 556 711 740\ndesign.PSU   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\nactual.PSU   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n            61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\nobs        852 788 694 689 731 716 797 714 530 541 560 561 267 258 803 785 823\ndesign.PSU   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\nactual.PSU   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n            78  79  80  81  82  83  84  85  86  87  88  89\nobs        829 696 751 696 724 713 683 592 946 598 647 251\ndesign.PSU   2   2   2   2   2   2   2   2   3   2   2   2\nactual.PSU   2   2   2   2   2   2   2   2   3   2   2   2\nData variables:\n [1] \"...1\"          \"SEQN\"          \"RIDAGEYR\"      \"RIAGENDR\"     \n [5] \"RIDRETH1\"      \"DMDBORN\"       \"INDFMPIR\"      \"SDMVPSU\"      \n [9] \"SDMVSTRA\"      \"WTINT2YR\"      \"WTMEC2YR\"      \"OHXDECAY\"     \n[13] \"OHXREST\"       \"LBXGLU\"        \"WTSAF2YR\"      \"LBXGH\"        \n[17] \"BMXBMI\"        \"Begin.Year\"    \"EndYear\"       \"age.cat\"      \n[21] \"LBXGH.cat\"     \"RIDAGEYR.cat\"  \"LBXGLU.cat\"    \"BMXBMI.cat\"   \n[25] \"INDFMPIR.cat\"  \"DMDBORN.cat\"   \"dental.caries\" \"diabetes\"     \n[29] \"WTMEC6YR\"     \n\n\nNow we can take our data subset from the survey design object.\n\nado_design &lt;- subset(nhanes_design, RIDAGEYR &gt;= 13 & RIDAGEYR &lt;= 18 & !is.na(OHXDECAY))\n\n#Also make a tibble of this data to analyze\nado_data &lt;- wt_nhanes %&gt;%\n  filter(RIDAGEYR &gt;= 13 & RIDAGEYR &lt;= 18) %&gt;% # Gets the 3660 nonedentulous adolescents\n  filter(!is.na(OHXDECAY)) %&gt;% # Gets the 3346 with non-NA dental carie variable\n  filter(!is.na(diabetes)) # Gets the 3046 with a diabetic status\n\nThen create tables using estimated counts from demographic survey weights.\n\ntab1 = svytable(~age.cat + dental.caries, ado_design)\ntab1\n\n       dental.caries\nage.cat   FALSE    TRUE\n  13-15 5869985 6545245\n  16-18 4858434 7112471\n\n\n\nsvytable(~RIDRETH1, ado_design)\n\nRIDRETH1\n              Mexican American             Non-Hispanic Black \n                       2764818                        3542439 \n            Non-Hispanic White                 Other Hispanic \n                      15249583                        1334732 \nOther Race - Including Multi-R \n                       1494563"
  },
  {
    "objectID": "notebooks/session3.html#logistic-regression",
    "href": "notebooks/session3.html#logistic-regression",
    "title": "Modeling and Testing",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nlogit1 &lt;- svyglm(dental.caries~ diabetes, family=quasibinomial, design=ado_design, na.action = na.omit)\nexp(coef(logit1))\n\n        (Intercept) diabetesprediabetic    diabetesdiabetic \n          1.2899734           0.9875545           2.9022173 \n\nsummary(logit1)\n\n\nCall:\nsvyglm(formula = dental.caries ~ diabetes, design = ado_design, \n    family = quasibinomial, na.action = na.omit)\n\nSurvey design:\nsubset(nhanes_design, RIDAGEYR &gt;= 13 & RIDAGEYR &lt;= 18 & !is.na(OHXDECAY))\n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          0.25462    0.06871   3.706 0.000575 ***\ndiabetesprediabetic -0.01252    0.14228  -0.088 0.930250    \ndiabetesdiabetic     1.06548    0.55644   1.915 0.061887 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.9922246)\n\nNumber of Fisher Scoring iterations: 4\n\n\nA Wald test for diabetes:\n\nregTermTest(logit1, ~diabetes)\n\nWald test for diabetes\n in svyglm(formula = dental.caries ~ diabetes, design = ado_design, \n    family = quasibinomial, na.action = na.omit)\nF =  1.835058  on  2  and  45  df: p= 0.17135"
  },
  {
    "objectID": "notebooks/session3.html#predictive-modeling-with-tidymodels",
    "href": "notebooks/session3.html#predictive-modeling-with-tidymodels",
    "title": "Modeling and Testing",
    "section": "Predictive Modeling with TidyModels",
    "text": "Predictive Modeling with TidyModels\nThis example comes from here\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'parsnip':\n  method          from     \n  autoplot.glmnet ggfortify\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ randomForest::combine() masks dplyr::combine()\n✖ scales::discard()       masks purrr::discard()\n✖ Matrix::expand()        masks tidyr::expand()\n✖ dplyr::filter()         masks stats::filter()\n✖ recipes::fixed()        masks stringr::fixed()\n✖ dplyr::lag()            masks stats::lag()\n✖ randomForest::margin()  masks ggplot2::margin()\n✖ Matrix::pack()          masks tidyr::pack()\n✖ yardstick::spec()       masks readr::spec()\n✖ recipes::step()         masks stats::step()\n✖ Matrix::unpack()        masks tidyr::unpack()\n✖ recipes::update()       masks Matrix::update(), stats::update()\n• Use tidymodels_prefer() to resolve common conflicts.\n\niris_split &lt;- initial_split(iris, prop = 0.6)\niris_split\n\n&lt;Training/Testing/Total&gt;\n&lt;90/60/150&gt;\n\n\n\niris_split %&gt;%\n  training() %&gt;%\n  glimpse()\n\nRows: 90\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 4.8, 5.0, 5.8, 4.9, 5.2, 5.7, 4.8, 4.4, 5.4, 5.1, 6.8, 6.…\n$ Sepal.Width  &lt;dbl&gt; 3.4, 2.3, 2.7, 3.1, 2.7, 4.4, 3.0, 3.2, 3.4, 3.8, 3.2, 2.…\n$ Petal.Length &lt;dbl&gt; 1.6, 3.3, 3.9, 1.5, 3.9, 1.5, 1.4, 1.3, 1.5, 1.9, 5.9, 4.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 1.0, 1.2, 0.1, 1.4, 0.4, 0.3, 0.2, 0.4, 0.4, 2.3, 1.…\n$ Species      &lt;fct&gt; setosa, versicolor, versicolor, setosa, versicolor, setos…\n\n\n\niris_recipe &lt;- training(iris_split) %&gt;%\n  recipe(Species ~.) %&gt;%\n  step_corr(all_predictors()) %&gt;%\n  step_center(all_predictors(), -all_outcomes()) %&gt;%\n  step_scale(all_predictors(), -all_outcomes()) %&gt;%\n  prep()\n\niris_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\n\n\n\n\n\n── Training information \n\n\nTraining data contained 90 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Correlation filter on: Petal.Length | Trained\n\n\n• Centering for: Sepal.Length, Sepal.Width, Petal.Width | Trained\n\n\n• Scaling for: Sepal.Length, Sepal.Width, Petal.Width | Trained\n\n\n\niris_testing &lt;- iris_recipe %&gt;%\n  bake(testing(iris_split)) \n\nglimpse(iris_testing)\n\nRows: 60\nColumns: 4\n$ Sepal.Length &lt;dbl&gt; -1.38591131, -1.03289617, -1.73892646, -0.56220931, -1.26…\n$ Sepal.Width  &lt;dbl&gt; 0.4052525, 1.3344940, -0.2916786, 1.5668043, -0.0593682, …\n$ Petal.Width  &lt;dbl&gt; -1.3735973, -1.3735973, -1.3735973, -1.3735973, -1.509746…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\n\niris_training &lt;- juice(iris_recipe)\n\nglimpse(iris_training)\n\nRows: 90\nColumns: 4\n$ Sepal.Length &lt;dbl&gt; -1.26823960, -1.03289617, -0.09152245, -1.15056788, -0.79…\n$ Sepal.Width  &lt;dbl&gt; 0.8698732, -1.6855407, -0.7562993, 0.1729422, -0.7562993,…\n$ Petal.Width  &lt;dbl&gt; -1.37359728, -0.28440120, -0.01210218, -1.50974679, 0.260…\n$ Species      &lt;fct&gt; setosa, versicolor, versicolor, setosa, versicolor, setos…\n\n\n\niris_rf &lt;-  rand_forest(trees = 100, mode = \"classification\") %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  fit(Species ~ ., data = iris_training)\n\n\niris_rf %&gt;%\n  predict(iris_testing) %&gt;%\n  bind_cols(iris_testing) %&gt;%\n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_class  &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ Sepal.Length &lt;dbl&gt; -1.38591131, -1.03289617, -1.73892646, -0.56220931, -1.26…\n$ Sepal.Width  &lt;dbl&gt; 0.4052525, 1.3344940, -0.2916786, 1.5668043, -0.0593682, …\n$ Petal.Width  &lt;dbl&gt; -1.3735973, -1.3735973, -1.3735973, -1.3735973, -1.509746…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\n\niris_rf %&gt;%\n  predict(iris_testing) %&gt;%\n  bind_cols(iris_testing) %&gt;%\n  metrics(truth = Species, estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.917\n2 kap      multiclass     0.873\n\n\n\nLooking up vignettes"
  },
  {
    "objectID": "notebooks/session3.html#regularized-regression-with-glmnet",
    "href": "notebooks/session3.html#regularized-regression-with-glmnet",
    "title": "Modeling and Testing",
    "section": "Regularized regression with glmnet",
    "text": "Regularized regression with glmnet\n\nbrowseVignettes(\"glmnet\")"
  },
  {
    "objectID": "notebooks/session3.html#bayesian-statistics-with-stan-and-brms",
    "href": "notebooks/session3.html#bayesian-statistics-with-stan-and-brms",
    "title": "Modeling and Testing",
    "section": "Bayesian Statistics with Stan and brms",
    "text": "Bayesian Statistics with Stan and brms\n\nbrowseVignettes(\"brms\")"
  },
  {
    "objectID": "notebooks/session1.html",
    "href": "notebooks/session1.html",
    "title": "R Basics and Reproducibility Tools",
    "section": "",
    "text": "You can get output from R simply by typing math in the console:\n\n3 + 5\n\n[1] 8\n\n12 / 7\n\n[1] 1.714286\n\n\nHowever, to do useful and interesting things, we need to assign values to objects. To create an object, we need to give it a name followed by the assignment operator &lt;-, and the value we want to give it:\n\nweight_kg &lt;- 55\n\n&lt;- is the assignment operator. It assigns values on the right to objects on the left. So, after executing x &lt;- 3, the value of x is 3. The arrow can be read as 3 goes into x. For historical reasons, you can also use = for assignments.\nIn RStudio, typing Alt + - (push Alt at the same time as the - key) will write &lt;- in a single keystroke in a PC, while typing Option + - (push Option at the same time as the - key) does the same in a Mac.\n\n\nObjects can be given any name such as x, current_temperature, or subject_id. You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid, but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they are the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names (e.g., c, T, mean, data, df, weights). If in doubt, check the help to see if the name is already in use. It’s also best to avoid dots (.) within an object name as in my.dataset. There are many functions in R with dots in their names for historical reasons, but because dots have a special meaning in R (for methods) and other programming languages, it’s best to avoid them. It is also recommended to use nouns for object names, and verbs for function names. It’s important to be consistent in the styling of your code (where you put spaces, how you name objects, etc.). Using a consistent coding style makes your code clearer to read for your future self and your collaborators. In R, some popular style guides are Google’s, the tidyverse’s style and the Bioconductor style guide. The tidyverse’s is very comprehensive and may seem overwhelming at first. You can install the lintr package to automatically check for issues in the styling of your code.\n\nObjects vs. variables: What are known as objects in R are known as variables in many other programming languages. Depending on the context, object and variable can have drastically different meanings. However, in this lesson, the two words are used synonymously. For more information see here.\n\nWhen assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name:\n\nweight_kg &lt;- 55    # doesn't print anything\n(weight_kg &lt;- 55)  # but putting parenthesis around the call prints the value of `weight_kg`\n\n[1] 55\n\nweight_kg          # and so does typing the name of the object\n\n[1] 55\n\n\nNow that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg):\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change an object’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nThis means that assigning a value to one object does not change the values of other objects For example, let’s store the animal’s weight in pounds in a new object, weight_lb:\n\nweight_lb &lt;- 2.2 * weight_kg\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\n\n\n\n\n\n\n\nChallenge:\n\n\n\nWhat do you think is the current content of the object weight_lb? 126.5 or 220?"
  },
  {
    "objectID": "notebooks/session1.html#creating-objects-in-r",
    "href": "notebooks/session1.html#creating-objects-in-r",
    "title": "R Basics and Reproducibility Tools",
    "section": "",
    "text": "You can get output from R simply by typing math in the console:\n\n3 + 5\n\n[1] 8\n\n12 / 7\n\n[1] 1.714286\n\n\nHowever, to do useful and interesting things, we need to assign values to objects. To create an object, we need to give it a name followed by the assignment operator &lt;-, and the value we want to give it:\n\nweight_kg &lt;- 55\n\n&lt;- is the assignment operator. It assigns values on the right to objects on the left. So, after executing x &lt;- 3, the value of x is 3. The arrow can be read as 3 goes into x. For historical reasons, you can also use = for assignments.\nIn RStudio, typing Alt + - (push Alt at the same time as the - key) will write &lt;- in a single keystroke in a PC, while typing Option + - (push Option at the same time as the - key) does the same in a Mac.\n\n\nObjects can be given any name such as x, current_temperature, or subject_id. You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid, but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they are the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names (e.g., c, T, mean, data, df, weights). If in doubt, check the help to see if the name is already in use. It’s also best to avoid dots (.) within an object name as in my.dataset. There are many functions in R with dots in their names for historical reasons, but because dots have a special meaning in R (for methods) and other programming languages, it’s best to avoid them. It is also recommended to use nouns for object names, and verbs for function names. It’s important to be consistent in the styling of your code (where you put spaces, how you name objects, etc.). Using a consistent coding style makes your code clearer to read for your future self and your collaborators. In R, some popular style guides are Google’s, the tidyverse’s style and the Bioconductor style guide. The tidyverse’s is very comprehensive and may seem overwhelming at first. You can install the lintr package to automatically check for issues in the styling of your code.\n\nObjects vs. variables: What are known as objects in R are known as variables in many other programming languages. Depending on the context, object and variable can have drastically different meanings. However, in this lesson, the two words are used synonymously. For more information see here.\n\nWhen assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name:\n\nweight_kg &lt;- 55    # doesn't print anything\n(weight_kg &lt;- 55)  # but putting parenthesis around the call prints the value of `weight_kg`\n\n[1] 55\n\nweight_kg          # and so does typing the name of the object\n\n[1] 55\n\n\nNow that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg):\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change an object’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nThis means that assigning a value to one object does not change the values of other objects For example, let’s store the animal’s weight in pounds in a new object, weight_lb:\n\nweight_lb &lt;- 2.2 * weight_kg\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\n\n\n\n\n\n\n\nChallenge:\n\n\n\nWhat do you think is the current content of the object weight_lb? 126.5 or 220?"
  },
  {
    "objectID": "notebooks/session1.html#comments",
    "href": "notebooks/session1.html#comments",
    "title": "R Basics and Reproducibility Tools",
    "section": "Comments",
    "text": "Comments\nThe comment character in R is #, anything to the right of a # in a script will be ignored by R. It is useful to leave notes, and explanations in your scripts.\nRStudio makes it easy to comment or uncomment a paragraph: after selecting the lines you want to comment, press at the same time on your keyboard Ctrl + Shift + C. If you only want to comment out one line, you can put the cursor at any location of that line (i.e. no need to select the whole line), then press Ctrl + Shift + C.\n\n\n\n\n\n\nChallenge\n\n\n\nWhat are the values after each statement in the following?\n\nmass &lt;- 47.5            # mass?\nage  &lt;- 122             # age?\nmass &lt;- mass * 2.0      # mass?\nage  &lt;- age - 20        # age?\nmass_index &lt;- mass/age  # mass_index?"
  },
  {
    "objectID": "notebooks/session1.html#code-chunks",
    "href": "notebooks/session1.html#code-chunks",
    "title": "R Basics and Reproducibility Tools",
    "section": "Code Chunks",
    "text": "Code Chunks\nYou can start a and end code chunk using three back ticks “```”. To have a chunk run as R code, you need to assign the chunk using {r}. You can then specify options for the chunk inside the brackets, such as {r, eval=FALSE}. Code chunks have a lot of options, but some of the most important are label, eval, echo, and output.\n\nx &lt;- 5\nx\n\n[1] 5\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry changing these options in the first of the two chunks below and re-rendering the document. What do each of these arguments do? Pay attention to both chunk’s output.\n\ny = 8\nx\n\n[1] 5\n\ny\n\n[1] 8\n\nx &lt;- x + y \n\n\nx # Show the value of x\n\n[1] 13"
  },
  {
    "objectID": "notebooks/session1.html#markdown",
    "href": "notebooks/session1.html#markdown",
    "title": "R Basics and Reproducibility Tools",
    "section": "Markdown",
    "text": "Markdown\nMarkdown is a language used to quickly create formatted text. It’s great to know as it is used in R Markdown, Quarto, Jupyter, Github documents, and many other places. A pure markdown file has a .md file extension.\nYou can find a quick guide to markdown here, throughout the course we will see various things markdown can do in the readings and in-class materials."
  },
  {
    "objectID": "notebooks/session1.html#packages",
    "href": "notebooks/session1.html#packages",
    "title": "R Basics and Reproducibility Tools",
    "section": "Packages",
    "text": "Packages\nBracket subsetting is handy, but it can be cumbersome and difficult to read, especially for complicated operations.\nSome packages can greatly facilitate our task when we manipulate data. Packages in R are basically sets of additional functions that let you do more stuff. The functions we’ve been using so far, like str() or data.frame(), come built into R; Loading packages can give you access to other specific functions. Before you use a package for the first time you need to install it on your machine, and then you should import it in every subsequent R session when you need it.\n\nThe package dplyr provides powerful tools for data manipulation tasks. It is built to work directly with data frames, with many manipulation tasks optimised.\nAs we will see latter on, sometimes we want a data frame to be reshaped to be able to do some specific analyses or for visualisation. The package tidyr addresses this common problem of reshaping data and provides tools for manipulating data in a tidy way.\n\nTo learn more about dplyr and tidyr after the workshop, you may want to check out this handy data transformation with dplyr cheatsheet and this one about tidyr.\n\nThe tidyverse package is an “umbrella-package” that installs several useful packages for data analysis which work well together, such as tidyr, dplyr, ggplot2, tibble, etc. These packages help us to work and interact with the data. They allow us to do many things with your data, such as subsetting, transforming, visualising, etc.\n\nPackages can be installed using the install.packages command. This downloads and installs the package into your entire R installation. This means that you would not need to re-install the package for a new project.\n\ninstall.packages(\"tidyverse\")\n\nOnce a package is installed, it can be loaded using the library function. This tells R to use all of the functions inside the loaded package. library loads packages into your R session, and thus this line needs to be run each time you open R.\n\n## load the tidyverse packages, incl. dplyr\nlibrary(\"tidyverse\")"
  },
  {
    "objectID": "notebooks/session1.html#functions-and-their-arguments",
    "href": "notebooks/session1.html#functions-and-their-arguments",
    "title": "R Basics and Reproducibility Tools",
    "section": "Functions and their arguments",
    "text": "Functions and their arguments\nFunctions are “canned scripts” that automate more complicated sets of commands including operations assignments, etc. Many functions are predefined, or can be made available by importing R packages (more on that later). A function usually gets one or more inputs called arguments. Functions often (but not always) return a value. A typical example would be the function sqrt(). The input (the argument) must be a number, and the return value (in fact, the output) is the square root of that number. Executing a function (‘running it’) is called calling the function. An example of a function call is:\n\nb &lt;- sqrt(a)\n\nHere, the value of a is given to the sqrt() function, the sqrt() function calculates the square root, and returns the value which is then assigned to the object b. This function is very simple, because it takes just one argument.\nThe return ‘value’ of a function need not be numerical (like that of sqrt()), and it also does not need to be a single item: it can be a set of things, or even a dataset. We’ll see that when we read data files into R.\nArguments can be anything, not only numbers or filenames, but also other objects. Exactly what each argument means differs per function, and must be looked up in the documentation (see below). Some functions take arguments which may either be specified by the user, or, if left out, take on a default value: these are called options. Options are typically used to alter the way the function operates, such as whether it ignores ‘bad values’, or what symbol to use in a plot. However, if you want something specific, you can specify a value of your choice which will be used instead of the default.\nLet’s try a function that can take multiple arguments: round().\n\nround(3.14159)\n\n[1] 3\n\n\nHere, we’ve called round() with just one argument, 3.14159, and it has returned the value 3. That’s because the default is to round to the nearest whole number. If we want more digits we can see how to do that by getting information about the round function. We can use args(round) or look at the help for this function using ?round.\n\nargs(round)\n\nfunction (x, digits = 0) \nNULL\n\n\n\n?round\n\nWe see that if we want a different number of digits, we can type digits=2 or however many we want.\n\nround(3.14159, digits = 2)\n\n[1] 3.14\n\n\nIf you provide the arguments in the exact same order as they are defined you don’t have to name them:\n\nround(3.14159, 2)\n\n[1] 3.14\n\n\nAnd if you do name the arguments, you can switch their order:\n\nround(digits = 2, x = 3.14159)\n\n[1] 3.14\n\n\nIt’s good practice to put the non-optional arguments (like the number you’re rounding) first in your function call, and to specify the names of all optional arguments. If you don’t, someone reading your code might have to look up the definition of a function with unfamiliar arguments to understand what you’re doing. By specifying the name of the arguments you are also safeguarding against possible future changes in the function interface, which may potentially add new arguments in between the existing ones."
  },
  {
    "objectID": "notebooks/session1.html#vectors-and-data-types",
    "href": "notebooks/session1.html#vectors-and-data-types",
    "title": "R Basics and Reproducibility Tools",
    "section": "Vectors and data types",
    "text": "Vectors and data types\nA vector is the most common and basic data type in R, and is pretty much the workhorse of R. A vector is composed by a series of values, such as numbers or characters. We can assign a series of values to a vector using the c() function. For example we can create a vector of animal weights and assign it to a new object weight_g:\n\nweight_g &lt;- c(50, 60, 65, 82)\nweight_g\n\n[1] 50 60 65 82\n\n\nA vector can also contain characters:\n\nstates &lt;- c(\"CT\", \"MA\", \"NY\")\nstates\n\n[1] \"CT\" \"MA\" \"NY\"\n\n\nThe quotes around “CT”, “MA”, etc. are essential here. Without the quotes R will assume there are objects called CT, MA and NY. As these objects don’t exist in R’s memory, there will be an error message.\nThere are many functions that allow you to inspect the content of a vector. length() tells you how many elements are in a particular vector:\n\nlength(weight_g)\n\n[1] 4\n\nlength(states)\n\n[1] 3\n\n\nAn important feature of a vector, is that all of the elements are the same type of data. The function class() indicates the class (the type of element) of an object:\n\nclass(weight_g)\n\n[1] \"numeric\"\n\nclass(states)\n\n[1] \"character\"\n\n\nThe function str() provides an overview of the structure of an object and its elements. It is a useful function when working with large and complex objects:\n\nstr(weight_g)\n\n num [1:4] 50 60 65 82\n\nstr(states)\n\n chr [1:3] \"CT\" \"MA\" \"NY\"\n\n\nYou can use the c() function to add other elements to your vector:\n\nweight_g &lt;- c(weight_g, 90) # add to the end of the vector\nweight_g &lt;- c(30, weight_g) # add to the beginning of the vector\nweight_g\n\n[1] 30 50 60 65 82 90\n\n\nIn the first line, we take the original vector weight_g, add the value 90 to the end of it, and save the result back into weight_g. Then we add the value 30 to the beginning, again saving the result back into weight_g.\nWe can do this over and over again to grow a vector, or assemble a dataset. As we program, this may be useful to add results that we are collecting or calculating.\nAn atomic vector is the simplest R data type and is a linear vector of a single type. Above, we saw 2 of the 6 main atomic vector types that R uses: \"character\" and \"numeric\" (or \"double\"). These are the basic building blocks that all R objects are built from. The other 4 atomic vector types are:\n\n\"logical\" for TRUE and FALSE (the boolean data type)\n\"integer\" for integer numbers (e.g., 2L, the L indicates to R that it’s an integer)\n\"complex\" to represent complex numbers with real and imaginary parts (e.g., 1 + 4i) and that’s all we’re going to say about them\n\"raw\" for bitstreams that we won’t discuss further\n\nYou can check the type of your vector using the typeof() function and inputting your vector as the argument.\nVectors are one of the many data structures that R uses. Other important ones are lists (list), matrices (matrix), data frames (data.frame), factors (factor) and arrays (array).\n\n\n\n\n\n\nChallenge:\n\n\n\nWe’ve seen that atomic vectors can be of type character, numeric (or double), integer, and logical. But what happens if we try to mix these types in a single vector?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nR implicitly converts them to all be the same type\n\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\nWhat will happen in each of these examples? (hint: use class() to check the data type of your objects and type in their names to see what happens):\n\nnum_char &lt;- c(1, 2, 3, \"a\")\nnum_logical &lt;- c(1, 2, 3, TRUE, FALSE)\nchar_logical &lt;- c(\"a\", \"b\", \"c\", TRUE)\ntricky &lt;- c(1, 2, 3, \"4\")\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nclass(num_char)\n\n[1] \"character\"\n\nnum_char\n\n[1] \"1\" \"2\" \"3\" \"a\"\n\nclass(num_logical)\n\n[1] \"numeric\"\n\nnum_logical\n\n[1] 1 2 3 1 0\n\nclass(char_logical)\n\n[1] \"character\"\n\nchar_logical\n\n[1] \"a\"    \"b\"    \"c\"    \"TRUE\"\n\nclass(tricky)\n\n[1] \"character\"\n\ntricky\n\n[1] \"1\" \"2\" \"3\" \"4\"\n\n\n\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\nWhy do you think it happens?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nVectors can be of only one data type. R tries to convert (coerce) the content of this vector to find a common denominator that doesn’t lose any information.\n\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\nHow many values in combined_logical are \"TRUE\" (as a character) in the following example:\n\nnum_logical &lt;- c(1, 2, 3, TRUE)\nchar_logical &lt;- c(\"a\", \"b\", \"c\", TRUE)\ncombined_logical &lt;- c(num_logical, char_logical)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOnly one. There is no memory of past data types, and the coercion happens the first time the vector is evaluated. Therefore, the TRUE in num_logical gets converted into a 1 before it gets converted into \"1\" in combined_logical.\n\ncombined_logical\n\n[1] \"1\"    \"2\"    \"3\"    \"1\"    \"a\"    \"b\"    \"c\"    \"TRUE\""
  },
  {
    "objectID": "notebooks/session1.html#subsetting-vectors",
    "href": "notebooks/session1.html#subsetting-vectors",
    "title": "R Basics and Reproducibility Tools",
    "section": "Subsetting vectors",
    "text": "Subsetting vectors\nIf we want to extract one or several values from a vector, we must provide one or several indices in square brackets. For instance:\n\nstates &lt;- c(\"CT\", \"MA\", \"NY\", \"PA\")\nstates[2]\n\n[1] \"MA\"\n\nstates[c(3, 2)]\n\n[1] \"NY\" \"MA\"\n\n\nWe can also repeat the indices to create an object with more elements than the original one:\n\nmore_states &lt;- states[c(1, 2, 3, 2, 1, 4)]\nmore_states\n\n[1] \"CT\" \"MA\" \"NY\" \"MA\" \"CT\" \"PA\"\n\n\nR indices start at 1. Programming languages like Fortran, MATLAB, Julia, and R start counting at 1, because that’s what human beings typically do. Languages in the C family (including C++, Java, Perl, and Python) count from 0 because that’s simpler for computers to do.\nFinally, it is also possible to get all the elements of a vector except some specified elements using negative indices:\n\nstates ## all states\n\n[1] \"CT\" \"MA\" \"NY\" \"PA\"\n\nstates[-1] ## all but the first one\n\n[1] \"MA\" \"NY\" \"PA\"\n\nstates[-c(1, 3)] ## all but 1st/3rd ones\n\n[1] \"MA\" \"PA\"\n\nstates[c(-1, -3)] ## all but 1st/3rd ones\n\n[1] \"MA\" \"PA\""
  },
  {
    "objectID": "notebooks/session1.html#conditional-subsetting",
    "href": "notebooks/session1.html#conditional-subsetting",
    "title": "R Basics and Reproducibility Tools",
    "section": "Conditional subsetting",
    "text": "Conditional subsetting\nAnother common way of subsetting is by using a logical vector. TRUE will select the element with the same index, while FALSE will not:\n\nweight_g &lt;- c(21, 34, 39, 54, 55)\nweight_g[c(TRUE, FALSE, TRUE, TRUE, FALSE)]\n\n[1] 21 39 54\n\n\nTypically, these logical vectors are not typed by hand, but are the output of other functions or logical tests. For instance, if you wanted to select only the values above 50:\n\n## will return logicals with TRUE for the indices that meet\n## the condition\nweight_g &gt; 50\n\n[1] FALSE FALSE FALSE  TRUE  TRUE\n\n## so we can use this to select only the values above 50\nweight_g[weight_g &gt; 50]\n\n[1] 54 55\n\n\nYou can combine multiple tests using & (both conditions are true, AND) or | (at least one of the conditions is true, OR):\n\nweight_g[weight_g &lt; 30 | weight_g &gt; 50]\n\n[1] 21 54 55\n\nweight_g[weight_g &gt;= 30 & weight_g == 21]\n\nnumeric(0)\n\n\nHere, &lt; stands for “less than”, &gt; for “greater than”, &gt;= for “greater than or equal to”, and == for “equal to”. The double equal sign == is a test for numerical equality between the left and right hand sides, and should not be confused with the single = sign, which performs variable assignment (similar to &lt;-).\nA common task is to search for certain strings in a vector. One could use the “or” operator | to test for equality to multiple values, but this can quickly become tedious. The function %in% allows you to test if any of the elements of a search vector are found:\n\nstates &lt;- c(\"CT\", \"MA\", \"NY\", \"PA\")\nstates[states == \"MA\" | states == \"NY\"] # returns both MA and NY\n\n[1] \"MA\" \"NY\"\n\nstates %in% c(\"MA\", \"NY\", \"MA\", \"PA\", \"WI\")\n\n[1] FALSE  TRUE  TRUE  TRUE\n\nstates[states %in% c(\"MA\", \"NY\", \"MA\", \"PA\", \"WI\")]\n\n[1] \"MA\" \"NY\" \"PA\""
  },
  {
    "objectID": "notebooks/session1.html#missing-data",
    "href": "notebooks/session1.html#missing-data",
    "title": "R Basics and Reproducibility Tools",
    "section": "Missing data",
    "text": "Missing data\nAs R was designed to analyze datasets, it includes the concept of missing data (which is uncommon in other programming languages). Missing data are represented in vectors as NA.\nWhen doing operations on numbers, most functions will return NA if the data you are working with include missing values. This feature makes it harder to overlook the cases where you are dealing with missing data. You can add the argument na.rm = TRUE to calculate the result while ignoring the missing values.\n\nheights &lt;- c(2, 4, 4, NA, 6)\nmean(heights)\n\n[1] NA\n\nmax(heights)\n\n[1] NA\n\nmean(heights, na.rm = TRUE)\n\n[1] 4\n\nmax(heights, na.rm = TRUE)\n\n[1] 6\n\n\nIf your data include missing values, you may want to become familiar with the functions is.na(), na.omit(), and complete.cases(). See below for examples.\n\n## Extract those elements which are not missing values.\nheights[!is.na(heights)]\n\n[1] 2 4 4 6\n\n## Returns the object with incomplete cases removed.\n## The returned object is an atomic vector of type `\"numeric\"`\n## (or `\"double\"`).\nna.omit(heights)\n\n[1] 2 4 4 6\nattr(,\"na.action\")\n[1] 4\nattr(,\"class\")\n[1] \"omit\"\n\n## Extract those elements which are complete cases.\n## The returned object is an atomic vector of type `\"numeric\"`\n## (or `\"double\"`).\nheights[complete.cases(heights)]\n\n[1] 2 4 4 6\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\n\nUsing this vector of heights in inches, create a new vector with the NAs removed.\n\n\nheights &lt;- c(63, 69, 60, 65, NA, 68, 61, 70, 61, 59, 64, 69, 63, 63, NA, 72, 65, 64, 70, 63, 65)\n\n\nUse the function median() to calculate the median of the heights vector.\nUse R to figure out how many people in the set are taller than 67 inches.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nheights_no_na &lt;- heights[!is.na(heights)]\n## or\nheights_no_na &lt;- na.omit(heights)\n\n\nmedian(heights, na.rm = TRUE)\n\n[1] 64\n\n\n\nheights_above_67 &lt;- heights_no_na[heights_no_na &gt; 67]\nlength(heights_above_67)\n\n[1] 6"
  },
  {
    "objectID": "notebooks/session1.html#categorical-data",
    "href": "notebooks/session1.html#categorical-data",
    "title": "R Basics and Reproducibility Tools",
    "section": "Categorical Data",
    "text": "Categorical Data\n\nFactors\nSince factors are special vectors, the same rules for selecting values using indices apply.\n\nexpression &lt;- factor(c(\"high\",\"low\",\"low\",\"medium\",\"high\",\"medium\",\"medium\",\"low\",\"low\",\"low\"))\n\nIn this vector we can imagine gene expression data has been stored as 3 categories or levels: low, medium, and high.\nLet’s extract the values of the factor with high expression:\n\nexpression[expression == \"high\"]    ## This will only return those elements in the factor equal to \"high\"\n\n[1] high high\nLevels: high low medium\n\n\nUnder the hood, factors are stored as integer values in R. To view the integer assignments under the hood you can use str():\n\nstr(expression)\n\n Factor w/ 3 levels \"high\",\"low\",\"medium\": 1 2 2 3 1 3 3 2 2 2\n\n\nThe categories are referred to as “factor levels”. As we learned earlier, the levels in the expression factor were assigned integers alphabetically, with high=1, low=2, medium=3. However, it makes more sense for us if low=1, medium=2 and high=3. We can change the order of the categories by releveling the factor.\nTo relevel the categories, you can add the levels argument to the factor() function, and give it a vector with the categories listed in the required order:\n\nexpression &lt;- factor(expression, levels=c(\"low\", \"medium\", \"high\"))     # you can relevel a factor \n\nNow we have a releveled factor with low as the lowest or first category, medium as the second and high as the third. This is reflected in the way they are listed in the output of str(), as well as in the numbering of which category is where in the factor.\n\nstr(expression)\n\n Factor w/ 3 levels \"low\",\"medium\",..: 3 1 1 2 3 2 2 1 1 1\n\n\n\nNote: Releveling often becomes necessary when you need a specific category in a factor to be the “base” category, i.e. category that is equal to 1. One example would be if you need the “control” to be the “base” in a given RNA-seq experiment."
  },
  {
    "objectID": "notebooks/session1.html#preview-for-next-session",
    "href": "notebooks/session1.html#preview-for-next-session",
    "title": "R Basics and Reproducibility Tools",
    "section": "Preview for Next Session",
    "text": "Preview for Next Session\n\nData Frame\nA data.frame is the de facto data structure for most tabular data and what we use for statistics and plotting. A data.frame is similar to a matrix in that it’s a collection of vectors of the same length and each vector represents a column. However, in a dataframe each vector can be of a different data type (e.g., characters, integers, factors).\nWe can create a dataframe by bringing vectors together to form the columns. We do this using the data.frame() function, and giving the function the different vectors we would like to bind together. This function will only work for vectors of the same length.\n\n# Create a data frame and store it as a variable called 'df'\nages &lt;- c(12, 14, 14, 16, 12, 15)\nis_diabetic &lt;- c(TRUE, FALSE, FALSE, FALSE, TRUE, FALSE)\nsex &lt;- factor(c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\", \"Female\"))\ndf &lt;- data.frame(ages, is_diabetic, sex)\n\nWe can see that a new variable called df has been created in our Environment within a new section called Data. In the Environment, it specifies that df has 6 observations of 3 variables. What does that mean? In R, rows always come first, so it means that df has 6 rows and 3 columns. We can get additional information if we click on the blue circle with the white triangle in the middle next to df. It will display information about each of the columns in the data frame, giving information about what the data type is of each of the columns and the first few values of those columns.\nAnother handy feature in RStudio is that if we hover the cursor over the variable name in the Environment, df, it will turn into a pointing finger. If you click on df, it will open the data frame as it’s own tab next to the script editor. We can explore the table interactively within this window. To close, just click on the X on the tab.\nAs with any variable, we can print the values stored inside to the console if we type the variable’s name and run.\n\ndf\n\n  ages is_diabetic    sex\n1   12        TRUE   Male\n2   14       FALSE Female\n3   14       FALSE Female\n4   16       FALSE   Male\n5   12        TRUE   Male\n6   15       FALSE Female\n\n\n\n\nLists\nLists are a data structure in R that can be perhaps a bit daunting at first, but soon become amazingly useful. A list is a data structure that can hold any number of any types of other data structures.\nIf you have variables of different data structures you wish to combine, you can put all of those into one list object by using the list() function and placing all the items you wish to combine within parentheses:\n\nlist1 &lt;- list(ages, df, age)\n\nWe see list1 appear within the Data section of our environment as a list of 3 components or variables. If we click on the blue circle with a triangle in the middle, it’s not quite as interpretable as it was for data frames.\nEssentially, each component is preceded by a colon. The first colon give the expression vector, the second colon precedes the df data frame, with the dollar signs indicating the different columns, the last colon gives the single value, age.\nLet’s type list1 and print to the console by running it.\n\nlist1\n\n[[1]]\n[1] 12 14 14 16 12 15\n\n[[2]]\n  ages is_diabetic    sex\n1   12        TRUE   Male\n2   14       FALSE Female\n3   14       FALSE Female\n4   16       FALSE   Male\n5   12        TRUE   Male\n6   15       FALSE Female\n\n[[3]]\n[1] 102"
  },
  {
    "objectID": "notebooks/session1.html#there-are-three-components-corresponding-to-the-three-different-variables-we-passed-in-and-what-you-see-is-that-structure-of-each-is-retained.-each-component-of-a-list-is-referenced-based-on-the-number-position.",
    "href": "notebooks/session1.html#there-are-three-components-corresponding-to-the-three-different-variables-we-passed-in-and-what-you-see-is-that-structure-of-each-is-retained.-each-component-of-a-list-is-referenced-based-on-the-number-position.",
    "title": "R Basics and Reproducibility Tools",
    "section": "There are three components corresponding to the three different variables we passed in, and what you see is that structure of each is retained. Each component of a list is referenced based on the number position.",
    "text": "There are three components corresponding to the three different variables we passed in, and what you see is that structure of each is retained. Each component of a list is referenced based on the number position.\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Design and Analysis",
    "section": "",
    "text": "Materials for data analysis module of Research Design and Analysis, 2023."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Research Design and Analysis",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Licenses",
    "section": "",
    "text": "All Software Carpentry, Data Carpentry, and Library Carpentry instructional material is made available under the Creative Commons Attribution license. The following is a human-readable summary of (and not a substitute for) the full legal text of the CC BY 4.0 license.\nYou are free:\n\nto Share—copy and redistribute the material in any medium or format\nto Adapt—remix, transform, and build upon the material\n\nfor any purpose, even commercially.\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\nUnder the following terms:\n\nAttribution—You must give appropriate credit (mentioning that your work is derived from work that is Copyright © Software Carpentry and, where practical, linking to http://software-carpentry.org/), provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nNo additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. With the understanding that:\nNotices:\n\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material."
  },
  {
    "objectID": "LICENSE.html#instructional-material",
    "href": "LICENSE.html#instructional-material",
    "title": "Licenses",
    "section": "",
    "text": "All Software Carpentry, Data Carpentry, and Library Carpentry instructional material is made available under the Creative Commons Attribution license. The following is a human-readable summary of (and not a substitute for) the full legal text of the CC BY 4.0 license.\nYou are free:\n\nto Share—copy and redistribute the material in any medium or format\nto Adapt—remix, transform, and build upon the material\n\nfor any purpose, even commercially.\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\nUnder the following terms:\n\nAttribution—You must give appropriate credit (mentioning that your work is derived from work that is Copyright © Software Carpentry and, where practical, linking to http://software-carpentry.org/), provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nNo additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. With the understanding that:\nNotices:\n\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material."
  },
  {
    "objectID": "LICENSE.html#software",
    "href": "LICENSE.html#software",
    "title": "Licenses",
    "section": "Software",
    "text": "Software\nExcept where otherwise noted, the example programs and other software provided by Software Carpentry and Data Carpentry are made available under the OSI-approved MIT license.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "LICENSE.html#trademark",
    "href": "LICENSE.html#trademark",
    "title": "Licenses",
    "section": "Trademark",
    "text": "Trademark\n“Software Carpentry” and “Data Carpentry” and their respective logos are registered trademarks of Community Initiatives."
  },
  {
    "objectID": "notebooks/session2.html",
    "href": "notebooks/session2.html",
    "title": "Summarizing and Visualizing data",
    "section": "",
    "text": "We will begin with a mostly processed dataset from NHANES designed to examine the relationship between diabetes and dental caries in adolescents. Specifically, this dataset contains the 3346 adolescents recorded in NHANES from 2005 to 2010 with non-missing dental decay data.\nWe can load the data into R as a dataframe using the read.csv function.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DT)\n\nnhanes &lt;- read.csv(\"../data/nhanes_diabetes.csv\")\n\nThis statement doesn’t produce any output because, as you might recall, assignments don’t display anything. If we want to check that our data has been loaded, we can see the contents of the data frame by typing its name:\n\nhead(nhanes)\n\n  X  SEQN RIDAGEYR RIAGENDR           RIDRETH1                       DMDBORN\n1 1 31127        0     Male Non-Hispanic White Born in 50 US States or Washi\n2 2 31128       11   Female Non-Hispanic Black Born in 50 US States or Washi\n3 3 31129       15     Male Non-Hispanic Black Born in 50 US States or Washi\n4 4 31130       85   Female Non-Hispanic White Born in 50 US States or Washi\n5 5 31131       44   Female Non-Hispanic Black Born in 50 US States or Washi\n6 6 31132       70     Male Non-Hispanic White Born in 50 US States or Washi\n  INDFMPIR SDMVPSU SDMVSTRA  WTINT2YR  WTMEC2YR OHXDECAY OHXREST LBXGLU\n1     0.75       2       44  6434.950  6571.396       NA      NA     NA\n2     0.77       1       52  9081.701  8987.042     TRUE   FALSE     NA\n3     2.71       1       51  5316.895  5586.719    FALSE   FALSE     NA\n4     1.99       2       46 29960.840 34030.995     TRUE    TRUE     NA\n5     4.65       1       48 26457.708 26770.585    FALSE    TRUE     90\n6     5.00       2       52 32961.510 35315.539    FALSE    TRUE    157\n  WTSAF2YR LBXGH BMXBMI Begin.Year EndYear age.cat        LBXGH.cat\n1       NA    NA     NA       2005    2006    &lt;NA&gt;             &lt;NA&gt;\n2       NA    NA  17.45       2005    2006    &lt;NA&gt;             &lt;NA&gt;\n3       NA   5.2  26.53       2005    2006   13-15            &lt;5.7%\n4     0.00    NA     NA       2005    2006     19+             &lt;NA&gt;\n5 67556.81   6.0  30.90       2005    2006     19+ &gt;=5.7% and &lt;6.5%\n6 80193.96   7.1  24.74       2005    2006     19+ &gt;=5.7% and &lt;6.5%\n  RIDAGEYR.cat  LBXGLU.cat BMXBMI.cat INDFMPIR.cat   DMDBORN.cat dental.caries\n1         &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;           &lt;1 Within the US            NA\n2         &lt;NA&gt;        &lt;NA&gt;     Normal           &lt;1 Within the US          TRUE\n3        13-15        &lt;NA&gt; Overweight          &gt;=1 Within the US         FALSE\n4         &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;          &gt;=1 Within the US          TRUE\n5         &lt;NA&gt;  &lt;100 mg/dl      Obese          &gt;=1 Within the US          TRUE\n6         &lt;NA&gt; &gt;=126 mg/dl     Normal          &gt;=1 Within the US          TRUE\n     diabetes\n1        &lt;NA&gt;\n2        &lt;NA&gt;\n3 nondiabetic\n4        &lt;NA&gt;\n5 prediabetic\n6    diabetic\n\n\nData frames are the de facto data structure for most tabular data, and what we use for statistics and plotting.\nA data frame can be created by hand, but most commonly they are generated by the functions read.csv() or read.table(); in other words, when importing spreadsheets from your hard drive (or the web).\nA data frame is the representation of data in the format of a table where the columns are vectors that all have the same length. Because columns are vectors, each column must contain a single type of data (e.g., characters, integers, factors).\nWe can get an overview with summary.\n\nsummary(nhanes)\n\n       X              SEQN          RIDAGEYR       RIAGENDR        \n Min.   :    1   Min.   :31127   Min.   : 0.00   Length:31034      \n 1st Qu.: 7759   1st Qu.:38885   1st Qu.: 9.00   Class :character  \n Median :15518   Median :46644   Median :25.00   Mode  :character  \n Mean   :15518   Mean   :46644   Mean   :31.18                     \n 3rd Qu.:23276   3rd Qu.:54402   3rd Qu.:51.00                     \n Max.   :31034   Max.   :62160   Max.   :85.00                     \n                                                                   \n   RIDRETH1           DMDBORN             INDFMPIR        SDMVPSU     \n Length:31034       Length:31034       Min.   :0.000   Min.   :1.000  \n Class :character   Class :character   1st Qu.:0.960   1st Qu.:1.000  \n Mode  :character   Mode  :character   Median :1.820   Median :2.000  \n                                       Mean   :2.295   Mean   :1.522  \n                                       3rd Qu.:3.598   3rd Qu.:2.000  \n                                       Max.   :5.000   Max.   :3.000  \n                                       NA's   :2424                   \n    SDMVSTRA        WTINT2YR         WTMEC2YR       OHXDECAY      \n Min.   :44.00   Min.   :  1225   Min.   :     0   Mode :logical  \n 1st Qu.:55.00   1st Qu.: 10194   1st Qu.:  9954   FALSE:14316    \n Median :65.00   Median : 19871   Median : 19584   TRUE :4079     \n Mean   :66.04   Mean   : 28701   Mean   : 28701   NA's :12639    \n 3rd Qu.:78.00   3rd Qu.: 36751   3rd Qu.: 37143                  \n Max.   :89.00   Max.   :186296   Max.   :192771                  \n                                                                  \n  OHXREST            LBXGLU         WTSAF2YR          LBXGH       \n Mode :logical   Min.   : 36.0   Min.   :     0   Min.   : 2.000  \n FALSE:7590      1st Qu.: 91.0   1st Qu.: 21695   1st Qu.: 5.100  \n TRUE :10805     Median : 98.0   Median : 51495   Median : 5.400  \n NA's :12639     Mean   :105.4   Mean   : 72640   Mean   : 5.587  \n                 3rd Qu.:107.0   3rd Qu.: 98156   3rd Qu.: 5.700  \n                 Max.   :584.0   Max.   :404656   Max.   :16.400  \n                 NA's   :21410   NA's   :20786    NA's   :11184   \n     BMXBMI         Begin.Year      EndYear       age.cat         \n Min.   : 11.74   Min.   :2005   Min.   :2006   Length:31034      \n 1st Qu.: 19.76   1st Qu.:2005   1st Qu.:2006   Class :character  \n Median : 24.94   Median :2007   Median :2008   Mode  :character  \n Mean   : 25.58   Mean   :2007   Mean   :2008                     \n 3rd Qu.: 30.01   3rd Qu.:2009   3rd Qu.:2010                     \n Max.   :130.21   Max.   :2009   Max.   :2010                     \n NA's   :3812                                                     \n  LBXGH.cat         RIDAGEYR.cat        LBXGLU.cat         BMXBMI.cat       \n Length:31034       Length:31034       Length:31034       Length:31034      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n INDFMPIR.cat       DMDBORN.cat        dental.caries     diabetes        \n Length:31034       Length:31034       Mode :logical   Length:31034      \n Class :character   Class :character   FALSE:5820      Class :character  \n Mode  :character   Mode  :character   TRUE :12575     Mode  :character  \n                                       NA's :12639"
  },
  {
    "objectID": "notebooks/session2.html#dataframes",
    "href": "notebooks/session2.html#dataframes",
    "title": "Summarizing and Visualizing data",
    "section": "",
    "text": "We will begin with a mostly processed dataset from NHANES designed to examine the relationship between diabetes and dental caries in adolescents. Specifically, this dataset contains the 3346 adolescents recorded in NHANES from 2005 to 2010 with non-missing dental decay data.\nWe can load the data into R as a dataframe using the read.csv function.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DT)\n\nnhanes &lt;- read.csv(\"../data/nhanes_diabetes.csv\")\n\nThis statement doesn’t produce any output because, as you might recall, assignments don’t display anything. If we want to check that our data has been loaded, we can see the contents of the data frame by typing its name:\n\nhead(nhanes)\n\n  X  SEQN RIDAGEYR RIAGENDR           RIDRETH1                       DMDBORN\n1 1 31127        0     Male Non-Hispanic White Born in 50 US States or Washi\n2 2 31128       11   Female Non-Hispanic Black Born in 50 US States or Washi\n3 3 31129       15     Male Non-Hispanic Black Born in 50 US States or Washi\n4 4 31130       85   Female Non-Hispanic White Born in 50 US States or Washi\n5 5 31131       44   Female Non-Hispanic Black Born in 50 US States or Washi\n6 6 31132       70     Male Non-Hispanic White Born in 50 US States or Washi\n  INDFMPIR SDMVPSU SDMVSTRA  WTINT2YR  WTMEC2YR OHXDECAY OHXREST LBXGLU\n1     0.75       2       44  6434.950  6571.396       NA      NA     NA\n2     0.77       1       52  9081.701  8987.042     TRUE   FALSE     NA\n3     2.71       1       51  5316.895  5586.719    FALSE   FALSE     NA\n4     1.99       2       46 29960.840 34030.995     TRUE    TRUE     NA\n5     4.65       1       48 26457.708 26770.585    FALSE    TRUE     90\n6     5.00       2       52 32961.510 35315.539    FALSE    TRUE    157\n  WTSAF2YR LBXGH BMXBMI Begin.Year EndYear age.cat        LBXGH.cat\n1       NA    NA     NA       2005    2006    &lt;NA&gt;             &lt;NA&gt;\n2       NA    NA  17.45       2005    2006    &lt;NA&gt;             &lt;NA&gt;\n3       NA   5.2  26.53       2005    2006   13-15            &lt;5.7%\n4     0.00    NA     NA       2005    2006     19+             &lt;NA&gt;\n5 67556.81   6.0  30.90       2005    2006     19+ &gt;=5.7% and &lt;6.5%\n6 80193.96   7.1  24.74       2005    2006     19+ &gt;=5.7% and &lt;6.5%\n  RIDAGEYR.cat  LBXGLU.cat BMXBMI.cat INDFMPIR.cat   DMDBORN.cat dental.caries\n1         &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;           &lt;1 Within the US            NA\n2         &lt;NA&gt;        &lt;NA&gt;     Normal           &lt;1 Within the US          TRUE\n3        13-15        &lt;NA&gt; Overweight          &gt;=1 Within the US         FALSE\n4         &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;          &gt;=1 Within the US          TRUE\n5         &lt;NA&gt;  &lt;100 mg/dl      Obese          &gt;=1 Within the US          TRUE\n6         &lt;NA&gt; &gt;=126 mg/dl     Normal          &gt;=1 Within the US          TRUE\n     diabetes\n1        &lt;NA&gt;\n2        &lt;NA&gt;\n3 nondiabetic\n4        &lt;NA&gt;\n5 prediabetic\n6    diabetic\n\n\nData frames are the de facto data structure for most tabular data, and what we use for statistics and plotting.\nA data frame can be created by hand, but most commonly they are generated by the functions read.csv() or read.table(); in other words, when importing spreadsheets from your hard drive (or the web).\nA data frame is the representation of data in the format of a table where the columns are vectors that all have the same length. Because columns are vectors, each column must contain a single type of data (e.g., characters, integers, factors).\nWe can get an overview with summary.\n\nsummary(nhanes)\n\n       X              SEQN          RIDAGEYR       RIAGENDR        \n Min.   :    1   Min.   :31127   Min.   : 0.00   Length:31034      \n 1st Qu.: 7759   1st Qu.:38885   1st Qu.: 9.00   Class :character  \n Median :15518   Median :46644   Median :25.00   Mode  :character  \n Mean   :15518   Mean   :46644   Mean   :31.18                     \n 3rd Qu.:23276   3rd Qu.:54402   3rd Qu.:51.00                     \n Max.   :31034   Max.   :62160   Max.   :85.00                     \n                                                                   \n   RIDRETH1           DMDBORN             INDFMPIR        SDMVPSU     \n Length:31034       Length:31034       Min.   :0.000   Min.   :1.000  \n Class :character   Class :character   1st Qu.:0.960   1st Qu.:1.000  \n Mode  :character   Mode  :character   Median :1.820   Median :2.000  \n                                       Mean   :2.295   Mean   :1.522  \n                                       3rd Qu.:3.598   3rd Qu.:2.000  \n                                       Max.   :5.000   Max.   :3.000  \n                                       NA's   :2424                   \n    SDMVSTRA        WTINT2YR         WTMEC2YR       OHXDECAY      \n Min.   :44.00   Min.   :  1225   Min.   :     0   Mode :logical  \n 1st Qu.:55.00   1st Qu.: 10194   1st Qu.:  9954   FALSE:14316    \n Median :65.00   Median : 19871   Median : 19584   TRUE :4079     \n Mean   :66.04   Mean   : 28701   Mean   : 28701   NA's :12639    \n 3rd Qu.:78.00   3rd Qu.: 36751   3rd Qu.: 37143                  \n Max.   :89.00   Max.   :186296   Max.   :192771                  \n                                                                  \n  OHXREST            LBXGLU         WTSAF2YR          LBXGH       \n Mode :logical   Min.   : 36.0   Min.   :     0   Min.   : 2.000  \n FALSE:7590      1st Qu.: 91.0   1st Qu.: 21695   1st Qu.: 5.100  \n TRUE :10805     Median : 98.0   Median : 51495   Median : 5.400  \n NA's :12639     Mean   :105.4   Mean   : 72640   Mean   : 5.587  \n                 3rd Qu.:107.0   3rd Qu.: 98156   3rd Qu.: 5.700  \n                 Max.   :584.0   Max.   :404656   Max.   :16.400  \n                 NA's   :21410   NA's   :20786    NA's   :11184   \n     BMXBMI         Begin.Year      EndYear       age.cat         \n Min.   : 11.74   Min.   :2005   Min.   :2006   Length:31034      \n 1st Qu.: 19.76   1st Qu.:2005   1st Qu.:2006   Class :character  \n Median : 24.94   Median :2007   Median :2008   Mode  :character  \n Mean   : 25.58   Mean   :2007   Mean   :2008                     \n 3rd Qu.: 30.01   3rd Qu.:2009   3rd Qu.:2010                     \n Max.   :130.21   Max.   :2009   Max.   :2010                     \n NA's   :3812                                                     \n  LBXGH.cat         RIDAGEYR.cat        LBXGLU.cat         BMXBMI.cat       \n Length:31034       Length:31034       Length:31034       Length:31034      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n INDFMPIR.cat       DMDBORN.cat        dental.caries     diabetes        \n Length:31034       Length:31034       Mode :logical   Length:31034      \n Class :character   Class :character   FALSE:5820      Class :character  \n Mode  :character   Mode  :character   TRUE :12575     Mode  :character  \n                                       NA's :12639"
  },
  {
    "objectID": "notebooks/session2.html#inspecting-data.frame-objects",
    "href": "notebooks/session2.html#inspecting-data.frame-objects",
    "title": "Summarizing and Visualizing data",
    "section": "Inspecting data.frame Objects",
    "text": "Inspecting data.frame Objects\nWe already saw how the functions head() and str() can be useful to check the content and the structure of a data frame. Here is a non-exhaustive list of functions to get a sense of the content/structure of the data. Let’s try them out!\nSize:\n\ndim(nhanes) - returns a vector with the number of rows as the first element, and the number of columns as the second element (the dimensions of the object).\nnrow(nhanes) - returns the number of rows.\nncol(nhanes) - returns the number of columns.\n\nContent:\n\nhead(nhanes) - shows the first 6 rows.\ntail(nhanes) - shows the last 6 rows.\n\nNames:\n\nnames(nhanes) - returns the column names (synonym of colnames() for data.frame objects).\nrownames(nhanes) - returns the row names.\n\nSummary:\n\nstr(nhanes) - structure of the object and information about the class, length and content of each column.\nsummary(nhanes) - summary statistics for each column.\n\nNote: most of these functions are “generic”, they can be used on other types of objects besides data.frame."
  },
  {
    "objectID": "notebooks/session2.html#loading-data-with-tidyverse",
    "href": "notebooks/session2.html#loading-data-with-tidyverse",
    "title": "Summarizing and Visualizing data",
    "section": "Loading data with tidyverse",
    "text": "Loading data with tidyverse\nYou can find a data transformation cheatsheet here.\nInstead of read.csv(), we will read in our data using the read_csv() function (notice the _ instead of the .), from the tidyverse package readr.\nLet’s also load in the metadata for this dataset.\n\nnhanes &lt;- read_csv(\"../data/nhanes_diabetes.csv\")\nnhanes_metadata &lt;- read_csv(\"../data/nhanes_diabetes_metadata.csv\")\n\n\n## view the data\ndatatable(head(nhanes, 100))\n\n\n\n\n\n\n\n## view the data\ndatatable(nhanes_metadata)\n\n\n\n\n\n\nNotice that the class of the data is now referred to as a “tibble”.\nTibbles tweak some of the behaviors of the data frame objects we introduced in the previously. The data structure is very similar to a data frame. For our purposes the only differences are that:\n\nIt displays the data type of each column under its name. Note that &lt;dbl&gt; is a data type defined to hold numeric values with decimal points.\nIt only prints the first few rows of data and only as many columns as fit on one screen.\n\nWe are now going to learn some of the most common dplyr functions:\n\nselect(): subset columns\nfilter(): subset rows on conditions\nmutate(): create new columns by using information from other columns\ngroup_by() and summarise(): create summary statistics on grouped data\narrange(): sort results\ncount(): count discrete values"
  },
  {
    "objectID": "notebooks/session2.html#selecting-columns-and-filtering-rows",
    "href": "notebooks/session2.html#selecting-columns-and-filtering-rows",
    "title": "Summarizing and Visualizing data",
    "section": "Selecting columns and filtering rows",
    "text": "Selecting columns and filtering rows\nTo select columns of a data frame, use select(). The first argument to this function is the data frame (nhanes), and the subsequent arguments are the columns to keep.\n\nselect(nhanes, RIAGENDR, DMDBORN, LBXGH, BMXBMI)\n\n# A tibble: 31,034 × 4\n   RIAGENDR DMDBORN                       LBXGH BMXBMI\n   &lt;chr&gt;    &lt;chr&gt;                         &lt;dbl&gt;  &lt;dbl&gt;\n 1 Male     Born in 50 US States or Washi  NA     NA  \n 2 Female   Born in 50 US States or Washi  NA     17.4\n 3 Male     Born in 50 US States or Washi   5.2   26.5\n 4 Female   Born in 50 US States or Washi  NA     NA  \n 5 Female   Born in 50 US States or Washi   6     30.9\n 6 Male     Born in 50 US States or Washi   7.1   24.7\n 7 Female   Born in 50 US States or Washi   4.7   16.8\n 8 Male     Born in 50 US States or Washi   5.9   30.6\n 9 Male     Born in 50 US States or Washi  NA     NA  \n10 Female   Born in 50 US States or Washi  NA     NA  \n# ℹ 31,024 more rows\n\n\nTo select all columns except certain ones, put a “-” in front of the variable to exclude it.\n\nselect(nhanes, -RIAGENDR, -DMDBORN, -LBXGH, -BMXBMI)\n\n# A tibble: 31,034 × 24\n    ...1  SEQN RIDAGEYR RIDRETH1     INDFMPIR SDMVPSU SDMVSTRA WTINT2YR WTMEC2YR\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 31127        0 Non-Hispani…     0.75       2       44    6435.    6571.\n 2     2 31128       11 Non-Hispani…     0.77       1       52    9082.    8987.\n 3     3 31129       15 Non-Hispani…     2.71       1       51    5317.    5587.\n 4     4 31130       85 Non-Hispani…     1.99       2       46   29961.   34031.\n 5     5 31131       44 Non-Hispani…     4.65       1       48   26458.   26771.\n 6     6 31132       70 Non-Hispani…     5          2       52   32962.   35316.\n 7     7 31133       16 Non-Hispani…     5          1       51    5635.    5921.\n 8     8 31134       73 Non-Hispani…    NA          2       48   43719.   44231.\n 9     9 31135        0 Other Race …     5          2       52   15712.   16144.\n10    10 31136       41 Non-Hispani…     3.57       2       51   26488.       0 \n# ℹ 31,024 more rows\n# ℹ 15 more variables: OHXDECAY &lt;lgl&gt;, OHXREST &lt;lgl&gt;, LBXGLU &lt;dbl&gt;,\n#   WTSAF2YR &lt;dbl&gt;, Begin.Year &lt;dbl&gt;, EndYear &lt;dbl&gt;, age.cat &lt;chr&gt;,\n#   LBXGH.cat &lt;chr&gt;, RIDAGEYR.cat &lt;chr&gt;, LBXGLU.cat &lt;chr&gt;, BMXBMI.cat &lt;chr&gt;,\n#   INDFMPIR.cat &lt;chr&gt;, DMDBORN.cat &lt;chr&gt;, dental.caries &lt;lgl&gt;, diabetes &lt;chr&gt;\n\n\nThis will select all the variables in nhanes except those in the statement.\nTo choose rows based on a specific criteria, use filter():\n\nfilter(nhanes, RIAGENDR == \"Male\")\n\n# A tibble: 15,401 × 28\n    ...1  SEQN RIDAGEYR RIAGENDR RIDRETH1      DMDBORN INDFMPIR SDMVPSU SDMVSTRA\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 31127        0 Male     Non-Hispanic… Born i…     0.75       2       44\n 2     3 31129       15 Male     Non-Hispanic… Born i…     2.71       1       51\n 3     6 31132       70 Male     Non-Hispanic… Born i…     5          2       52\n 4     8 31134       73 Male     Non-Hispanic… Born i…    NA          2       48\n 5     9 31135        0 Male     Other Race -… Born i…     5          2       52\n 6    12 31138        3 Male     Mexican Amer… Born i…     2.13       2       57\n 7    15 31141       16 Male     Mexican Amer… Born i…     5          2       48\n 8    16 31142        7 Male     Mexican Amer… Born i…     0.51       2       54\n 9    17 31143       19 Male     Non-Hispanic… Born i…     5          1       44\n10    18 31144       21 Male     Other Hispan… Born i…     0.46       1       45\n# ℹ 15,391 more rows\n# ℹ 19 more variables: WTINT2YR &lt;dbl&gt;, WTMEC2YR &lt;dbl&gt;, OHXDECAY &lt;lgl&gt;,\n#   OHXREST &lt;lgl&gt;, LBXGLU &lt;dbl&gt;, WTSAF2YR &lt;dbl&gt;, LBXGH &lt;dbl&gt;, BMXBMI &lt;dbl&gt;,\n#   Begin.Year &lt;dbl&gt;, EndYear &lt;dbl&gt;, age.cat &lt;chr&gt;, LBXGH.cat &lt;chr&gt;,\n#   RIDAGEYR.cat &lt;chr&gt;, LBXGLU.cat &lt;chr&gt;, BMXBMI.cat &lt;chr&gt;, INDFMPIR.cat &lt;chr&gt;,\n#   DMDBORN.cat &lt;chr&gt;, dental.caries &lt;lgl&gt;, diabetes &lt;chr&gt;\n\nfilter(nhanes, RIAGENDR == \"Male\" & LBXGLU &gt; 80)\n\n# A tibble: 4,646 × 28\n    ...1  SEQN RIDAGEYR RIAGENDR RIDRETH1      DMDBORN INDFMPIR SDMVPSU SDMVSTRA\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1     6 31132       70 Male     Non-Hispanic… Born i…     5          2       52\n 2     8 31134       73 Male     Non-Hispanic… Born i…    NA          2       48\n 3    24 31150       79 Male     Non-Hispanic… Born i…     1.22       2       44\n 4    29 31155       38 Male     Other Race -… Born E…     3.5        2       48\n 5    32 31158       71 Male     Non-Hispanic… Born i…     3.71       2       50\n 6    41 31167       37 Male     Non-Hispanic… Born i…     4.16       1       44\n 7    67 31193       51 Male     Non-Hispanic… Born i…     1.01       1       49\n 8    69 31195       73 Male     Non-Hispanic… Born i…     5          2       48\n 9    97 31223       61 Male     Mexican Amer… Born i…     1.68       2       58\n10   101 31227       64 Male     Non-Hispanic… Born i…     5          2       48\n# ℹ 4,636 more rows\n# ℹ 19 more variables: WTINT2YR &lt;dbl&gt;, WTMEC2YR &lt;dbl&gt;, OHXDECAY &lt;lgl&gt;,\n#   OHXREST &lt;lgl&gt;, LBXGLU &lt;dbl&gt;, WTSAF2YR &lt;dbl&gt;, LBXGH &lt;dbl&gt;, BMXBMI &lt;dbl&gt;,\n#   Begin.Year &lt;dbl&gt;, EndYear &lt;dbl&gt;, age.cat &lt;chr&gt;, LBXGH.cat &lt;chr&gt;,\n#   RIDAGEYR.cat &lt;chr&gt;, LBXGLU.cat &lt;chr&gt;, BMXBMI.cat &lt;chr&gt;, INDFMPIR.cat &lt;chr&gt;,\n#   DMDBORN.cat &lt;chr&gt;, dental.caries &lt;lgl&gt;, diabetes &lt;chr&gt;\n\nfilter(nhanes, RIAGENDR == \"Male\" & !is.na(LBXGH))\n\n# A tibble: 9,751 × 28\n    ...1  SEQN RIDAGEYR RIAGENDR RIDRETH1      DMDBORN INDFMPIR SDMVPSU SDMVSTRA\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1     3 31129       15 Male     Non-Hispanic… Born i…     2.71       1       51\n 2     6 31132       70 Male     Non-Hispanic… Born i…     5          2       52\n 3     8 31134       73 Male     Non-Hispanic… Born i…    NA          2       48\n 4    17 31143       19 Male     Non-Hispanic… Born i…     5          1       44\n 5    18 31144       21 Male     Other Hispan… Born i…     0.46       1       45\n 6    24 31150       79 Male     Non-Hispanic… Born i…     1.22       2       44\n 7    28 31154       62 Male     Non-Hispanic… Born i…     1.56       1       45\n 8    29 31155       38 Male     Other Race -… Born E…     3.5        2       48\n 9    32 31158       71 Male     Non-Hispanic… Born i…     3.71       2       50\n10    37 31163       17 Male     Non-Hispanic… Born i…     2.23       1       56\n# ℹ 9,741 more rows\n# ℹ 19 more variables: WTINT2YR &lt;dbl&gt;, WTMEC2YR &lt;dbl&gt;, OHXDECAY &lt;lgl&gt;,\n#   OHXREST &lt;lgl&gt;, LBXGLU &lt;dbl&gt;, WTSAF2YR &lt;dbl&gt;, LBXGH &lt;dbl&gt;, BMXBMI &lt;dbl&gt;,\n#   Begin.Year &lt;dbl&gt;, EndYear &lt;dbl&gt;, age.cat &lt;chr&gt;, LBXGH.cat &lt;chr&gt;,\n#   RIDAGEYR.cat &lt;chr&gt;, LBXGLU.cat &lt;chr&gt;, BMXBMI.cat &lt;chr&gt;, INDFMPIR.cat &lt;chr&gt;,\n#   DMDBORN.cat &lt;chr&gt;, dental.caries &lt;lgl&gt;, diabetes &lt;chr&gt;"
  },
  {
    "objectID": "notebooks/session2.html#pipes",
    "href": "notebooks/session2.html#pipes",
    "title": "Summarizing and Visualizing data",
    "section": "Pipes",
    "text": "Pipes\nWhat if you want to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, or pipes.\nWith intermediate steps, you create a temporary data frame and use that as input to the next function, like this:\n\nnhanes2 &lt;- filter(nhanes, RIAGENDR == \"Male\")\nnhanes3 &lt;- select(nhanes2, RIAGENDR, RIDAGEYR, dental.caries, LBXGH)\nnhanes3\n\n# A tibble: 15,401 × 4\n   RIAGENDR RIDAGEYR dental.caries LBXGH\n   &lt;chr&gt;       &lt;dbl&gt; &lt;lgl&gt;         &lt;dbl&gt;\n 1 Male            0 NA             NA  \n 2 Male           15 FALSE           5.2\n 3 Male           70 TRUE            7.1\n 4 Male           73 TRUE            5.9\n 5 Male            0 NA             NA  \n 6 Male            3 NA             NA  \n 7 Male           16 FALSE          NA  \n 8 Male            7 FALSE          NA  \n 9 Male           19 FALSE           4.7\n10 Male           21 FALSE           4.8\n# ℹ 15,391 more rows\n\n\nThis is readable, but can clutter up your workspace with lots of intermediate objects that you have to name individually. With multiple steps, that can be hard to keep track of.\nYou can also nest functions (i.e. one function inside of another), like this:\n\nnhanes3 &lt;- select(filter(nhanes, RIAGENDR == \"Male\"), RIAGENDR, RIDAGEYR, dental.caries, LBXGH)\nnhanes3\n\n# A tibble: 15,401 × 4\n   RIAGENDR RIDAGEYR dental.caries LBXGH\n   &lt;chr&gt;       &lt;dbl&gt; &lt;lgl&gt;         &lt;dbl&gt;\n 1 Male            0 NA             NA  \n 2 Male           15 FALSE           5.2\n 3 Male           70 TRUE            7.1\n 4 Male           73 TRUE            5.9\n 5 Male            0 NA             NA  \n 6 Male            3 NA             NA  \n 7 Male           16 FALSE          NA  \n 8 Male            7 FALSE          NA  \n 9 Male           19 FALSE           4.7\n10 Male           21 FALSE           4.8\n# ℹ 15,391 more rows\n\n\nThis is handy, but can be difficult to read if too many functions are nested, as R evaluates the expression from the inside out (in this case, filtering, then selecting).\nThe last option, pipes, are a recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset.\nPipes in R look like |&gt; (made available via the magrittr package) or |&gt; (through base R). If you use RStudio, you can type the pipe with Ctrl + Shift + M if you have a PC or Cmd + Shift + M if you have a Mac.\nIn the above code, we use the pipe to send the nhanes dataset first through filter() to keep rows where RIAGENDR is Male, then through select() to keep only the selected columns.\nThe pipe |&gt; takes the object on its left and passes it directly as the first argument to the function on its right, we don’t need to explicitly include the data frame as an argument to the filter() and select() functions any more.\n\nnhanes |&gt;\n  filter(RIAGENDR == \"Male\") |&gt;\n  select(RIAGENDR, RIDAGEYR, dental.caries, LBXGH)\n\n# A tibble: 15,401 × 4\n   RIAGENDR RIDAGEYR dental.caries LBXGH\n   &lt;chr&gt;       &lt;dbl&gt; &lt;lgl&gt;         &lt;dbl&gt;\n 1 Male            0 NA             NA  \n 2 Male           15 FALSE           5.2\n 3 Male           70 TRUE            7.1\n 4 Male           73 TRUE            5.9\n 5 Male            0 NA             NA  \n 6 Male            3 NA             NA  \n 7 Male           16 FALSE          NA  \n 8 Male            7 FALSE          NA  \n 9 Male           19 FALSE           4.7\n10 Male           21 FALSE           4.8\n# ℹ 15,391 more rows\n\n\nSome may find it helpful to read the pipe like the word “then”. For instance, in the above example, we took the data frame rna, then we filtered for rows with RIAGENDR == \"Male\", then we selected columns RIAGENDR, RIDAGEYR, dental.caries, and LBXGH.\nThe dplyr functions by themselves are somewhat simple, but by combining them into linear workflows with the pipe, we can accomplish more complex manipulations of data frames.\nIf we want to create a new object with this smaller version of the data, we can assign it a new name:\n\nnhanes3 |&gt;\n  filter(RIAGENDR == \"Male\") |&gt;\n  select(RIAGENDR, RIDAGEYR, dental.caries, LBXGH)\n\n# A tibble: 15,401 × 4\n   RIAGENDR RIDAGEYR dental.caries LBXGH\n   &lt;chr&gt;       &lt;dbl&gt; &lt;lgl&gt;         &lt;dbl&gt;\n 1 Male            0 NA             NA  \n 2 Male           15 FALSE           5.2\n 3 Male           70 TRUE            7.1\n 4 Male           73 TRUE            5.9\n 5 Male            0 NA             NA  \n 6 Male            3 NA             NA  \n 7 Male           16 FALSE          NA  \n 8 Male            7 FALSE          NA  \n 9 Male           19 FALSE           4.7\n10 Male           21 FALSE           4.8\n# ℹ 15,391 more rows\n\nnhanes3\n\n# A tibble: 15,401 × 4\n   RIAGENDR RIDAGEYR dental.caries LBXGH\n   &lt;chr&gt;       &lt;dbl&gt; &lt;lgl&gt;         &lt;dbl&gt;\n 1 Male            0 NA             NA  \n 2 Male           15 FALSE           5.2\n 3 Male           70 TRUE            7.1\n 4 Male           73 TRUE            5.9\n 5 Male            0 NA             NA  \n 6 Male            3 NA             NA  \n 7 Male           16 FALSE          NA  \n 8 Male            7 FALSE          NA  \n 9 Male           19 FALSE           4.7\n10 Male           21 FALSE           4.8\n# ℹ 15,391 more rows\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\nUsing pipes, subset the nhanes data to keep female participants 15 years or older, where the LBXGH is greater than 5.2 (and is not NA), and retain only the columns RIAGENDR, RIDAGEYR, and LBXGLU.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnhanes |&gt;\n  filter(LBXGH &gt; 5.2,\n         RIAGENDR == \"Female\",\n         RIDAGEYR &gt;= 15 ) |&gt;\n  select(RIAGENDR, RIDAGEYR, LBXGLU)\n\n# A tibble: 5,852 × 3\n   RIAGENDR RIDAGEYR LBXGLU\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 Female         44     90\n 2 Female         85     NA\n 3 Female         59     86\n 4 Female         43     NA\n 5 Female         71    124\n 6 Female         18     92\n 7 Female         46    157\n 8 Female         60     NA\n 9 Female         47     NA\n10 Female         33     71\n# ℹ 5,842 more rows"
  },
  {
    "objectID": "notebooks/session2.html#first-steps-with-ggplot2",
    "href": "notebooks/session2.html#first-steps-with-ggplot2",
    "title": "Summarizing and Visualizing data",
    "section": "First steps with ggplot2",
    "text": "First steps with ggplot2\nYou can find a ggplot2 cheatsheet here.\nggplot2 is the most popular data visualisation R package. Its ggplot() function is at the core of this package, and this whole approach is colloquially known as “ggplot” with the resulting figures sometimes affectionately called “ggplots”. The “gg” in these names reflects the “grammar of graphics” used to construct the figures. ggplot2 benefits from a wide variety of supplementary R packages that further enhance its functionality.\nThe data visualization with ggplot cheatsheet from the RStudio website is a great reference to have on-hand when creating pltos. If you want inspiration for ways to creatively visualise your data, we suggest reviewing websites like the R graph gallery and Data-to-viz.\nWe can summarize the ggplot approach as:\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) +  &lt;GEOM_FUNCTION&gt;()\nWhere we define a dataset, choose which variables map to which aspects of the plot, and then choose the geom() or type of plot to draw.\nLet’s plug the NHANES dataset into a plot.\n\nggplot(nhanes) \n\n\n\n\nSince we haven’t told ggplot what we want to display, we just get a blank plot. If we add some mappings for the x and y axes:\n\nggplot(nhanes, aes(x = LBXGH, y = LBXGLU))\n\n\n\n\nWe now get labeled axes and scales based on the variable range. Finally, we can add a geom(). Let’s make a scatterplot, created with geom_point() in ggplot.\n\nggplot(nhanes, aes(x = LBXGH, y = LBXGLU)) +\n  geom_point()\n\nWarning: Removed 21442 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFor large datasets, scatterplots start to become less useful and can be computationally intensive. Instead, let’s try the geom_hex to plot density.\n\nggplot(nhanes, aes(x = LBXGH, y = LBXGLU)) +\n  geom_hex(bins = 50)\n\nWarning: Removed 21442 rows containing non-finite values (`stat_binhex()`).\n\n\n\n\n\nNow, let’s make a boxplot showing how BMI varies by subject ethnicity. Inside of the geom_boxplot function, we’ll also set the varwidth parameter to true so that the box sizes vary with how many samples are in each category.\n\nggplot(nhanes, aes(x = RIDRETH1, y = BMXBMI)) + \n  geom_boxplot(varwidth = TRUE)\n\nWarning: Removed 3812 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nInstead of a boxplot, try making a violin plot.\n\n# Violin plot\nggplot(nhanes, aes(x = RIDRETH1, y = BMXBMI)) + \n  geom_violin()\n\nWarning: Removed 3812 rows containing non-finite values (`stat_ydensity()`).\n\n\n\n\n\n\n\nNote that we can also easily make boxplots using R’s builtin plotting boxplot function.\n\nboxplot(BMXBMI ~ RIDRETH1, data = nhanes)"
  },
  {
    "objectID": "notebooks/session2.html#mapping-variables",
    "href": "notebooks/session2.html#mapping-variables",
    "title": "Summarizing and Visualizing data",
    "section": "Mapping Variables",
    "text": "Mapping Variables\nBeyond the actual axes we can use mappings to encode variables as various aspects of a plot. Some of the most commonly used other mapping types are shape, fill, color, size, and linetype.\nFor instance, let’s take our scatterplot from before and color the points by diabetic status.\n\nggplot(nhanes, aes(x = LBXGH, y = BMXBMI, color = diabetes)) +\n  geom_point() \n\nWarning: Removed 11441 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIt is difficult to tell how many diabetic participants are in this plot, as it’s possible that the red diabetic points have been covered by the blue and green points. We can alter the transparency of the points by changing alpha. Remember we can also change parts of the plot outside of aes() to have them not depend on any variable.\n\nggplot(nhanes, aes(x = LBXGH, y = BMXBMI, color = diabetes)) +\n  geom_point(alpha = 0.4) \n\nWarning: Removed 11441 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWe can also have a single variable encoded into multiple parts of the plot.\n\nggplot(nhanes, aes(x = LBXGH, y = BMXBMI, color = diabetes, shape = diabetes)) +\n  geom_point(alpha = 0.6) \n\nWarning: Removed 11441 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIt’s still hard to tell. Let’s instead make a facet grid to separate out the distributions and drop the NA values.\n\nnhanes |&gt;\n  filter(!is.na(diabetes)) |&gt;\nggplot(aes(x = LBXGH, y = BMXBMI, color = diabetes)) +\n  geom_point(alpha = 0.6) +\n  facet_grid(~diabetes)\n\nWarning: Removed 289 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "notebooks/session2.html#customizing-plots",
    "href": "notebooks/session2.html#customizing-plots",
    "title": "Summarizing and Visualizing data",
    "section": "Customizing Plots",
    "text": "Customizing Plots\nTaking a figure all the way to publication-quality can require careful fine tuning. ggplot has a variety of useful themes and other ways to improve a figure’s appearance and readability.\nHere’s an example of some of what you can do. Note that changing the fig.width setting for the code block will not effect how the image looks when exported.\n\n#Maybe we want a color scheme from a Wes Anderson movie:\nlibrary(wesanderson)\n\n#Note that this font import can take multiple minutes to run\npal &lt;- wes_palette(\"Zissou1\", 2, type = \"continuous\")\n\nggplot(nhanes, aes(x = RIDRETH1, y = BMXBMI, fill = dental.caries)) + \n geom_boxplot() +\n theme_minimal() +\n ggtitle(\"BMI by ethicity and dental caries\") +\n xlab(\"Ethnicity\") +\n ylab(\"BMI\") +\n scale_fill_manual(values = pal, name = \"Dental Caries\") +\n theme(text = element_text(size=14), axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\nWarning: Removed 3812 rows containing non-finite values (`stat_boxplot()`)."
  },
  {
    "objectID": "notebooks/session2.html#interactive-plots-and-widgets",
    "href": "notebooks/session2.html#interactive-plots-and-widgets",
    "title": "Summarizing and Visualizing data",
    "section": "Interactive plots and widgets",
    "text": "Interactive plots and widgets\nWe can use plotly to add basic interactivity to plots.\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following objects are masked from 'package:flextable':\n\n    highlight, style\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\np &lt;- ggplot(nhanes, aes(x = BMXBMI, fill = RIAGENDR)) +\n  geom_histogram(position = \"stack\") \n\nggplotly(p)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3812 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\nAnd we can create more complex interactive plots using the shiny package.\n\nlibrary(shiny)\nselectInput(\n  'breaks', label = 'Number of bins:',\n  choices = c(10, 20, 35, 50), selected = 20\n)\n\nrenderPlot({\n  ggplot(nhanes, aes(x = BMXBMI, fill = RIAGENDR)) +\n  geom_histogram(position = \"stack\", bins = as.numeric(input$breaks)) \n  \n})\n\nThis is just scratching the surface of interactive plots. You can find examples of creating full interactive dashboards in The Epidemiologist R Handbook."
  },
  {
    "objectID": "notebooks/session2.html#creating-maps-and-using-shape-files",
    "href": "notebooks/session2.html#creating-maps-and-using-shape-files",
    "title": "Summarizing and Visualizing data",
    "section": "Creating maps and using shape files",
    "text": "Creating maps and using shape files\nAs with everything else, there are a variety of ways to create maps in R. Let’s look at 2 examples, creating simple maps with the maps package and importing a shapefile using the sf package.\nFor simple information we want to see on a standard map, we can use the maps package. For instance, we can grab a map of the USA and plot some coordinates.\n\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(ggmap)\n\nℹ Google's Terms of Service: &lt;https://mapsplatform.google.com&gt;\nℹ Please cite ggmap if you use it! Use `citation(\"ggmap\")` for details.\n\n\n\nAttaching package: 'ggmap'\n\n\nThe following object is masked from 'package:plotly':\n\n    wind\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\nusa &lt;- map_data(\"usa\") \n\nlabs &lt;- data.frame(\n  long = c(-122.064873, -122.306417),\n  lat = c(36.951968, 47.644855),\n  names = c(\"SWFSC-FED\", \"NWFSC\"),\n  stringsAsFactors = FALSE\n  )  \n\nggplot() +\n  geom_polygon(data = usa, aes(x=long, y = lat, group = group), fill = \"coral\", color = \"brown\") + \n  coord_fixed(1.3) +\n  geom_point(data = labs, aes(x = long, y = lat), color = \"black\", size = 5) +\n  geom_point(data = labs, aes(x = long, y = lat), color = \"gold\", size = 4)\n\n\n\n\nOr we can get data for states\n\nstates &lt;- map_data(\"state\")\nhead(states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nAnd then grab and plot just new england.\n\nnew_england &lt;- subset(states, region %in% c('connecticut', 'maine', 'massachusetts', 'new hampshire', 'rhode island', 'vermont'))\n\nggplot(data = new_england) + \n  geom_polygon(aes(x = long, y = lat, group = group), fill = \"palegreen\", color = \"black\") + \n  coord_fixed(1.3)\n\n\n\n\nLet’s zoom in and plot population density by county in CT.\n\nct_df &lt;- subset(states, region == \"connecticut\")\n\ncounties &lt;- map_data(\"county\")\nct_county &lt;- subset(counties, region == \"connecticut\")\n\nWe start by plotting just the state boundaries. Let’s get rid of the background gridlines too.\n\nct_base &lt;- ggplot(data = ct_df, mapping = aes(x = long, y = lat, group = group)) + \n  coord_fixed(1.3) + \n  geom_polygon(color = \"black\", fill = \"gray\")\nct_base + theme_nothing()\n\n\n\n\nNow we can add the county borders in white.\n\nct_base + theme_nothing() + \n  geom_polygon(data = ct_county, color = \"white\", aes(fill = subregion))\n\n\n\n\n\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE\n\nlinelist &lt;- readRDS(\"../data/linelist_cleaned.rds\")\n\n# generate 1000 random row numbers, from the number of rows in linelist\nsample_rows &lt;- sample(nrow(linelist), 1000)\n\n# subset linelist to keep only the sample rows, and all columns\nlinelist &lt;- linelist[sample_rows,]\n\nWe use the package sf (spatial features) and its function st_as_sf() to create the new object we call linelist_sf. This new object looks essentially the same as the linelist, but the columns lon and lat have been designated as coordinate columns, and a coordinate reference system (CRS) has been assigned for when the points are displayed.\n\nlinelist_sf &lt;- linelist |&gt;\n     sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n\ndatatable(head(linelist_sf, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )\n\n\n\n\n\n\n\nsle_adm3_raw &lt;- read_sf(\"../data/gis/sle_adm3.shp\")\n\n# ADM3 level clean\nsle_adm3 &lt;- sle_adm3_raw |&gt;\n  janitor::clean_names() |&gt; # standardize column names\n  filter(admin2name %in% c(\"Western Area Urban\", \"Western Area Rural\")) # filter to keep certain areas\n\nThe package tmap offers simple mapping capabilities for both static (“plot” mode) and interactive (“view” mode) with just a few lines of code. We can plot the distribution of ebola cases using tmap.\n\ntmap_mode(\"plot\") # choose either \"view\" or \"plot\"\n\ntmap mode set to plotting\n\ntm_shape(linelist_sf) + tm_dots(size=0.08, col='blue')\n\n\n\n\nThis is not very useful without the administrative border data.\n\ntmap_mode(\"plot\") # choose either \"view\" or \"plot\"\n\ntmap mode set to plotting\n\ntm_shape(sle_adm3,               # admin boundaries shapefile\n           bbox = c(-13.3, 8.43,  # corner\n                  -13.2, 8.51)) +   # corner\n  tm_polygons(col = \"#F7F7F7\")+    # show polygons in light grey\n  tm_borders(col = \"#000000\",      # show borders with color and line weight\n             lwd = 2) +\n  tm_text(\"admin3name\")   +         # column text to display for each polygon\n  tm_shape(linelist_sf) +\n  tm_dots(size=0.08, col='blue', alpha = 0.5) +\n  tm_layout(title = \"Distribution of Ebola cases\")   # give title to map\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\nJohn A. Graves has a great example analysis in R using the Dartmouth atlas data to examine different definitions of health care markets which you can find here."
  },
  {
    "objectID": "notebooks/session2.html#exporting-plots",
    "href": "notebooks/session2.html#exporting-plots",
    "title": "Summarizing and Visualizing data",
    "section": "Exporting plots",
    "text": "Exporting plots\nExporting ggplots is made easy with the ggsave() function from ggplot2. It can work in two ways, either:\n\nSpecify the name of the plot object, then the file path and name with extension\n\nFor example: ggsave(my_plot, \"plots/my_plot.png\"))\n\n\nRun the command with only a file path, to save the last plot that was printed\n\nFor example: ggsave(\"plots/my_plot.png\"))\n\n\nYou can export as png, pdf, jpeg, tiff, bmp, svg, or several other file types, by specifying the file extension in the file path.\nYou can also specify the arguments width =, height =, and units = (either “in”, “cm”, or “mm”). You can also specify dpi = with a number for plot resolution (e.g. 300). See the function details by entering ?ggsave or reading the documentation online.\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/), as well as materials created by Laurent Gatto, Charlotte Soneson, Jenny Drnevich, Robert Castelo, and Kevin Rue-Albert. These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "notebooks/session4.html",
    "href": "notebooks/session4.html",
    "title": "Data wrangling I",
    "section": "",
    "text": "Let’s load the data and the column labels.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DT)\n\nben_summary_2008 &lt;- read_csv(\"../data/de_synpuf/DE1_0_2008_Beneficiary_Summary_File_Sample_1.csv\")\n\nRows: 116352 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): DESYNPUF_ID, BENE_ESRD_IND, SP_STATE_CODE, BENE_COUNTY_CD, PLAN_CV...\ndbl (27): BENE_BIRTH_DT, BENE_DEATH_DT, BENE_SEX_IDENT_CD, BENE_RACE_CD, BEN...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nben_summary_2009 &lt;- read_csv(\"../data/de_synpuf/DE1_0_2009_Beneficiary_Summary_File_Sample_1.csv\")\n\nRows: 114538 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): DESYNPUF_ID, BENE_ESRD_IND, SP_STATE_CODE, BENE_COUNTY_CD, PLAN_CV...\ndbl (27): BENE_BIRTH_DT, BENE_DEATH_DT, BENE_SEX_IDENT_CD, BENE_RACE_CD, BEN...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nben_summary_labels &lt;- read_csv(\"../data/de_synpuf/ben_metadata.csv\")\n\nRows: 32 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, Label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary(ben_summary_2008)\n\n DESYNPUF_ID        BENE_BIRTH_DT      BENE_DEATH_DT      BENE_SEX_IDENT_CD\n Length:116352      Min.   :19090101   Min.   :20080101   Min.   :1.000    \n Class :character   1st Qu.:19281101   1st Qu.:20080301   1st Qu.:1.000    \n Mode  :character   Median :19360501   Median :20080701   Median :2.000    \n                    Mean   :19364181   Mean   :20080654   Mean   :1.553    \n                    3rd Qu.:19420301   3rd Qu.:20081001   3rd Qu.:2.000    \n                    Max.   :19831201   Max.   :20081201   Max.   :2.000    \n                                       NA's   :114538                      \n  BENE_RACE_CD   BENE_ESRD_IND      SP_STATE_CODE      BENE_COUNTY_CD    \n Min.   :1.000   Length:116352      Length:116352      Length:116352     \n 1st Qu.:1.000   Class :character   Class :character   Class :character  \n Median :1.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :1.285                                                           \n 3rd Qu.:1.000                                                           \n Max.   :5.000                                                           \n                                                                         \n BENE_HI_CVRAGE_TOT_MONS BENE_SMI_CVRAGE_TOT_MONS BENE_HMO_CVRAGE_TOT_MONS\n Min.   : 0.00           Min.   : 0.0             Min.   : 0.000          \n 1st Qu.:12.00           1st Qu.:12.0             1st Qu.: 0.000          \n Median :12.00           Median :12.0             Median : 0.000          \n Mean   :11.14           Mean   :10.5             Mean   : 2.576          \n 3rd Qu.:12.00           3rd Qu.:12.0             3rd Qu.: 0.000          \n Max.   :12.00           Max.   :12.0             Max.   :12.000          \n                                                                          \n PLAN_CVRG_MOS_NUM   SP_ALZHDMTA        SP_CHF       SP_CHRNKIDN   \n Length:116352      Min.   :1.000   Min.   :1.000   Min.   :1.000  \n Class :character   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000  \n Mode  :character   Median :2.000   Median :2.000   Median :2.000  \n                    Mean   :1.807   Mean   :1.715   Mean   :1.839  \n                    3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n                    Max.   :2.000   Max.   :2.000   Max.   :2.000  \n                                                                   \n    SP_CNCR         SP_COPD       SP_DEPRESSN     SP_DIABETES   \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.936   Mean   :1.865   Mean   :1.787   Mean   :1.621  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n                                                                \n  SP_ISCHMCHT     SP_OSTEOPRS       SP_RA_OA      SP_STRKETIA   \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.579   Mean   :1.827   Mean   :1.846   Mean   :1.955  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n                                                                \n  MEDREIMB_IP       BENRES_IP         PPPYMT_IP         MEDREIMB_OP     \n Min.   : -3000   Min.   :    0.0   Min.   :    0.00   Min.   :  -90.0  \n 1st Qu.:     0   1st Qu.:    0.0   1st Qu.:    0.00   1st Qu.:    0.0  \n Median :     0   Median :    0.0   Median :    0.00   Median :   20.0  \n Mean   :  2214   Mean   :  249.1   Mean   :   99.14   Mean   :  622.2  \n 3rd Qu.:     0   3rd Qu.:    0.0   3rd Qu.:    0.00   3rd Qu.:  550.0  \n Max.   :164220   Max.   :53096.0   Max.   :68000.00   Max.   :50020.0  \n                                                                        \n   BENRES_OP         PPPYMT_OP         MEDREIMB_CAR     BENRES_CAR    \n Min.   :    0.0   Min.   :    0.00   Min.   :    0   Min.   :   0.0  \n 1st Qu.:    0.0   1st Qu.:    0.00   1st Qu.:    0   1st Qu.:   0.0  \n Median :    0.0   Median :    0.00   Median :  610   Median : 170.0  \n Mean   :  197.5   Mean   :   25.72   Mean   : 1162   Mean   : 328.7  \n 3rd Qu.:  180.0   3rd Qu.:    0.00   3rd Qu.: 1650   3rd Qu.: 480.0  \n Max.   :12450.0   Max.   :14400.00   Max.   :21160   Max.   :5260.0  \n                                                                      \n   PPPYMT_CAR     \n Min.   :   0.00  \n 1st Qu.:   0.00  \n Median :   0.00  \n Mean   :  18.36  \n 3rd Qu.:   0.00  \n Max.   :2110.00  \n                  \n\nDT::datatable(ben_summary_labels)\n\n\n\n\n\n\n\n\n\nNow let’s load and link in the inpatient data. To make things simpler, we will drop everything except for the 2008 data.\n\ninp_data &lt;- read_csv(\"../data/de_synpuf/DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv\")\n\nRows: 66773 Columns: 81\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (23): DESYNPUF_ID, PRVDR_NUM, AT_PHYSN_NPI, OP_PHYSN_NPI, OT_PHYSN_NPI, ...\ndbl (13): CLM_ID, SEGMENT, CLM_FROM_DT, CLM_THRU_DT, CLM_PMT_AMT, NCH_PRMRY_...\nlgl (45): HCPCS_CD_1, HCPCS_CD_2, HCPCS_CD_3, HCPCS_CD_4, HCPCS_CD_5, HCPCS_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ninp_labels &lt;- read_csv(\"../data/de_synpuf/inp_metadata.csv\")\n\nRows: 23 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, Label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary(inp_data)\n\n DESYNPUF_ID            CLM_ID             SEGMENT       CLM_FROM_DT      \n Length:66773       Min.   :1.960e+14   Min.   :1.000   Min.   :20071127  \n Class :character   1st Qu.:1.963e+14   1st Qu.:1.000   1st Qu.:20080812  \n Mode  :character   Median :1.965e+14   Median :1.000   Median :20090316  \n                    Mean   :1.965e+14   Mean   :1.001   Mean   :20088463  \n                    3rd Qu.:1.968e+14   3rd Qu.:1.000   3rd Qu.:20091112  \n                    Max.   :1.970e+14   Max.   :2.000   Max.   :20101230  \n                                                        NA's   :68        \n  CLM_THRU_DT        PRVDR_NUM          CLM_PMT_AMT    NCH_PRMRY_PYR_CLM_PD_AMT\n Min.   :20080101   Length:66773       Min.   :-8000   Min.   :    0.0         \n 1st Qu.:20080818   Class :character   1st Qu.: 4000   1st Qu.:    0.0         \n Median :20090321   Mode  :character   Median : 7000   Median :    0.0         \n Mean   :20088611                      Mean   : 9574   Mean   :  398.9         \n 3rd Qu.:20091117                      3rd Qu.:11000   3rd Qu.:    0.0         \n Max.   :20101231                      Max.   :57000   Max.   :68000.0         \n NA's   :68                                                                    \n AT_PHYSN_NPI       OP_PHYSN_NPI       OT_PHYSN_NPI        CLM_ADMSN_DT     \n Length:66773       Length:66773       Length:66773       Min.   :20071127  \n Class :character   Class :character   Class :character   1st Qu.:20080812  \n Mode  :character   Mode  :character   Mode  :character   Median :20090316  \n                                                          Mean   :20088463  \n                                                          3rd Qu.:20091112  \n                                                          Max.   :20101230  \n                                                                            \n ADMTNG_ICD9_DGNS_CD CLM_PASS_THRU_PER_DIEM_AMT NCH_BENE_IP_DDCTBL_AMT\n Length:66773        Min.   :  0.00             Min.   :1024          \n Class :character    1st Qu.:  0.00             1st Qu.:1024          \n Mode  :character    Median :  0.00             Median :1068          \n                     Mean   : 28.98             Mean   :1057          \n                     3rd Qu.: 10.00             3rd Qu.:1068          \n                     Max.   :500.00             Max.   :1100          \n                                                NA's   :2178          \n NCH_BENE_PTA_COINSRNC_LBLTY_AM NCH_BENE_BLOOD_DDCTBL_LBLTY_AM\n Min.   :    0.00               Min.   :   0.00               \n 1st Qu.:    0.00               1st Qu.:   0.00               \n Median :    0.00               Median :   0.00               \n Mean   :   90.03               Mean   :   1.59               \n 3rd Qu.:    0.00               3rd Qu.:   0.00               \n Max.   :34000.00               Max.   :2000.00               \n                                                              \n CLM_UTLZTN_DAY_CNT NCH_BENE_DSCHRG_DT  CLM_DRG_CD        ICD9_DGNS_CD_1    \n Min.   :  0.000    Min.   :20080101   Length:66773       Length:66773      \n 1st Qu.:  2.000    1st Qu.:20080818   Class :character   Class :character  \n Median :  4.000    Median :20090321   Mode  :character   Mode  :character  \n Mean   :  5.583    Mean   :20088612                                        \n 3rd Qu.:  7.000    3rd Qu.:20091117                                        \n Max.   :136.000    Max.   :20101231                                        \n NA's   :68                                                                 \n ICD9_DGNS_CD_2     ICD9_DGNS_CD_3     ICD9_DGNS_CD_4     ICD9_DGNS_CD_5    \n Length:66773       Length:66773       Length:66773       Length:66773      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n ICD9_DGNS_CD_6     ICD9_DGNS_CD_7     ICD9_DGNS_CD_8     ICD9_DGNS_CD_9    \n Length:66773       Length:66773       Length:66773       Length:66773      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n ICD9_DGNS_CD_10    ICD9_PRCDR_CD_1    ICD9_PRCDR_CD_2    ICD9_PRCDR_CD_3   \n Length:66773       Length:66773       Length:66773       Length:66773      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n ICD9_PRCDR_CD_4    ICD9_PRCDR_CD_5    ICD9_PRCDR_CD_6    HCPCS_CD_1    \n Length:66773       Length:66773       Length:66773       Mode:logical  \n Class :character   Class :character   Class :character   NA's:66773    \n Mode  :character   Mode  :character   Mode  :character                 \n                                                                        \n                                                                        \n                                                                        \n                                                                        \n HCPCS_CD_2     HCPCS_CD_3     HCPCS_CD_4     HCPCS_CD_5     HCPCS_CD_6    \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_7     HCPCS_CD_8     HCPCS_CD_9     HCPCS_CD_10    HCPCS_CD_11   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_12    HCPCS_CD_13    HCPCS_CD_14    HCPCS_CD_15    HCPCS_CD_16   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_17    HCPCS_CD_18    HCPCS_CD_19    HCPCS_CD_20    HCPCS_CD_21   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_22    HCPCS_CD_23    HCPCS_CD_24    HCPCS_CD_25    HCPCS_CD_26   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_27    HCPCS_CD_28    HCPCS_CD_29    HCPCS_CD_30    HCPCS_CD_31   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_32    HCPCS_CD_33    HCPCS_CD_34    HCPCS_CD_35    HCPCS_CD_36   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_37    HCPCS_CD_38    HCPCS_CD_39    HCPCS_CD_40    HCPCS_CD_41   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_42    HCPCS_CD_43    HCPCS_CD_44    HCPCS_CD_45   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                            \n                                                            \n                                                            \n                                                            \n                                                            \n\nDT::datatable(inp_labels)"
  },
  {
    "objectID": "notebooks/session4.html#load-beneficiaries-summary-file",
    "href": "notebooks/session4.html#load-beneficiaries-summary-file",
    "title": "Data wrangling I",
    "section": "",
    "text": "Let’s load the data and the column labels.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DT)\n\nben_summary_2008 &lt;- read_csv(\"../data/de_synpuf/DE1_0_2008_Beneficiary_Summary_File_Sample_1.csv\")\n\nRows: 116352 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): DESYNPUF_ID, BENE_ESRD_IND, SP_STATE_CODE, BENE_COUNTY_CD, PLAN_CV...\ndbl (27): BENE_BIRTH_DT, BENE_DEATH_DT, BENE_SEX_IDENT_CD, BENE_RACE_CD, BEN...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nben_summary_2009 &lt;- read_csv(\"../data/de_synpuf/DE1_0_2009_Beneficiary_Summary_File_Sample_1.csv\")\n\nRows: 114538 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): DESYNPUF_ID, BENE_ESRD_IND, SP_STATE_CODE, BENE_COUNTY_CD, PLAN_CV...\ndbl (27): BENE_BIRTH_DT, BENE_DEATH_DT, BENE_SEX_IDENT_CD, BENE_RACE_CD, BEN...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nben_summary_labels &lt;- read_csv(\"../data/de_synpuf/ben_metadata.csv\")\n\nRows: 32 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, Label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary(ben_summary_2008)\n\n DESYNPUF_ID        BENE_BIRTH_DT      BENE_DEATH_DT      BENE_SEX_IDENT_CD\n Length:116352      Min.   :19090101   Min.   :20080101   Min.   :1.000    \n Class :character   1st Qu.:19281101   1st Qu.:20080301   1st Qu.:1.000    \n Mode  :character   Median :19360501   Median :20080701   Median :2.000    \n                    Mean   :19364181   Mean   :20080654   Mean   :1.553    \n                    3rd Qu.:19420301   3rd Qu.:20081001   3rd Qu.:2.000    \n                    Max.   :19831201   Max.   :20081201   Max.   :2.000    \n                                       NA's   :114538                      \n  BENE_RACE_CD   BENE_ESRD_IND      SP_STATE_CODE      BENE_COUNTY_CD    \n Min.   :1.000   Length:116352      Length:116352      Length:116352     \n 1st Qu.:1.000   Class :character   Class :character   Class :character  \n Median :1.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :1.285                                                           \n 3rd Qu.:1.000                                                           \n Max.   :5.000                                                           \n                                                                         \n BENE_HI_CVRAGE_TOT_MONS BENE_SMI_CVRAGE_TOT_MONS BENE_HMO_CVRAGE_TOT_MONS\n Min.   : 0.00           Min.   : 0.0             Min.   : 0.000          \n 1st Qu.:12.00           1st Qu.:12.0             1st Qu.: 0.000          \n Median :12.00           Median :12.0             Median : 0.000          \n Mean   :11.14           Mean   :10.5             Mean   : 2.576          \n 3rd Qu.:12.00           3rd Qu.:12.0             3rd Qu.: 0.000          \n Max.   :12.00           Max.   :12.0             Max.   :12.000          \n                                                                          \n PLAN_CVRG_MOS_NUM   SP_ALZHDMTA        SP_CHF       SP_CHRNKIDN   \n Length:116352      Min.   :1.000   Min.   :1.000   Min.   :1.000  \n Class :character   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000  \n Mode  :character   Median :2.000   Median :2.000   Median :2.000  \n                    Mean   :1.807   Mean   :1.715   Mean   :1.839  \n                    3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n                    Max.   :2.000   Max.   :2.000   Max.   :2.000  \n                                                                   \n    SP_CNCR         SP_COPD       SP_DEPRESSN     SP_DIABETES   \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.936   Mean   :1.865   Mean   :1.787   Mean   :1.621  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n                                                                \n  SP_ISCHMCHT     SP_OSTEOPRS       SP_RA_OA      SP_STRKETIA   \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.579   Mean   :1.827   Mean   :1.846   Mean   :1.955  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n                                                                \n  MEDREIMB_IP       BENRES_IP         PPPYMT_IP         MEDREIMB_OP     \n Min.   : -3000   Min.   :    0.0   Min.   :    0.00   Min.   :  -90.0  \n 1st Qu.:     0   1st Qu.:    0.0   1st Qu.:    0.00   1st Qu.:    0.0  \n Median :     0   Median :    0.0   Median :    0.00   Median :   20.0  \n Mean   :  2214   Mean   :  249.1   Mean   :   99.14   Mean   :  622.2  \n 3rd Qu.:     0   3rd Qu.:    0.0   3rd Qu.:    0.00   3rd Qu.:  550.0  \n Max.   :164220   Max.   :53096.0   Max.   :68000.00   Max.   :50020.0  \n                                                                        \n   BENRES_OP         PPPYMT_OP         MEDREIMB_CAR     BENRES_CAR    \n Min.   :    0.0   Min.   :    0.00   Min.   :    0   Min.   :   0.0  \n 1st Qu.:    0.0   1st Qu.:    0.00   1st Qu.:    0   1st Qu.:   0.0  \n Median :    0.0   Median :    0.00   Median :  610   Median : 170.0  \n Mean   :  197.5   Mean   :   25.72   Mean   : 1162   Mean   : 328.7  \n 3rd Qu.:  180.0   3rd Qu.:    0.00   3rd Qu.: 1650   3rd Qu.: 480.0  \n Max.   :12450.0   Max.   :14400.00   Max.   :21160   Max.   :5260.0  \n                                                                      \n   PPPYMT_CAR     \n Min.   :   0.00  \n 1st Qu.:   0.00  \n Median :   0.00  \n Mean   :  18.36  \n 3rd Qu.:   0.00  \n Max.   :2110.00  \n                  \n\nDT::datatable(ben_summary_labels)"
  },
  {
    "objectID": "notebooks/session4.html#load-inpatient-table",
    "href": "notebooks/session4.html#load-inpatient-table",
    "title": "Data wrangling I",
    "section": "",
    "text": "Now let’s load and link in the inpatient data. To make things simpler, we will drop everything except for the 2008 data.\n\ninp_data &lt;- read_csv(\"../data/de_synpuf/DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv\")\n\nRows: 66773 Columns: 81\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (23): DESYNPUF_ID, PRVDR_NUM, AT_PHYSN_NPI, OP_PHYSN_NPI, OT_PHYSN_NPI, ...\ndbl (13): CLM_ID, SEGMENT, CLM_FROM_DT, CLM_THRU_DT, CLM_PMT_AMT, NCH_PRMRY_...\nlgl (45): HCPCS_CD_1, HCPCS_CD_2, HCPCS_CD_3, HCPCS_CD_4, HCPCS_CD_5, HCPCS_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ninp_labels &lt;- read_csv(\"../data/de_synpuf/inp_metadata.csv\")\n\nRows: 23 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, Label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary(inp_data)\n\n DESYNPUF_ID            CLM_ID             SEGMENT       CLM_FROM_DT      \n Length:66773       Min.   :1.960e+14   Min.   :1.000   Min.   :20071127  \n Class :character   1st Qu.:1.963e+14   1st Qu.:1.000   1st Qu.:20080812  \n Mode  :character   Median :1.965e+14   Median :1.000   Median :20090316  \n                    Mean   :1.965e+14   Mean   :1.001   Mean   :20088463  \n                    3rd Qu.:1.968e+14   3rd Qu.:1.000   3rd Qu.:20091112  \n                    Max.   :1.970e+14   Max.   :2.000   Max.   :20101230  \n                                                        NA's   :68        \n  CLM_THRU_DT        PRVDR_NUM          CLM_PMT_AMT    NCH_PRMRY_PYR_CLM_PD_AMT\n Min.   :20080101   Length:66773       Min.   :-8000   Min.   :    0.0         \n 1st Qu.:20080818   Class :character   1st Qu.: 4000   1st Qu.:    0.0         \n Median :20090321   Mode  :character   Median : 7000   Median :    0.0         \n Mean   :20088611                      Mean   : 9574   Mean   :  398.9         \n 3rd Qu.:20091117                      3rd Qu.:11000   3rd Qu.:    0.0         \n Max.   :20101231                      Max.   :57000   Max.   :68000.0         \n NA's   :68                                                                    \n AT_PHYSN_NPI       OP_PHYSN_NPI       OT_PHYSN_NPI        CLM_ADMSN_DT     \n Length:66773       Length:66773       Length:66773       Min.   :20071127  \n Class :character   Class :character   Class :character   1st Qu.:20080812  \n Mode  :character   Mode  :character   Mode  :character   Median :20090316  \n                                                          Mean   :20088463  \n                                                          3rd Qu.:20091112  \n                                                          Max.   :20101230  \n                                                                            \n ADMTNG_ICD9_DGNS_CD CLM_PASS_THRU_PER_DIEM_AMT NCH_BENE_IP_DDCTBL_AMT\n Length:66773        Min.   :  0.00             Min.   :1024          \n Class :character    1st Qu.:  0.00             1st Qu.:1024          \n Mode  :character    Median :  0.00             Median :1068          \n                     Mean   : 28.98             Mean   :1057          \n                     3rd Qu.: 10.00             3rd Qu.:1068          \n                     Max.   :500.00             Max.   :1100          \n                                                NA's   :2178          \n NCH_BENE_PTA_COINSRNC_LBLTY_AM NCH_BENE_BLOOD_DDCTBL_LBLTY_AM\n Min.   :    0.00               Min.   :   0.00               \n 1st Qu.:    0.00               1st Qu.:   0.00               \n Median :    0.00               Median :   0.00               \n Mean   :   90.03               Mean   :   1.59               \n 3rd Qu.:    0.00               3rd Qu.:   0.00               \n Max.   :34000.00               Max.   :2000.00               \n                                                              \n CLM_UTLZTN_DAY_CNT NCH_BENE_DSCHRG_DT  CLM_DRG_CD        ICD9_DGNS_CD_1    \n Min.   :  0.000    Min.   :20080101   Length:66773       Length:66773      \n 1st Qu.:  2.000    1st Qu.:20080818   Class :character   Class :character  \n Median :  4.000    Median :20090321   Mode  :character   Mode  :character  \n Mean   :  5.583    Mean   :20088612                                        \n 3rd Qu.:  7.000    3rd Qu.:20091117                                        \n Max.   :136.000    Max.   :20101231                                        \n NA's   :68                                                                 \n ICD9_DGNS_CD_2     ICD9_DGNS_CD_3     ICD9_DGNS_CD_4     ICD9_DGNS_CD_5    \n Length:66773       Length:66773       Length:66773       Length:66773      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n ICD9_DGNS_CD_6     ICD9_DGNS_CD_7     ICD9_DGNS_CD_8     ICD9_DGNS_CD_9    \n Length:66773       Length:66773       Length:66773       Length:66773      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n ICD9_DGNS_CD_10    ICD9_PRCDR_CD_1    ICD9_PRCDR_CD_2    ICD9_PRCDR_CD_3   \n Length:66773       Length:66773       Length:66773       Length:66773      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n ICD9_PRCDR_CD_4    ICD9_PRCDR_CD_5    ICD9_PRCDR_CD_6    HCPCS_CD_1    \n Length:66773       Length:66773       Length:66773       Mode:logical  \n Class :character   Class :character   Class :character   NA's:66773    \n Mode  :character   Mode  :character   Mode  :character                 \n                                                                        \n                                                                        \n                                                                        \n                                                                        \n HCPCS_CD_2     HCPCS_CD_3     HCPCS_CD_4     HCPCS_CD_5     HCPCS_CD_6    \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_7     HCPCS_CD_8     HCPCS_CD_9     HCPCS_CD_10    HCPCS_CD_11   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_12    HCPCS_CD_13    HCPCS_CD_14    HCPCS_CD_15    HCPCS_CD_16   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_17    HCPCS_CD_18    HCPCS_CD_19    HCPCS_CD_20    HCPCS_CD_21   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_22    HCPCS_CD_23    HCPCS_CD_24    HCPCS_CD_25    HCPCS_CD_26   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_27    HCPCS_CD_28    HCPCS_CD_29    HCPCS_CD_30    HCPCS_CD_31   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_32    HCPCS_CD_33    HCPCS_CD_34    HCPCS_CD_35    HCPCS_CD_36   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_37    HCPCS_CD_38    HCPCS_CD_39    HCPCS_CD_40    HCPCS_CD_41   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n HCPCS_CD_42    HCPCS_CD_43    HCPCS_CD_44    HCPCS_CD_45   \n Mode:logical   Mode:logical   Mode:logical   Mode:logical  \n NA's:66773     NA's:66773     NA's:66773     NA's:66773    \n                                                            \n                                                            \n                                                            \n                                                            \n                                                            \n\nDT::datatable(inp_labels)"
  },
  {
    "objectID": "notebooks/session4.html#adding-more-rows",
    "href": "notebooks/session4.html#adding-more-rows",
    "title": "Data wrangling I",
    "section": "Adding more rows",
    "text": "Adding more rows\nLet’s say we wanted to combine the 2008 and 2009 beneficiary data into a single column. We can accomplish this using the bind_rows function. Before doing this, let’s make sure that the two files contain the same columns. Any missing columns would be filled in with NA.\n\ncolnames(ben_summary_2008)\n\n [1] \"DESYNPUF_ID\"              \"BENE_BIRTH_DT\"           \n [3] \"BENE_DEATH_DT\"            \"BENE_SEX_IDENT_CD\"       \n [5] \"BENE_RACE_CD\"             \"BENE_ESRD_IND\"           \n [7] \"SP_STATE_CODE\"            \"BENE_COUNTY_CD\"          \n [9] \"BENE_HI_CVRAGE_TOT_MONS\"  \"BENE_SMI_CVRAGE_TOT_MONS\"\n[11] \"BENE_HMO_CVRAGE_TOT_MONS\" \"PLAN_CVRG_MOS_NUM\"       \n[13] \"SP_ALZHDMTA\"              \"SP_CHF\"                  \n[15] \"SP_CHRNKIDN\"              \"SP_CNCR\"                 \n[17] \"SP_COPD\"                  \"SP_DEPRESSN\"             \n[19] \"SP_DIABETES\"              \"SP_ISCHMCHT\"             \n[21] \"SP_OSTEOPRS\"              \"SP_RA_OA\"                \n[23] \"SP_STRKETIA\"              \"MEDREIMB_IP\"             \n[25] \"BENRES_IP\"                \"PPPYMT_IP\"               \n[27] \"MEDREIMB_OP\"              \"BENRES_OP\"               \n[29] \"PPPYMT_OP\"                \"MEDREIMB_CAR\"            \n[31] \"BENRES_CAR\"               \"PPPYMT_CAR\"              \n\ncolnames(ben_summary_2009)\n\n [1] \"DESYNPUF_ID\"              \"BENE_BIRTH_DT\"           \n [3] \"BENE_DEATH_DT\"            \"BENE_SEX_IDENT_CD\"       \n [5] \"BENE_RACE_CD\"             \"BENE_ESRD_IND\"           \n [7] \"SP_STATE_CODE\"            \"BENE_COUNTY_CD\"          \n [9] \"BENE_HI_CVRAGE_TOT_MONS\"  \"BENE_SMI_CVRAGE_TOT_MONS\"\n[11] \"BENE_HMO_CVRAGE_TOT_MONS\" \"PLAN_CVRG_MOS_NUM\"       \n[13] \"SP_ALZHDMTA\"              \"SP_CHF\"                  \n[15] \"SP_CHRNKIDN\"              \"SP_CNCR\"                 \n[17] \"SP_COPD\"                  \"SP_DEPRESSN\"             \n[19] \"SP_DIABETES\"              \"SP_ISCHMCHT\"             \n[21] \"SP_OSTEOPRS\"              \"SP_RA_OA\"                \n[23] \"SP_STRKETIA\"              \"MEDREIMB_IP\"             \n[25] \"BENRES_IP\"                \"PPPYMT_IP\"               \n[27] \"MEDREIMB_OP\"              \"BENRES_OP\"               \n[29] \"PPPYMT_OP\"                \"MEDREIMB_CAR\"            \n[31] \"BENRES_CAR\"               \"PPPYMT_CAR\"              \n\nall(colnames(ben_summary_2008) == colnames(ben_summary_2009))\n\n[1] TRUE\n\n\nSince everything is the same, let’s go ahead and combine the datasets.\n\nben_summary &lt;- bind_rows(ben_summary_2008, ben_summary_2009)\n\nWhile the data combined successfully, we likely have introduced duplicates into the table. We’ll see more on how to detect duplicates next session, but for now let’s remove any rows which are an exact match using the unique function.\n\nben_summary &lt;- unique(ben_summary)\n\nHowever, for the rest of the analysis let’s just focus on data from 2008, and just a subset of the total columns.\n\nben_summary &lt;- ben_summary_2008[,1:8]\ninp_data &lt;- inp_data[,1:20]"
  },
  {
    "objectID": "notebooks/session4.html#merging-datasets-by-matching-values",
    "href": "notebooks/session4.html#merging-datasets-by-matching-values",
    "title": "Data wrangling I",
    "section": "Merging datasets by matching values",
    "text": "Merging datasets by matching values\nNow we want to combine the beneficiary data with the inpatient data. We can use the various join functions in dyplr to combine the datasets in different ways.\n\nJoining\nThe dyplr functions inner_join, full_join, left_join, and right_join are similar to joins in other languages such as SQL. Their behavior differs in how they handle data which does not have a corresponding match in the other dataset.\nIn order to join we have to choose what to join on or by, that is, which variable in each dataset will be used to determine whether two rows are matching. In this example, we’ll want to join on DESYNPUF_ID.\nIn a full join, the SQL equivalent of an outer join, all observations from both datasets are kept.\n\nfull_result &lt;- full_join(ben_summary, inp_data, by = \"DESYNPUF_ID\")\nsummary(full_result)\n\n DESYNPUF_ID        BENE_BIRTH_DT      BENE_DEATH_DT      BENE_SEX_IDENT_CD\n Length:145345      Min.   :19090101   Min.   :20080101   Min.   :1.000    \n Class :character   1st Qu.:19280301   1st Qu.:20080401   1st Qu.:1.000    \n Mode  :character   Median :19351101   Median :20080701   Median :2.000    \n                    Mean   :19360775   Mean   :20080662   Mean   :1.555    \n                    3rd Qu.:19420201   3rd Qu.:20081001   3rd Qu.:2.000    \n                    Max.   :19831201   Max.   :20081201   Max.   :2.000    \n                                       NA's   :143476                      \n  BENE_RACE_CD   BENE_ESRD_IND      SP_STATE_CODE      BENE_COUNTY_CD    \n Min.   :1.000   Length:145345      Length:145345      Length:145345     \n 1st Qu.:1.000   Class :character   Class :character   Class :character  \n Median :1.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :1.277                                                           \n 3rd Qu.:1.000                                                           \n Max.   :5.000                                                           \n                                                                         \n     CLM_ID             SEGMENT       CLM_FROM_DT        CLM_THRU_DT      \n Min.   :1.960e+14   Min.   :1       Min.   :20071127   Min.   :20080101  \n 1st Qu.:1.963e+14   1st Qu.:1       1st Qu.:20080812   1st Qu.:20080818  \n Median :1.965e+14   Median :1       Median :20090316   Median :20090321  \n Mean   :1.965e+14   Mean   :1       Mean   :20088463   Mean   :20088611  \n 3rd Qu.:1.968e+14   3rd Qu.:1       3rd Qu.:20091112   3rd Qu.:20091117  \n Max.   :1.970e+14   Max.   :2       Max.   :20101230   Max.   :20101231  \n NA's   :78572       NA's   :78572   NA's   :78640      NA's   :78640     \n  PRVDR_NUM          CLM_PMT_AMT    NCH_PRMRY_PYR_CLM_PD_AMT AT_PHYSN_NPI      \n Length:145345      Min.   :-8000   Min.   :    0.0          Length:145345     \n Class :character   1st Qu.: 4000   1st Qu.:    0.0          Class :character  \n Mode  :character   Median : 7000   Median :    0.0          Mode  :character  \n                    Mean   : 9574   Mean   :  398.9                            \n                    3rd Qu.:11000   3rd Qu.:    0.0                            \n                    Max.   :57000   Max.   :68000.0                            \n                    NA's   :78572   NA's   :78572                              \n OP_PHYSN_NPI       OT_PHYSN_NPI        CLM_ADMSN_DT      ADMTNG_ICD9_DGNS_CD\n Length:145345      Length:145345      Min.   :20071127   Length:145345      \n Class :character   Class :character   1st Qu.:20080812   Class :character   \n Mode  :character   Mode  :character   Median :20090316   Mode  :character   \n                                       Mean   :20088463                      \n                                       3rd Qu.:20091112                      \n                                       Max.   :20101230                      \n                                       NA's   :78572                         \n CLM_PASS_THRU_PER_DIEM_AMT NCH_BENE_IP_DDCTBL_AMT\n Min.   :  0.00             Min.   :1024          \n 1st Qu.:  0.00             1st Qu.:1024          \n Median :  0.00             Median :1068          \n Mean   : 28.98             Mean   :1057          \n 3rd Qu.: 10.00             3rd Qu.:1068          \n Max.   :500.00             Max.   :1100          \n NA's   :78572              NA's   :80750         \n NCH_BENE_PTA_COINSRNC_LBLTY_AM NCH_BENE_BLOOD_DDCTBL_LBLTY_AM\n Min.   :    0.00               Min.   :   0.00               \n 1st Qu.:    0.00               1st Qu.:   0.00               \n Median :    0.00               Median :   0.00               \n Mean   :   90.03               Mean   :   1.59               \n 3rd Qu.:    0.00               3rd Qu.:   0.00               \n Max.   :34000.00               Max.   :2000.00               \n NA's   :78572                  NA's   :78572                 \n CLM_UTLZTN_DAY_CNT NCH_BENE_DSCHRG_DT  CLM_DRG_CD       \n Min.   :  0.00     Min.   :20080101   Length:145345     \n 1st Qu.:  2.00     1st Qu.:20080818   Class :character  \n Median :  4.00     Median :20090321   Mode  :character  \n Mean   :  5.58     Mean   :20088612                     \n 3rd Qu.:  7.00     3rd Qu.:20091117                     \n Max.   :136.00     Max.   :20101231                     \n NA's   :78640      NA's   :78572                        \n\n\nWe can see that this dataset has more rows than either the inpatient data or beneficiary table. This is because we now have rows both for patients in the beneficiaries table but not the inpatient table, and patients in the inpatient table but not the beneficiary table (since the beneficiary table is only from 2008, but the inpatient data is 2008-2010).\nWe could instead do a left join, where we keep all of the inpatient data but drop beneficiaries who had no inpatient data. In this case we expect there to be some drops, but if we knew everything should match we could also set the unmatched argument to \"error\". This will tell the function to cause an error if there are unmatched rows, as opposed to dropping them.\n\nleft_result &lt;- left_join(inp_data, ben_summary, by = \"DESYNPUF_ID\")\nsummary(left_result)\n\n DESYNPUF_ID            CLM_ID             SEGMENT       CLM_FROM_DT      \n Length:66773       Min.   :1.960e+14   Min.   :1.000   Min.   :20071127  \n Class :character   1st Qu.:1.963e+14   1st Qu.:1.000   1st Qu.:20080812  \n Mode  :character   Median :1.965e+14   Median :1.000   Median :20090316  \n                    Mean   :1.965e+14   Mean   :1.001   Mean   :20088463  \n                    3rd Qu.:1.968e+14   3rd Qu.:1.000   3rd Qu.:20091112  \n                    Max.   :1.970e+14   Max.   :2.000   Max.   :20101230  \n                                                        NA's   :68        \n  CLM_THRU_DT        PRVDR_NUM          CLM_PMT_AMT    NCH_PRMRY_PYR_CLM_PD_AMT\n Min.   :20080101   Length:66773       Min.   :-8000   Min.   :    0.0         \n 1st Qu.:20080818   Class :character   1st Qu.: 4000   1st Qu.:    0.0         \n Median :20090321   Mode  :character   Median : 7000   Median :    0.0         \n Mean   :20088611                      Mean   : 9574   Mean   :  398.9         \n 3rd Qu.:20091117                      3rd Qu.:11000   3rd Qu.:    0.0         \n Max.   :20101231                      Max.   :57000   Max.   :68000.0         \n NA's   :68                                                                    \n AT_PHYSN_NPI       OP_PHYSN_NPI       OT_PHYSN_NPI        CLM_ADMSN_DT     \n Length:66773       Length:66773       Length:66773       Min.   :20071127  \n Class :character   Class :character   Class :character   1st Qu.:20080812  \n Mode  :character   Mode  :character   Mode  :character   Median :20090316  \n                                                          Mean   :20088463  \n                                                          3rd Qu.:20091112  \n                                                          Max.   :20101230  \n                                                                            \n ADMTNG_ICD9_DGNS_CD CLM_PASS_THRU_PER_DIEM_AMT NCH_BENE_IP_DDCTBL_AMT\n Length:66773        Min.   :  0.00             Min.   :1024          \n Class :character    1st Qu.:  0.00             1st Qu.:1024          \n Mode  :character    Median :  0.00             Median :1068          \n                     Mean   : 28.98             Mean   :1057          \n                     3rd Qu.: 10.00             3rd Qu.:1068          \n                     Max.   :500.00             Max.   :1100          \n                                                NA's   :2178          \n NCH_BENE_PTA_COINSRNC_LBLTY_AM NCH_BENE_BLOOD_DDCTBL_LBLTY_AM\n Min.   :    0.00               Min.   :   0.00               \n 1st Qu.:    0.00               1st Qu.:   0.00               \n Median :    0.00               Median :   0.00               \n Mean   :   90.03               Mean   :   1.59               \n 3rd Qu.:    0.00               3rd Qu.:   0.00               \n Max.   :34000.00               Max.   :2000.00               \n                                                              \n CLM_UTLZTN_DAY_CNT NCH_BENE_DSCHRG_DT  CLM_DRG_CD        BENE_BIRTH_DT     \n Min.   :  0.000    Min.   :20080101   Length:66773       Min.   :19090101  \n 1st Qu.:  2.000    1st Qu.:20080818   Class :character   1st Qu.:19260801  \n Median :  4.000    Median :20090321   Mode  :character   Median :19340501  \n Mean   :  5.583    Mean   :20088612                      Mean   :19351193  \n 3rd Qu.:  7.000    3rd Qu.:20091117                      3rd Qu.:19411001  \n Max.   :136.000    Max.   :20101231                      Max.   :19831201  \n NA's   :68                                                                 \n BENE_DEATH_DT      BENE_SEX_IDENT_CD  BENE_RACE_CD   BENE_ESRD_IND     \n Min.   :20080301   Min.   :1.000     Min.   :1.000   Length:66773      \n 1st Qu.:20080701   1st Qu.:1.000     1st Qu.:1.000   Class :character  \n Median :20080901   Median :2.000     Median :1.000   Mode  :character  \n Mean   :20080879   Mean   :1.565     Mean   :1.245                     \n 3rd Qu.:20081101   3rd Qu.:2.000     3rd Qu.:1.000                     \n Max.   :20081201   Max.   :2.000     Max.   :5.000                     \n NA's   :66586                                                          \n SP_STATE_CODE      BENE_COUNTY_CD    \n Length:66773       Length:66773      \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n                                      \n\n\nThere are also inner joins, where only rows which match are kept. We’ll see more analysis with the synthetic CMS dataset next session."
  },
  {
    "objectID": "notebooks/session4.html#data",
    "href": "notebooks/session4.html#data",
    "title": "Data wrangling I",
    "section": "Data",
    "text": "Data\n\nnhanes &lt;- read.csv(\"../data/nhanes_diabetes.csv\")"
  },
  {
    "objectID": "notebooks/session4.html#mutate",
    "href": "notebooks/session4.html#mutate",
    "title": "Data wrangling I",
    "section": "Mutate",
    "text": "Mutate\nFrequently you’ll want to create new columns based on the values of existing columns, for example to do unit conversions, or to find the ratio of values in two columns. For this we’ll use mutate().\nTo create a new column of age in months:\n\nnhanes %&gt;%\n  mutate(age.months = RIDAGEYR * 12) %&gt;%\n  select(age.months, RIDAGEYR) %&gt;%\n  head()\n\n  age.months RIDAGEYR\n1          0        0\n2        132       11\n3        180       15\n4       1020       85\n5        528       44\n6        840       70\n\n\nYou can also create a second new column based on the first new column within the same call of mutate():\n\nnhanes %&gt;%\n  mutate(age.months = RIDAGEYR * 12,\n         lived_200_months = age.months &gt;= 200) %&gt;%\n  select(age.months, RIDAGEYR, lived_200_months) %&gt;%\n  head()\n\n  age.months RIDAGEYR lived_200_months\n1          0        0            FALSE\n2        132       11            FALSE\n3        180       15            FALSE\n4       1020       85             TRUE\n5        528       44             TRUE\n6        840       70             TRUE"
  },
  {
    "objectID": "notebooks/session4.html#split-apply-combine-data-analysis",
    "href": "notebooks/session4.html#split-apply-combine-data-analysis",
    "title": "Data wrangling I",
    "section": "Split-apply-combine data analysis",
    "text": "Split-apply-combine data analysis\nMany data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results. dplyr makes this very easy through the use of the group_by() function.\n\nnhanes %&gt;%\n  group_by(DMDBORN)\n\n# A tibble: 31,034 × 28\n# Groups:   DMDBORN [7]\n       X  SEQN RIDAGEYR RIAGENDR RIDRETH1      DMDBORN INDFMPIR SDMVPSU SDMVSTRA\n   &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;      &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt;\n 1     1 31127        0 Male     Non-Hispanic… Born i…     0.75       2       44\n 2     2 31128       11 Female   Non-Hispanic… Born i…     0.77       1       52\n 3     3 31129       15 Male     Non-Hispanic… Born i…     2.71       1       51\n 4     4 31130       85 Female   Non-Hispanic… Born i…     1.99       2       46\n 5     5 31131       44 Female   Non-Hispanic… Born i…     4.65       1       48\n 6     6 31132       70 Male     Non-Hispanic… Born i…     5          2       52\n 7     7 31133       16 Female   Non-Hispanic… Born i…     5          1       51\n 8     8 31134       73 Male     Non-Hispanic… Born i…    NA          2       48\n 9     9 31135        0 Male     Other Race -… Born i…     5          2       52\n10    10 31136       41 Female   Non-Hispanic… Born i…     3.57       2       51\n# ℹ 31,024 more rows\n# ℹ 19 more variables: WTINT2YR &lt;dbl&gt;, WTMEC2YR &lt;dbl&gt;, OHXDECAY &lt;lgl&gt;,\n#   OHXREST &lt;lgl&gt;, LBXGLU &lt;int&gt;, WTSAF2YR &lt;dbl&gt;, LBXGH &lt;dbl&gt;, BMXBMI &lt;dbl&gt;,\n#   Begin.Year &lt;int&gt;, EndYear &lt;int&gt;, age.cat &lt;chr&gt;, LBXGH.cat &lt;chr&gt;,\n#   RIDAGEYR.cat &lt;chr&gt;, LBXGLU.cat &lt;chr&gt;, BMXBMI.cat &lt;chr&gt;, INDFMPIR.cat &lt;chr&gt;,\n#   DMDBORN.cat &lt;chr&gt;, dental.caries &lt;lgl&gt;, diabetes &lt;chr&gt;\n\n\nThe group_by() function doesn’t perform any data processing, it groups the data into subsets: in the example above, our initial tibble of 31034 observations is split into 7 groups based on the DMDBORN variable.\nWe could similarly decide to group the tibble by sex:\n\nnhanes %&gt;%\n  group_by(RIAGENDR)\n\n# A tibble: 31,034 × 28\n# Groups:   RIAGENDR [2]\n       X  SEQN RIDAGEYR RIAGENDR RIDRETH1      DMDBORN INDFMPIR SDMVPSU SDMVSTRA\n   &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;      &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt;\n 1     1 31127        0 Male     Non-Hispanic… Born i…     0.75       2       44\n 2     2 31128       11 Female   Non-Hispanic… Born i…     0.77       1       52\n 3     3 31129       15 Male     Non-Hispanic… Born i…     2.71       1       51\n 4     4 31130       85 Female   Non-Hispanic… Born i…     1.99       2       46\n 5     5 31131       44 Female   Non-Hispanic… Born i…     4.65       1       48\n 6     6 31132       70 Male     Non-Hispanic… Born i…     5          2       52\n 7     7 31133       16 Female   Non-Hispanic… Born i…     5          1       51\n 8     8 31134       73 Male     Non-Hispanic… Born i…    NA          2       48\n 9     9 31135        0 Male     Other Race -… Born i…     5          2       52\n10    10 31136       41 Female   Non-Hispanic… Born i…     3.57       2       51\n# ℹ 31,024 more rows\n# ℹ 19 more variables: WTINT2YR &lt;dbl&gt;, WTMEC2YR &lt;dbl&gt;, OHXDECAY &lt;lgl&gt;,\n#   OHXREST &lt;lgl&gt;, LBXGLU &lt;int&gt;, WTSAF2YR &lt;dbl&gt;, LBXGH &lt;dbl&gt;, BMXBMI &lt;dbl&gt;,\n#   Begin.Year &lt;int&gt;, EndYear &lt;int&gt;, age.cat &lt;chr&gt;, LBXGH.cat &lt;chr&gt;,\n#   RIDAGEYR.cat &lt;chr&gt;, LBXGLU.cat &lt;chr&gt;, BMXBMI.cat &lt;chr&gt;, INDFMPIR.cat &lt;chr&gt;,\n#   DMDBORN.cat &lt;chr&gt;, dental.caries &lt;lgl&gt;, diabetes &lt;chr&gt;\n\n\nOnce the data has been grouped, subsequent operations will be applied on each group independently.\n\nThe summarise() function\ngroup_by() is often used together with summarise(), which collapses each group into a single-row summary of that group.\ngroup_by() takes as arguments the column names that contain the categorical variables for which you want to calculate the summary statistics. So to compute the mean BMXBMI by birthplace:\n\nnhanes %&gt;%\n  group_by(DMDBORN) %&gt;%\n  summarise(mean_bmi = mean(BMXBMI, na.rm = TRUE))\n\n# A tibble: 7 × 2\n  DMDBORN                        mean_bmi\n  &lt;chr&gt;                             &lt;dbl&gt;\n1 Born Elsewhere                     25.4\n2 Born in 50 US States or Washi      25.3\n3 Born in Mexico                     27.5\n4 Born in Other Non-Spanish Spea     25.8\n5 Born in Other Spanish Speaking     27.9\n6 Don't Know                         22.9\n7 Refused                            23.8\n\n\nWe can can also group by multiple columns:\n\nnhanes %&gt;%\n  group_by(RIAGENDR, RIDRETH1) %&gt;%\n  summarise(mean_bmi = mean(BMXBMI, na.rm = TRUE))\n\n`summarise()` has grouped output by 'RIAGENDR'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 10 × 3\n# Groups:   RIAGENDR [2]\n   RIAGENDR RIDRETH1                       mean_bmi\n   &lt;chr&gt;    &lt;chr&gt;                             &lt;dbl&gt;\n 1 Female   Mexican American                   25.3\n 2 Female   Non-Hispanic Black                 27.3\n 3 Female   Non-Hispanic White                 25.9\n 4 Female   Other Hispanic                     25.8\n 5 Female   Other Race - Including Multi-R     23.2\n 6 Male     Mexican American                   24.6\n 7 Male     Non-Hispanic Black                 25.1\n 8 Male     Non-Hispanic White                 25.9\n 9 Male     Other Hispanic                     25.1\n10 Male     Other Race - Including Multi-R     23.0\n\n\nOnce the data is grouped, you can also summarise multiple variables at the same time (and not necessarily on the same variable). For instance, we could add a column indicating the median plasma glucose by sex and ethnicity:\n\nnhanes %&gt;%\n  group_by(RIAGENDR, RIDRETH1) %&gt;%\n  summarise(mean_plasma_glucose = mean(LBXGLU),\n            median_plasma_glucose = median(LBXGLU))\n\n`summarise()` has grouped output by 'RIAGENDR'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 10 × 4\n# Groups:   RIAGENDR [2]\n   RIAGENDR RIDRETH1                   mean_plasma_glucose median_plasma_glucose\n   &lt;chr&gt;    &lt;chr&gt;                                    &lt;dbl&gt;                 &lt;int&gt;\n 1 Female   Mexican American                            NA                    NA\n 2 Female   Non-Hispanic Black                          NA                    NA\n 3 Female   Non-Hispanic White                          NA                    NA\n 4 Female   Other Hispanic                              NA                    NA\n 5 Female   Other Race - Including Mu…                  NA                    NA\n 6 Male     Mexican American                            NA                    NA\n 7 Male     Non-Hispanic Black                          NA                    NA\n 8 Male     Non-Hispanic White                          NA                    NA\n 9 Male     Other Hispanic                              NA                    NA\n10 Male     Other Race - Including Mu…                  NA                    NA\n\n\n\n\nCounting\nWhen working with data, we often want to know the number of observations found for each factor or combination of factors. For this task, dplyr provides count(). For example, if we wanted to count the number of rows of data for each age, we would do:\n\nnhanes %&gt;%\n    count(RIDAGEYR) %&gt;%\n    datatable()\n\n\n\n\n\n\nThe count() function is shorthand for something we’ve already seen: grouping by a variable, and summarising it by counting the number of observations in that group. In other words, nhanes %&gt;% count(age.years) is equivalent to:\n\nnhanes %&gt;%\n    group_by(RIDAGEYR) %&gt;%\n    summarise(n = n())\n\n# A tibble: 86 × 2\n   RIDAGEYR     n\n      &lt;int&gt; &lt;int&gt;\n 1        0  1437\n 2        1   987\n 3        2   990\n 4        3   621\n 5        4   716\n 6        5   622\n 7        6   610\n 8        7   606\n 9        8   629\n10        9   593\n# ℹ 76 more rows\n\n\nThe previous example shows the use of count() to count the number of rows/observations for one factor (i.e., infection). If we wanted to count a combination of factors, such as age and sex, we would specify the first and the second factor as the arguments of count():\n\nnhanes %&gt;%\n    count(RIDAGEYR, RIAGENDR)  %&gt;%\n    datatable()\n\n\n\n\n\n\nIt is sometimes useful to sort the result to facilitate the comparisons. We can use arrange() to sort the table. For instance, we might want to arrange the table above by age:\n\nnhanes %&gt;%\n    count(RIDAGEYR, RIAGENDR) %&gt;%\n    arrange(RIDAGEYR)  %&gt;%\n    datatable()\n\n\n\n\n\n\nor by counts:\n\nnhanes %&gt;%\n    count(RIDAGEYR, RIAGENDR) %&gt;%\n    arrange(n)  %&gt;%\n    datatable()\n\n\n\n\n\n\nTo sort in descending order, we need to add the desc() function:\n\nnhanes %&gt;%\n    count(RIDAGEYR, RIAGENDR) %&gt;%\n    arrange(desc(n))  %&gt;%\n    datatable()"
  },
  {
    "objectID": "notebooks/session4.html#numeric-variables",
    "href": "notebooks/session4.html#numeric-variables",
    "title": "Data wrangling I",
    "section": "Numeric variables",
    "text": "Numeric variables\nLet’s start by converting numeric columns:\n\n# Set numeric columns to be numeric\nnhanes_clean &lt;- mutate(nhanes_raw,\n                   RIDAGEYR = as.numeric(RIDAGEYR),\n                   LBXGLU = as.numeric(LBXGLU),\n                   LBXGH = as.numeric(LBXGH),\n                   BMXBMI = as.numeric(BMXBMI))"
  },
  {
    "objectID": "notebooks/session4.html#categorical-variables",
    "href": "notebooks/session4.html#categorical-variables",
    "title": "Data wrangling I",
    "section": "Categorical variables",
    "text": "Categorical variables\nMany of the NHANES values are categorical data, but right now are stored as text. We can check what values exist by converting them to factors before making the decision of how to handle them in the analysis:\n\nnhanes_clean |&gt;\n  mutate(across(c(RIAGENDR, RIDRETH1, DMDBORN), as.factor), .keep = \"none\") |&gt;\n  summary()\n\n   RIAGENDR                               RIDRETH1    \n Female:15633   Mexican American              : 7388  \n Male  :15401   Non-Hispanic Black            : 6878  \n                Non-Hispanic White            :12463  \n                Other Hispanic                : 2683  \n                Other Race - Including Multi-R: 1622  \n                                                      \n                                                      \n                           DMDBORN     \n \"Born in 50 US States or Washi:25732  \n Born Elsewhere                :  588  \n Born in Mexico                : 2511  \n Born in Other Non-Spanish Spea: 1065  \n Born in Other Spanish Speaking: 1120  \n Don't Know                    :    2  \n Refused                       :   16  \n\n\nLet’s recode the “Other Race - Including Multi-R” ethnicity value.\n\nnhanes_clean &lt;- nhanes_clean |&gt;\n  mutate(RIDRETH1 = \n           recode(RIDRETH1, \"Other Race - Including Multi-R\" = \"Other/Multi-Racial\"))\n\nIt looks like one of the options for DMDBORN has a lingering quote character. This can happen due to irregularities in how NHANES data is presented or small mistakes in data processing. We use the gsub function to replace all instances of double quotes with the empty string in the column.\n\nnhanes_clean &lt;- mutate(nhanes_clean,\n                   RIAGENDR = as.factor(RIAGENDR),\n                   RIDRETH1 = as.factor(RIDRETH1),\n                   DMDBORN = gsub(\"\\\"\", \"\", DMDBORN), # Remove quotes\n                   DMDBORN = as.factor(DMDBORN),\n                   OHXDECAY = (OHXDECAY == \"Yes\"),\n                   OHXREST = (OHXREST == \"Yes\"))\n\ngsub is a useful function which will replace all matching string instances based on a regular expression. There is a corresponding sub function which only replaces the first match. Regular expressions in R are mostly identical to other implementations. There is a cheatsheet on string manipulation in R, including a regular expression reference, here."
  },
  {
    "objectID": "notebooks/session4.html#new-variables",
    "href": "notebooks/session4.html#new-variables",
    "title": "Data wrangling I",
    "section": "New variables",
    "text": "New variables\n\nMaking ages with cut\nWe might want to make age categories for our data in addition to having age in years. We can do this using the cut function in base R.\n\nmutate(nhanes_raw,\n        age.cat = cut(\n          RIDAGEYR,\n          breaks = c(0, 18, 65, max(RIDAGEYR)),\n          include.lowest = TRUE,\n          right = FALSE,\n          labels = c(\"Under 18\",\"18-65\",\"65+\")\n          )) |&gt;\n  group_by(age.cat) |&gt; count()\n\n# A tibble: 3 × 2\n# Groups:   age.cat [3]\n  age.cat      n\n  &lt;fct&gt;    &lt;int&gt;\n1 Under 18 12716\n2 18-65    14050\n3 65+       4268\n\n\nWe can also generate categories using seq and labels using paste0.\n\nage_seq &lt;- seq(from = 0, to = 90, by = 10)\nmutate(nhanes_raw,\n        age.cat = cut(\n          RIDAGEYR,\n          breaks = age_seq,\n          include.lowest = TRUE,\n          labels = paste0(age_seq[1:9] + 1, \"-\", age_seq[1:9] + 10)\n          )) |&gt;\n  group_by(age.cat) |&gt; count()\n\n# A tibble: 9 × 2\n# Groups:   age.cat [9]\n  age.cat     n\n  &lt;fct&gt;   &lt;int&gt;\n1 1-10     8421\n2 11-20    5776\n3 21-30    3005\n4 31-40    2949\n5 41-50    2864\n6 51-60    2579\n7 61-70    2517\n8 71-80    2575\n9 81-90     348\n\n\nWe can also make quantile breaks for numeric values using the quantile function.\n\nmutate(nhanes_raw,\n        glucose.cat = cut(\n          LBXGLU,\n          breaks = quantile(\n            LBXGLU,\n            probs = seq(0, 1, by = 0.1),\n            na.rm = TRUE\n          ),\n          include.lowest = TRUE,\n          )) |&gt;\n  group_by(glucose.cat) |&gt; count()\n\n# A tibble: 11 × 2\n# Groups:   glucose.cat [11]\n   glucose.cat     n\n   &lt;fct&gt;       &lt;int&gt;\n 1 [36,86]      1145\n 2 (86,90]      1021\n 3 (90,93]       990\n 4 (93,95]       763\n 5 (95,98]      1036\n 6 (98,101]      994\n 7 (101,105]     994\n 8 (105,111]     864\n 9 (111,125]     868\n10 (125,584]     949\n11 &lt;NA&gt;        21410\n\n\nIf we want to make more complex categories, we can use the base R ifelse function or case_when in tidyverse.\n\n# Set columns to categories as in the paper\nnhanes_clean &lt;- mutate(nhanes_clean,\n                    age.cat = cut(\n                      RIDAGEYR,\n                      breaks = c(13, 15, 18, 100),\n                      include.lowest = TRUE,\n                      labels = c(\"13-15\", \"16-18\", \"19+\")),\n                   \n                    plasma.glucose.cat = case_when(\n                     LBXGLU &lt; 100 ~ \"&lt;100 mg/dl\",\n                     LBXGLU &lt; 126 ~ \"&gt;=100 mg/dl and &lt;126 mg/dl\", \n                     LBXGLU &gt;= 126 ~ \"&gt;=126 mg/dl\",\n                     .default = NA),\n                   \n                   hba1c.cat = case_when(\n                     LBXGH &lt; 5.7 ~ \"&lt;5.7%\",\n                     LBXGH &gt;= 5.7 ~ \"&gt;=5.7% and &lt;6.5%\",\n                     LBXGH &gt;= 6.5 ~ \"&gt;= 6.5%\",\n                     .default = NA),\n                   \n                   bmi.cat = case_when( \n                     BMXBMI &lt; 25 ~ \"Normal\", \n                     BMXBMI &lt; 30 ~ \"Overweight\",\n                     BMXBMI &gt;= 30 ~ \"Obese\",\n                     .default = NA), \n                   \n                   family.PIR.cat = case_when(\n                     INDFMPIR == \"PIR value greater than or equa\" ~ \"&gt;= 1\",\n                     INDFMPIR == \"Value greater than or equal to\" ~ \"&gt;= 1\",\n                     as.numeric(INDFMPIR) &gt;= 1 ~ \"&gt;=1\",\n                     as.numeric(INDFMPIR) &lt; 1 ~ \"&lt;1\",\n                     .default = NA),\n                   \n                   birthplace = case_when(\n                     DMDBORN == \"Born in 50 US States or Washi\" ~ \"Within the US\",\n                     is.na(DMDBORN) ~ NA,\n                     DMDBORN == \"Don't Know\" ~ NA,\n                     DMDBORN == \"Refused\" ~ NA,\n                    .default = \"Outside the US\"),\n                   dental.caries = OHXDECAY | OHXREST)\n\nUsing ifelse:\n\n# Add diabetes column\nnhanes_clean &lt;- nhanes_clean |&gt; \n          mutate(diabetes = \n           ifelse(LBXGH &gt;= 6.5 | LBXGLU &gt;= 126, \"diabetic\",\n           ifelse(LBXGH &gt;= 5.7 | LBXGLU &gt;= 100, \"nondiabetic\",\n           ifelse(is.na(is.na(LBXGH) & is.na(LBXGLU)),NA,\n                  \"nondiabetic\"))))"
  },
  {
    "objectID": "notebooks/session4.html#combining-and-joining-strings",
    "href": "notebooks/session4.html#combining-and-joining-strings",
    "title": "Data wrangling I",
    "section": "Combining and joining strings",
    "text": "Combining and joining strings\nLet’s go through some common string manipulations in R. We’ve already seen gsub and sub. We will now be using functions from the stringr package, which provide a consistent naming structure.\nFirst, we can use str_c to combine strings.\n\nstr_c(\"String1\", \"String2\", \"String3\")\n\n[1] \"String1String2String3\"\n\nstr_c(\"String1\", \"String2\", \"String3\", sep = \", \")\n\n[1] \"String1, String2, String3\"\n\n\nVectors of strings will be combined by element, and we can specify both seq and collapse.\n\nfirst_names &lt;- c(\"abdul\", \"fahruk\", \"janice\") \nlast_names  &lt;- c(\"hussein\", \"akinleye\", \"okeke\")\n\n# sep displays between the respective input strings, while collapse displays between the elements produced\nstr_c(first_names, last_names, sep = \" \", collapse = \";  \")\n\n[1] \"abdul hussein;  fahruk akinleye;  janice okeke\"\n\n\nWe can split strings with str_split.\n\nstr_split(string = \"jaundice, fever, chills\",\n          pattern = \",\")\n\n[[1]]\n[1] \"jaundice\" \" fever\"   \" chills\""
  },
  {
    "objectID": "notebooks/session4.html#combining-and-joining-columns",
    "href": "notebooks/session4.html#combining-and-joining-columns",
    "title": "Data wrangling I",
    "section": "Combining and joining columns",
    "text": "Combining and joining columns\nLet’s make a small dataframe to demonstrate splitting and combining on columns.\n\n# Make a small example dataframe\n\ndf &lt;- data.frame(\n  case_ID = c(1:6),\n  symptoms  = c(\"jaundice, fever, chills\",     # patient 1\n                \"chills, aches, pains\",        # patient 2 \n                \"fever\",                       # patient 3\n                \"vomiting, diarrhoea\",         # patient 4\n                \"bleeding from gums, fever\",   # patient 5\n                \"rapid pulse, headache\"),      # patient 6\n  outcome = c(\"Recover\", \"Death\", \"Death\", \"Recover\", \"Recover\", \"Recover\"))\n\ndatatable(df)\n\n\n\n\n\n\nWe can split the symptoms column using separate:\n\ndf_split &lt;- separate(df, symptoms, into = c(\"sym_1\", \"sym_2\", \"sym_3\"), extra = \"merge\")\n\nWarning: Expected 3 pieces. Missing pieces filled with `NA` in 2 rows [3, 4].\n\n\nOr, if we wanted to combine columns together, we can use unite:\n\ndf_split %&gt;% \n  unite(\n    col = \"all_symptoms\",         # name of the new united column\n    c(\"sym_1\", \"sym_2\", \"sym_3\"), # columns to unite\n    sep = \", \",                   # separator to use in united column\n    remove = TRUE,                # if TRUE, removes input cols from the data frame\n    na.rm = TRUE                  # if TRUE, missing values are removed before uniting\n  )\n\n  case_ID                all_symptoms outcome\n1       1     jaundice, fever, chills Recover\n2       2        chills, aches, pains   Death\n3       3                       fever   Death\n4       4         vomiting, diarrhoea Recover\n5       5 bleeding, from, gums, fever Recover\n6       6      rapid, pulse, headache Recover"
  },
  {
    "objectID": "notebooks/session4.html#standardizing-strings",
    "href": "notebooks/session4.html#standardizing-strings",
    "title": "Data wrangling I",
    "section": "Standardizing Strings",
    "text": "Standardizing Strings\nThere are a lot of useful functions for manipulating strings we won’t have time to demonstrate. You can find information on the use of these at this link."
  },
  {
    "objectID": "notebooks/session4.html#patterns",
    "href": "notebooks/session4.html#patterns",
    "title": "Data wrangling I",
    "section": "Patterns",
    "text": "Patterns\nTo find the presence of a pattern, we can use str_detect.\n\nstr_detect(string = \"primary school teacher\", pattern = \"teach\")\n\n[1] TRUE\n\nstr_detect(string = \"Teacher\", pattern = regex(\"teach\", ignore_case = T))\n\n[1] TRUE"
  },
  {
    "objectID": "notebooks/session4.html#data-1",
    "href": "notebooks/session4.html#data-1",
    "title": "Data wrangling I",
    "section": "Data",
    "text": "Data\nLet’s load in a simulated ebola outbread dataset to take a look at some other cleaning operations in R.\n\nlibrary(readxl)\nlinelist_raw &lt;- read_excel(\"../data/linelist_raw.xlsx\")\n\nNew names:\n• `` -&gt; `...28`\n\ndatatable(head(linelist_raw, 50))\n\n\n\n\n\n\nWe can also quickly summarize our data using the skimr package.\n\nskimr::skim(linelist_raw)\n\n\nData summary\n\n\nName\nlinelist_raw\n\n\nNumber of rows\n6611\n\n\nNumber of columns\n28\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n17\n\n\nnumeric\n8\n\n\nPOSIXct\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncase_id\n137\n0.98\n6\n6\n0\n5888\n0\n\n\ndate onset\n293\n0.96\n10\n10\n0\n580\n0\n\n\noutcome\n1500\n0.77\n5\n7\n0\n2\n0\n\n\ngender\n324\n0.95\n1\n1\n0\n2\n0\n\n\nhospital\n1512\n0.77\n5\n36\n0\n13\n0\n\n\ninfector\n2323\n0.65\n6\n6\n0\n2697\n0\n\n\nsource\n2323\n0.65\n5\n7\n0\n2\n0\n\n\nage\n107\n0.98\n1\n2\n0\n75\n0\n\n\nage_unit\n7\n1.00\n5\n6\n0\n2\n0\n\n\nfever\n258\n0.96\n2\n3\n0\n2\n0\n\n\nchills\n258\n0.96\n2\n3\n0\n2\n0\n\n\ncough\n258\n0.96\n2\n3\n0\n2\n0\n\n\naches\n258\n0.96\n2\n3\n0\n2\n0\n\n\nvomit\n258\n0.96\n2\n3\n0\n2\n0\n\n\ntime_admission\n844\n0.87\n5\n5\n0\n1091\n0\n\n\nmerged_header\n0\n1.00\n1\n1\n0\n1\n0\n\n\n…28\n0\n1.00\n1\n1\n0\n1\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngeneration\n7\n1.00\n16.60\n5.71\n0.00\n13.00\n16.00\n20.00\n37.00\n▁▆▇▂▁\n\n\nlon\n7\n1.00\n-13.23\n0.02\n-13.27\n-13.25\n-13.23\n-13.22\n-13.21\n▅▃▃▅▇\n\n\nlat\n7\n1.00\n8.47\n0.01\n8.45\n8.46\n8.47\n8.48\n8.49\n▅▇▇▇▆\n\n\nrow_num\n0\n1.00\n3240.91\n1857.83\n1.00\n1647.50\n3241.00\n4836.50\n6481.00\n▇▇▇▇▇\n\n\nwt_kg\n7\n1.00\n52.69\n18.59\n-11.00\n41.00\n54.00\n66.00\n111.00\n▁▃▇▅▁\n\n\nht_cm\n7\n1.00\n125.25\n49.57\n4.00\n91.00\n130.00\n159.00\n295.00\n▂▅▇▂▁\n\n\nct_blood\n7\n1.00\n21.26\n1.67\n16.00\n20.00\n22.00\n22.00\n26.00\n▁▃▇▃▁\n\n\ntemp\n158\n0.98\n38.60\n0.95\n35.20\n38.30\n38.80\n39.20\n40.80\n▁▂▂▇▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ninfection date\n2322\n0.65\n2012-04-09\n2015-04-27\n2014-10-04\n538\n\n\nhosp date\n7\n1.00\n2012-04-20\n2015-04-30\n2014-10-15\n570\n\n\ndate_of_outcome\n1068\n0.84\n2012-05-14\n2015-06-04\n2014-10-26\n575"
  },
  {
    "objectID": "notebooks/session4.html#cleaning-column-names",
    "href": "notebooks/session4.html#cleaning-column-names",
    "title": "Data wrangling I",
    "section": "Cleaning column names",
    "text": "Cleaning column names\nThe columns names of linelist_raw are printed below using names() from base R. We can see that initially:\n\nSome names contain spaces (e.g. infection date)\n\nDifferent naming patterns are used for dates (date onset vs. infection date)\n\nThere must have been a merged header across the two last columns in the .xlsx. We know this because the name of two merged columns (“merged_header”) was assigned by R to the first column, and the second column was assigned a placeholder name “…28” (as it was then empty and is the 28th column).\n\n\nnames(linelist_raw)\n\n [1] \"case_id\"         \"generation\"      \"infection date\"  \"date onset\"     \n [5] \"hosp date\"       \"date_of_outcome\" \"outcome\"         \"gender\"         \n [9] \"hospital\"        \"lon\"             \"lat\"             \"infector\"       \n[13] \"source\"          \"age\"             \"age_unit\"        \"row_num\"        \n[17] \"wt_kg\"           \"ht_cm\"           \"ct_blood\"        \"fever\"          \n[21] \"chills\"          \"cough\"           \"aches\"           \"vomit\"          \n[25] \"temp\"            \"time_admission\"  \"merged_header\"   \"...28\"          \n\n\n\nAutomatic cleaning\nThe function clean_names() from the package janitor standardizes column names and makes them unique by doing the following:\n\nConverts all names to consist of only underscores, numbers, and letters\n\nAccented characters are transliterated to ASCII (e.g. german o with umlaut becomes “o”, spanish “enye” becomes “n”)\n\nCapitalization preference for the new column names can be specified using the case = argument (“snake” is default, alternatives include “sentence”, “title”, “small_camel”…)\n\nYou can specify specific name replacements by providing a vector to the replace = argument (e.g. replace = c(onset = \"date_of_onset\"))\n\nBelow, the cleaning pipeline begins by using clean_names() on the raw linelist.\n\n# pipe the raw dataset through the function clean_names(), assign result as \"linelist\"  \nlinelist &lt;- linelist_raw %&gt;% \n  janitor::clean_names()\n\n# see the new column names\nnames(linelist)\n\n [1] \"case_id\"         \"generation\"      \"infection_date\"  \"date_onset\"     \n [5] \"hosp_date\"       \"date_of_outcome\" \"outcome\"         \"gender\"         \n [9] \"hospital\"        \"lon\"             \"lat\"             \"infector\"       \n[13] \"source\"          \"age\"             \"age_unit\"        \"row_num\"        \n[17] \"wt_kg\"           \"ht_cm\"           \"ct_blood\"        \"fever\"          \n[21] \"chills\"          \"cough\"           \"aches\"           \"vomit\"          \n[25] \"temp\"            \"time_admission\"  \"merged_header\"   \"x28\"            \n\n\n\n\nManual Cleaning\nRe-naming columns manually is often necessary, even after the standardization step above. Below, re-naming is performed using the rename() function from the dplyr package, as part of a pipe chain. rename() uses the style NEW = OLD - the new column name is given before the old column name.\nBelow, a re-naming command is added to the cleaning pipeline. Spaces have been added strategically to align code for easier reading.\n\nlinelist &lt;- linelist_raw %&gt;%\n    \n    # standardize column name syntax\n    janitor::clean_names() %&gt;% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome)\n\nNow you can see that the columns names have been changed:\n\n\n [1] \"case_id\"              \"generation\"           \"date_infection\"      \n [4] \"date_onset\"           \"date_hospitalisation\" \"date_outcome\"        \n [7] \"outcome\"              \"gender\"               \"hospital\"            \n[10] \"lon\"                  \"lat\"                  \"infector\"            \n[13] \"source\"               \"age\"                  \"age_unit\"            \n[16] \"row_num\"              \"wt_kg\"                \"ht_cm\"               \n[19] \"ct_blood\"             \"fever\"                \"chills\"              \n[22] \"cough\"                \"aches\"                \"vomit\"               \n[25] \"temp\"                 \"time_admission\"       \"merged_header\"       \n[28] \"x28\""
  },
  {
    "objectID": "notebooks/session4.html#advanced-column-selection",
    "href": "notebooks/session4.html#advanced-column-selection",
    "title": "Data wrangling I",
    "section": "Advanced Column Selection",
    "text": "Advanced Column Selection\nWe’ve seen how to select individual columns by name. Tidyverse has a number of helper functions which let us select columns in more complex ways:\nUse where() to specify logical criteria for columns. If providing a function inside where(), do not include the function’s empty parentheses. The command below selects columns that are class Numeric.\n\n# select columns that are class Numeric\nlinelist %&gt;% \n  select(where(is.numeric)) %&gt;% \n  names()\n\n[1] \"generation\" \"lon\"        \"lat\"        \"row_num\"    \"wt_kg\"     \n[6] \"ht_cm\"      \"ct_blood\"   \"temp\"      \n\n\nUse contains() to select only columns in which the column name contains a specified character string. ends_with() and starts_with() provide more nuance.\n\n# select columns containing certain characters\nlinelist %&gt;% \n  select(contains(\"date\")) %&gt;% \n  names()\n\n[1] \"date_infection\"       \"date_onset\"           \"date_hospitalisation\"\n[4] \"date_outcome\"        \n\n\nThe function matches() works similarly to contains() but can be provided a regular expression, such as multiple strings separated by OR bars within the parentheses:\n\n# searched for multiple character matches\nlinelist %&gt;% \n  select(matches(\"onset|hosp|fev\")) %&gt;%   # note the OR symbol \"|\"\n  names()\n\n[1] \"date_onset\"           \"date_hospitalisation\" \"hospital\"            \n[4] \"fever\""
  },
  {
    "objectID": "notebooks/sessions.html",
    "href": "notebooks/sessions.html",
    "title": "Sessions",
    "section": "",
    "text": "Sessions\nThis set of pages contains the in-class materials we will be using."
  },
  {
    "objectID": "resources/install.html",
    "href": "resources/install.html",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "Open an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\n\n\n\n\n\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the executable file. Run the .exe file and follow the installation instructions.\n\nPermissions\nNote that you should install R and RStudio to a drive where you have read and write permissions. Otherwise, your ability to install R packages (a frequent occurrence) will be impacted. If you encounter problems, try opening RStudio by right-clicking the icon and selecting “Run as administrator”. Other tips can be found in the page [R on network drives].\nHow to update R and RStudio\nYour version of R is printed to the R Console at start-up. You can also run sessionInfo().\nTo update R, go to the website mentioned above and re-install R. Be aware that the old R version will still exist in your computer. You can temporarily run an older version (older “installation”) of R by clicking “Tools” -&gt; “Global Options” in RStudio and choosing an R version. This can be useful if you want to use a package that has not been updated to work on the newest version of R.\nTo update RStudio, you can go to the website above and re-download RStudio. Another option is to click “Help” -&gt; “Check for Updates” within RStudio, but this may not show the very latest updates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions adapted from guide developed by HMS Research computing and Chapter 3 of the The Epidemiologist R Handbook."
  },
  {
    "objectID": "resources/install.html#mac-users",
    "href": "resources/install.html#mac-users",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "Open an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder."
  },
  {
    "objectID": "resources/install.html#windows-users",
    "href": "resources/install.html#windows-users",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "Open an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the executable file. Run the .exe file and follow the installation instructions.\n\nPermissions\nNote that you should install R and RStudio to a drive where you have read and write permissions. Otherwise, your ability to install R packages (a frequent occurrence) will be impacted. If you encounter problems, try opening RStudio by right-clicking the icon and selecting “Run as administrator”. Other tips can be found in the page [R on network drives].\nHow to update R and RStudio\nYour version of R is printed to the R Console at start-up. You can also run sessionInfo().\nTo update R, go to the website mentioned above and re-install R. Be aware that the old R version will still exist in your computer. You can temporarily run an older version (older “installation”) of R by clicking “Tools” -&gt; “Global Options” in RStudio and choosing an R version. This can be useful if you want to use a package that has not been updated to work on the newest version of R.\nTo update RStudio, you can go to the website above and re-download RStudio. Another option is to click “Help” -&gt; “Check for Updates” within RStudio, but this may not show the very latest updates."
  },
  {
    "objectID": "resources/install.html#reference",
    "href": "resources/install.html#reference",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "Instructions adapted from guide developed by HMS Research computing and Chapter 3 of the The Epidemiologist R Handbook."
  }
]