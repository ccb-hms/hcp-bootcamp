---
title: "Modeling and Testing"
format: 
  html: default
code-annotations: select
---

# Data

We import the dataset of cases from a simulated Ebola epidemic. 

```{r}
library(tidyverse)
library(DT)

set.seed(1) # Set the random seed
# set.seed(Sys.time()) # Useful if you want to "unset" the seed

linelist <- readRDS("../data/linelist_cleaned.rds")
```

If we always only ever look at the first rows of a dataset, we might miss other patterns later on. Thus, this time let's look at a random 100 samples of this dataset. 

```{r}
# Base R way
datatable(linelist[sample(nrow(linelist), 100),], filter = "top")

# Tidyverse way
linelist |>
  slice_sample(n = 100) |>
  datatable(filter = "top")
```

# Distributions in R

Every probability distribution in R has four functions, with a root distribution name and a single letter prefix:

- p for "probability", the cumulative distribution function (c. d. f.)
- q for "quantile", the inverse c. d. f.
- d for "density", the density function (p. f. or p. d. f.)
- r for "random", a random variable having the specified distribution 

For instance, for the binomial distribution we have

```{r}
p <- 0.3
n <- 10

# Probability of seeing at most 5 successes
pbinom(5, size=n, prob=p)

# Get the 50th percentile of successes
qbinom(0.5, size=n, prob=p)

# The probability of seeing exactly 5 successes
dbinom(5, size=n, prob=p)

# Generate 40 random samples from this binomial distribution
set.seed(10)
rbinom(40, size=n, prob=p)
```

# Performing simple tests

R has built-in functionality to perform common statistical tests. 
The functions take the form of `NAME.test`, such as `t.test`, `shapiro.test`, `binomial.test`, `wilcox.test`, etc. 

Lets run a t-test:

```{r}
# Specify data
t.test(age_years ~ gender, data = linelist)
```

The formula syntax `age_years ~ gender` is used across different models and statistical methods in R. 
For a t-test, we want to put the numeric column on the left side of the equation and the categorical column on the right side. 

`t.test` also has optional arguments to specify variations, such as if a test is paired or if the alternative hypothesis is single sided. We can perform a single-sample t-test against a specific value by using the argument `mu`. 

```{r}
t.test(linelist$age_years, mu = 25)
```

The results of a test can be stored to get individual values. 

```{r}
result <- t.test(age_years ~ gender, data = linelist)
names(result)
```
```{r}
result$p.value
```


# Regression

## Example Analysis - Infant Mortality 

```{r}
infdeath = read.table( "../data/infdeath.txt", head=TRUE, sep="\t")
dim(infdeath)
colnames(infdeath)
```

We have 20 measurements on - `safe`=access to safe water - `breast`= percentage of mothers breast feeding at 6 months - `death` = infant death rate

When performing any time of analysis where we look at the relationship between two variables, we need to be careful to not mix up correlation and causation.

```{r}
plot(infdeath$breast, infdeath$death, xlab="% Breast Feeding", 
     ylab="Infant Death Rate", pty="s", pch=19, cex.axis=1.5,
     cex.lab=1.5)
```

Looking at this data naively, it suggests that longer time breast feeding leads to greater infant mortality.

```{r}
plot(infdeath$safe, infdeath$breast, ylab="% Breast Feeding", 
     xlab="% Access to safe water", pty="s", pch=19, cex.axis=1.5,
     cex.lab=1.5)
```

However, there is a **latent variable** which is actually influencing our result. Countries with access to safe water breast feed for less time.

Often, as in the example above, there is a third variable (access to clean water) that affects both the response and the predictor. To truly get at causation we typically need a randomized controlled experiment However, we cannot always do experiments, e.g. most epidemiology.

We can use the `lm` function in R to create a linear model. We then also use R's formula notation to tell it what we want the model to describe. Here, we want to examine the relationship between mortality and breastfeeding, which we can specify by giving R the linear model `y~x`, or here `infdeath$death ~ infdeath$breast`. Note that we don't need to tell R that we want the model to have an intercept, it incorporates one automatically.

```{r}
plot(infdeath$breast, infdeath$death, xlab="% Breast Feeding", 
     ylab="Infant Death Rate", pty="s", pch=19, cex.axis=1.5,
     cex.lab=1.5, xlim=c(0, 100), ylim=c(-30,150))
lm1 = lm(infdeath$death~infdeath$breast)
abline(lm1, col="blue")
```

We can directly output the model coefficients and other information, here the learned slope and intercept of the line, by using `summary`.

```{r}
summary(lm1)
```

We can see that the intercept is about -27 and the slope is about 1.5.

### Residual Plots

We can plot the residuals ($y$-axis) against any covariates ($x$'s) and the predicted values ($\hat{y}$'s).
We want to look for trends, such as increasing or decreasing variability (fans), and outliers.
When data are collected over time, we sometimes look for relationships between successive residuals.

Let's return to our child birth example and make a residual plot. 

```{r residplots, echo=FALSE}
par(mfrow=c(1,2))
plot(lm1$fitted.values, lm1$residuals, xlab="Fitted Values", ylab="Residuals")
abline(h=0)
plot(lm1$model$`infdeath$breast`, lm1$residuals, xlab="Proportion of Mothers Breastfeeding at 6mo", ylab="Residuals")
abline(h=0)
```

## Confidence and Prediction intervals

There are two kinds of predictions that are interesting:

    -   *Confidence interval*: predict the mean response for a given value of $x$
    -   *Prediction interval*: predict the value of the response $y$ for an individual whose covariate is $x$

Almost all regression methods in R have prediction methods associated with them.
Confidence intervals have less variability than prediction intervals. 

Returning to our example on infant death

```{r}
lm1 = lm(death~breast, data=infdeath)
summary(lm1)
```

We can predict the mean response for country where 62% of the women breast feed at 6 months using the `predict1 function.

```{r, echo=TRUE}
predict(lm1, newdata=list(breast=62), interval="confidence")
```

Or we can predict the response for a specific country where 62% of the women breast feed at 6 months

```{r, echo=TRUE}
predict(lm1, newdata=list(breast=62), interval="prediction")
```

Note how the lower and upper bounds are much tighter for the confidence interval than for the prediction interval. 

## Regressions using factors

As we've seen, a *factor* is a variable that takes on a discrete set of different values.
These values can be unordered (e.g. Male/Female or European, Asian, African, etc.) or they can be ordered (age: less than 18, 18-40, more than 40).
In regression models, we typically implement factors using *dummy variables*.

### Fitting factors in regression models

Suppose we have a factor with $K$ levels
We can fit factors in two ways

    -   **M1:** includes an intercept in the model and then use $K-1$ indicator variables
    -   **M2L=:** no intercept and use $K$ indicator variables.

In **M1** the intercept is the mean value of $y$, and each $\beta_j$ is the difference in mean for the $j^{th}$ retained factor level from the overall mean.
In **M2** each of the $\beta_j$ is the mean value for $y$ only within factor level $j$.

To show an example, suppose our factor is sex, which has two levels, M and F.
Our models could be

$$
  M1:  Y = \beta_0 + \beta_M \cdot 1_{M} + \epsilon \\
  E[Y | F]= \beta_0 \quad \mbox{and} \quad E[Y|M] = \beta_0 + \beta_M
$$
```{r FMreg}
heights = runif(40, min=60, max=75)
sex = sample(c("M","F"), 40, replace=TRUE)
lm1 = lm(heights ~ sex)
summary(lm1)
```

Or 

$$ 
  M2: y = \beta_M \cdot 1_{M} + \beta_{F} \cdot 1_{F} + \epsilon \\
  E[Y|F] = \beta_F \quad \mbox{and} E[Y|M] = \beta_M
$$
```{r FMreg2}
# Adding -1 tells R that we don't want an intercept
lm2 = lm(heights ~ sex - 1)
summary(lm2)
```

Note that there are some issues you have to worry about when fitting a model without an intercept.
With model M1 that has an intercept then for each group the test for $H_0: \beta_j = 0$ tests whether that group mean is different from the mean for the group that was used to determine the intercept.
However, in M2 the test for each group, $H_0: \beta_j = 0$ is then comparing the mean for that group to zero (0).
In M2, multiple-$R^2$ does not have a reasonable interpretation, and we get a very high value. 

## Example Analysis - Spline Regression with NHANES

### Load the data

There are 6063 observations, some are incomplete and have missing values for some covariates. There are 22 covariates, which have cryptic names and you need to use the meta-data to resolve them.  The survey is very complex and typically any analysis requires a substantial amount of reading of the documentation.  Here we will guide you past some of the hurdles.

We load up the data and the metadata. In the metadata we have a textual description of the phenotype, the short name, and the target.  The target tells us which of the sampled individuals was eligible to answer the question. 

```{r loadData}
nhanesDataPath = ""
load("../data/nhanes_spline/d4.rda")
load("../data/nhanes_spline/metaD.rda")
DT::datatable(metaD)
```

We will look at the relationship between the variable LBXTC (which is Total Cholesterol in mg/dL measured by a blood draw) and the age of the participant in years. 

```{r, echo=FALSE}
plot(d4$RIDAGEYR, d4$LBXTC, xlab="Age in Years", ylab="Total Cholesterol, mg/dL")
```

And we can see that in this plot, over-plotting is a substantial issue here.
You might also notice what seems like a lot of data at age 80, this is because any age over 80 was truncated to 80 to prevent reidentification of survey participants. In a complete analysis, this should probably be adjusted for in some way, but we will ignore it for now.

We can try some other methods, such as `hexbin` plotting and `smoothScatter` to get a better idea of the distribution of the data.  

```{r, echo=FALSE}
smoothScatter(d4$RIDAGEYR, d4$LBXTC, xlab="Age in Years", ylab="Total Cholesterol, mg/dL")
```

Now we can see a few outliers - with extremely high serum cholesterol.
We get a sense that the trend is not exactly a straight line, but rather a parabola, lower for the young and the old and a bit higher in the middle.

We fit a linear model first.

```{r}    
lm1 = lm(d4$LBXTC ~ d4$RIDAGEYR)
summary(lm1)
```

```{r, echo=TRUE}
plot(lm1$fitted.values, lm1$residuals)

##fit a loess curve
l2 = loess(lm1$residuals ~ lm1$fitted.values)
pl = predict(l2, newdata=sort(lm1$fitted.values))
lines(x=sort(lm1$fitted.values), y=pl, col="blue", lwd=2)
abline(h=0, col="red")

```

Notice that both terms in the model are very significant, but that the multiple $R^2$ is only around 2%.  
So age, in years, is not explaining very much of the variation. 
But because we have such a large data set, the parameter estimates are found to be significantly different from zero.

### Spline Models

When a linear model does not appear sufficient we can try other models.
One choice is to use natural splines, which are very flexible.
They create a "knotted" line, where at each knot the slope of the line can change. 
They are based on B-splines with the prevision that the model is linear outside the range of the data.

However, we have to decide the number of knots we want to use in the model. 
Based on the initial analysis, we chose to use df=7, which gives five internal knots when fitting the splines.
You have almost 6,000 degrees of freedom here, so using up a few to get a more appropriate fit seems good.

```{r}
library("splines")
lm2 = lm(d4$LBXTC ~ ns(d4$RIDAGEYR, df=7))
summary(lm2)
```


We can use an anova to compare the models.
```{r}
anova(lm1, lm2)
```

## Principle Components

Let's now take the continuous variables and look at principle components. 
To calculate the PCA we'll also need to remove any missing data in continuous variables. 

```{r}
library(ggfortify)

cvars = c("RIDAGEYR", "INDFMPIR", "LBDHDD", "LBXGH", "BMXBMI", "LBXTC")
cont_d = d4[, cvars]

complete_ind = complete.cases(cont_d)

cont_d = cont_d[complete_ind,]
d4_missing = d4[!complete_ind,]
d4_comp = d4[complete_ind,]

pcs = prcomp(cont_d)
autoplot(pcs)

which(abs(pcs$x[,1]) > max(abs(pcs$x[,1]))*0.95)
which(abs(pcs$x[,2]) > max(abs(pcs$x[,2]))*0.95)
```

## Random Forests

Random Forests are a simple way to get a sense of how important different variables are in predicting a variable of interest.


```{r}
library("randomForest")
rf1 = randomForest(LBXTC ~ ., proximity=TRUE, data=cont_d)
varImpPlot(rf1)
```

# High-performance modeling

One initial task we might want to perform is checking for performance bottlenecks in our analysis.

```{r, eval = FALSE}
library(profvis)
profvis({
  
  # loess curve
  l2 = loess(lm1$residuals ~ lm1$fitted.values)
  pl = predict(l2, newdata=sort(lm1$fitted.values))

  # Spline regression
  lm2 = lm(d4$LBXTC ~ ns(d4$RIDAGEYR, df=7))
  summary(lm2)
  
  # Comparison
  anova(lm1, lm2)
  
  # PCA and processing
  cvars = c("RIDAGEYR", "INDFMPIR", "LBDHDD", "LBXGH", "BMXBMI", "LBXTC")
  cont_d = d4[, cvars]
  
  complete_ind = complete.cases(cont_d)
  
  cont_d = cont_d[complete_ind,]
  d4_missing = d4[!complete_ind,]
  d4_comp = d4[complete_ind,]
  
  pcs = prcomp(cont_d)
  
  which(abs(pcs$x[,1]) > max(abs(pcs$x[,1]))*0.95)
  which(abs(pcs$x[,2]) > max(abs(pcs$x[,2]))*0.95)
  
  # Random forest
  rf1 = randomForest(LBXTC ~ ., proximity=TRUE, data=cont_d)
})
```

We can take a look at some packages in R for creating models for [data sets too large for memory](https://cran.r-project.org/web/views/HighPerformanceComputing.html)

# Analyzing Survey Data

```{r}
library(survey)
```

# Data

Let's take another look at the NHANES diabetes data we were using yesterday. 
This time we will perform an analysis using only a subset of the total dataset, focusing on adolescents. 

```{r}
all_nhanes <- read_csv("../data/nhanes_diabetes.csv")
nhanes_metadata <- read_csv("../data/nhanes_diabetes_metadata.csv")
datatable(nhanes_metadata)
```

## Preparing Survey Data

### Removing NA weights

We can't have any missing values in the survey design variables. 

Remove all rows with `NA` for the main survey design variables; 
`WTMEC2YR`, `SDMVPSU`, and `SDMVSTRA`.

```{r}
wt_nhanes <- all_nhanes %>%
  drop_na(WTMEC2YR, SDMVPSU, SDMVSTRA)
```


## Creating combined survey weights

Currently, our survey weights `WTSAF2YR` are for each 2 year cycle. 
We need to combine them to represent the full 6-year period we are investigating. 
Luckily NHANES has an [official guide](https://wwwn.cdc.gov/nchs/nhanes/tutorials/weighting.aspx) for combining these weights.
It turns out, all we need to do is divide all weights by 3. 

```{r}
##try de-tidying it to see if we can get the models to behave
wt_nhanes <- wt_nhanes %>%
  mutate(WTMEC6YR = WTMEC2YR * 1/3)
wt_nhanes=data.frame(wt_nhanes)
 wt_nhanes$diabetes = factor(wt_nhanes$diabetes, levels=c("nondiabetic","prediabetic","diabetic"))
```

## Creating the survey design object
We need to use some specialized survey analysis methods which are contained in the `survey` package written by Thomas Lumley.

```{r}
nhanes_design <- svydesign(id     = ~SDMVPSU,
                          strata  = ~SDMVSTRA,
                          weights = ~WTMEC6YR,
                          nest    = TRUE,
                          survey.lonely.psu = "adjust",
                          data    = wt_nhanes)

summary(nhanes_design)
```

Now we can take our data subset from the survey design object. 

```{r}
ado_design <- subset(nhanes_design, RIDAGEYR >= 13 & RIDAGEYR <= 18 & !is.na(OHXDECAY))

#Also make a tibble of this data to analyze
ado_data <- wt_nhanes %>%
  filter(RIDAGEYR >= 13 & RIDAGEYR <= 18) %>% # Gets the 3660 nonedentulous adolescents
  filter(!is.na(OHXDECAY)) %>% # Gets the 3346 with non-NA dental carie variable
  filter(!is.na(diabetes)) # Gets the 3046 with a diabetic status
```


Then create tables using estimated counts from demographic survey weights.

```{r}
tab1 = svytable(~age.cat + dental.caries, ado_design)
tab1
```

```{r svyEthn}
svytable(~RIDRETH1, ado_design)
```

## Logistic Regression

```{r}
logit1 <- svyglm(dental.caries~ diabetes, family=quasibinomial, design=ado_design, na.action = na.omit)
exp(coef(logit1))
summary(logit1)
```

A Wald test for `diabetes`:

```{r}
regTermTest(logit1, ~diabetes)
```

# Exploring packages

## Predictive Modeling with TidyModels

This example comes from [here](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/)

```{r}
library(tidymodels)

iris_split <- initial_split(iris, prop = 0.6)
iris_split
```

```{r}
iris_split %>%
  training() %>%
  glimpse()
```

```{r}
iris_recipe <- training(iris_split) %>%
  recipe(Species ~.) %>%
  step_corr(all_predictors()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep()

iris_recipe
```

```{r}
iris_testing <- iris_recipe %>%
  bake(testing(iris_split)) 

glimpse(iris_testing)
```

```{r}
iris_training <- juice(iris_recipe)

glimpse(iris_training)
```

```{r}
iris_rf <-  rand_forest(trees = 100, mode = "classification") %>%
  set_engine("randomForest") %>%
  fit(Species ~ ., data = iris_training)
```

```{r}
iris_rf %>%
  predict(iris_testing) %>%
  bind_cols(iris_testing) %>%
  glimpse()
```

```{r}
iris_rf %>%
  predict(iris_testing) %>%
  bind_cols(iris_testing) %>%
  metrics(truth = Species, estimate = .pred_class)
```

### Looking up vignettes

## Regularized regression with glmnet

```{r, eval=FALSE}
browseVignettes("glmnet")
```

## Bayesian Statistics with Stan and brms

```{r, eval=FALSE}
browseVignettes("brms")
```

# Other packages of note

- Deep learning with [`keras`](https://cran.r-project.org/web/packages/keras/vignettes/)
- [caret](https://topepo.github.io/caret/) for predictive modeling
- CRAN Task View on [machine learning](https://cran.r-project.org/web/views/MachineLearning.html)
- CRAN task view on [Bayesian Inference](https://cran.r-project.org/web/views/Bayesian.html)